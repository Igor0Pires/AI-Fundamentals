{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regressão Linear Múltipla — Módulo 4, Notebook 1/4**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao Problema](#introducao)\n",
    "2. [Intuição](#intuicao)\n",
    "3. [Compreendendo o Algoritmo](#como-funciona)\n",
    "4. [Formulação Formal](#formal)\n",
    "5. [Métricas de Avaliação](#metricas)\n",
    "6. [Implementação do Zero](#from-scratch)\n",
    "7. [Usando Scikit-learn](#sklearn)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao Problema**\n",
    "\n",
    "Imagine que você é um corretor de imóveis iniciante e sua gerente, a Sra. Ana, uma avaliadora experiente, consegue estimar o preço de uma casa apenas olhando para algumas de suas características. Você, por outro lado, se sente um pouco perdido. Como ela faz isso?\n",
    "\n",
    "Você pergunta a ela, e a Sra. Ana responde: \"É uma questão de entender o impacto de cada característica. Um quarto a mais não adiciona um valor fixo; o impacto dele depende da área total da casa. A localização, o número de banheiros, a idade do imóvel... tudo isso contribui para o preço final, e meu trabalho é ponderar a importância de cada um desses fatores.\"\n",
    "\n",
    "E como podemos criar um modelo que aprenda essas ponderações a partir de dados históricos? E se, em vez de depender apenas da intuição, pudéssemos usar a matemática para encontrar a 'receita' exata que combina essas características para chegar ao preço mais provável?\n",
    "\n",
    "É exatamente esse o problema que a **Regressão Linear Múltipla** se propõe a resolver. Ela nos ajuda a entender e quantificar a relação entre **duas ou mais variáveis independentes** (nossos 'ingredientes', como área, quartos, etc.) e uma **variável dependente** contínua (nosso 'resultado', o preço da casa).\n",
    "\n",
    "**Diferença importante:** Nos algoritmos anteriores (KNN, SVM, etc.), trabalhamos com **classificação** - prever categorias discretas (ex: \"Setosa\", \"Versicolor\", \"Virginica\" ou \"Sobreviveu/Não Sobreviveu\"). Agora, estamos lidando com **regressão** - prever valores contínuos (ex: preço de R$ 250.000, R$ 380.000, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição**\n",
    "\n",
    "A Regressão Linear Múltipla expande a ideia da regressão linear simples. Se na regressão simples tentamos encontrar a **melhor linha** que descreve a relação entre *uma* variável de entrada e a saída, na regressão múltipla, tentamos encontrar o **melhor 'plano' ou 'hiperplano'** que descreve a relação entre *múltiplas* variáveis de entrada e a saída.\n",
    "\n",
    "Pense na 'receita de bolo' da Sra. Ana para precificar uma casa. A receita dela pode ser algo como:\n",
    "\n",
    "**Preço Estimado = (Preço Base) + (Valor por m² × Área) + (Valor por Quarto × N° de Quartos) - (Depreciação por Ano × Idade do Imóvel)**\n",
    "\n",
    "O que a Regressão Linear Múltipla faz é exatamente encontrar os valores para 'Preço Base', 'Valor por m²', 'Valor por Quarto' e 'Depreciação por Ano'. Esses valores são os **coeficientes** (ou pesos) do nosso modelo. Cada coeficiente nos diz o seguinte:\n",
    "\n",
    "> *Mantendo todas as outras características constantes, qual é o impacto médio de se adicionar uma unidade desta característica no resultado final?*\n",
    "\n",
    "Por exemplo, se o 'Valor por Quarto' for R$ 50.000, isso significa que, para casas com a mesma área e idade, adicionar um quarto extra tende a aumentar o preço em R$ 50.000, em média."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta vez, vamos trabalhar com o dataset **[California Housing](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)** - dados reais de preços de casas na Califórnia. Queremos prever o preço médio de uma casa com base em características como:\n",
    "\n",
    "- **MedInc**: Renda média da região\n",
    "- **HouseAge**: Idade média das casas\n",
    "- **AveRooms**: Número médio de cômodos\n",
    "- **AveBedrms**: Número médio de quartos\n",
    "- **Population**: População da região\n",
    "- **AveOccup**: Média de ocupantes por casa\n",
    "- **Latitude** e **Longitude**: Localização geográfica\n",
    "\n",
    "Pergunta: **Qual será o preço de uma casa com essas características específicas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTANDO AS BIBLIOTECAS\n",
    "# =======================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGANDO O CALIFORNIA HOUSING DATASET\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data  # Features: 8 características das casas\n",
    "y = housing.target  # Target: Preço médio (em centenas de milhares de dólares)\n",
    "\n",
    "print(\"Dataset California Housing:\")\n",
    "print(f\"Número de amostras: {X.shape[0]}\")\n",
    "print(f\"Número de features: {X.shape[1]}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nFeatures:\")\n",
    "for i, nome in enumerate(housing.feature_names):\n",
    "    print(f\"{i+1}. {nome}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nVisualizando os dados:\")\n",
    "df = pd.DataFrame(X, columns=housing.feature_names)\n",
    "df['Price'] = y\n",
    "print(df.head())\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando correlação com o preço\n",
    "print(\"Correlação de cada feature com o Preço:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correlacoes = []\n",
    "for i, feature_name in enumerate(housing.feature_names):\n",
    "    corr = np.corrcoef(X[:, i], y)[0, 1]\n",
    "    correlacoes.append((feature_name, corr))\n",
    "    print(f\"{feature_name:15} → Correlação: {corr:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Selecionando as 3 features com maior correlação:\")\n",
    "correlacoes_sorted = sorted(correlacoes, key=lambda x: abs(x[1]), reverse=True)\n",
    "features_selecionadas = correlacoes_sorted[:3]\n",
    "\n",
    "print()\n",
    "for i, (feat_name, corr_value) in enumerate(features_selecionadas, 1):\n",
    "    print(f\"{i}. {feat_name:15} → {abs(corr_value):.4f} (impacto forte no preço)\")\n",
    "\n",
    "# Vou simplificar para as 3 features com maior correlação\n",
    "features = [feat[0] for feat in features_selecionadas]\n",
    "indices = [list(housing.feature_names).index(f) for f in features]\n",
    "\n",
    "print(f\"\\nFeatures selecionadas: {features}\")\n",
    "print(f\"Índices: {indices}\")\n",
    "\n",
    "X_simples = X[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um exemplo: uma casa específica para prever o preço\n",
    "nova_casa = np.array([5.0, 6.0, 15.0])  # MedInc=5.0, AveRooms=6.0, HouseAge=15.0\n",
    "\n",
    "print(\"Nossa casa para avaliação:\")\n",
    "print(f\"  Renda média da região: ${nova_casa[0] * 10000:.0f}\")\n",
    "print(f\"  Número médio de cômodos: {nova_casa[1]:.1f}\")\n",
    "print(f\"  Idade média das casas: {nova_casa[2]:.0f} anos\")\n",
    "print(\"\\nQual será o preço estimado?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando a relação entre features e preço\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "fig.suptitle(\"Relação entre Features e Preço das Casas\", fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (ax, feature) in enumerate(zip(axes, features)):\n",
    "    # Amostragem para melhor visualização\n",
    "    sample_indices = np.random.choice(X.shape[0], 1000, replace=False)\n",
    "    \n",
    "    ax.scatter(X_simples[sample_indices, i], y[sample_indices], \n",
    "               alpha=0.5, s=20, c=y[sample_indices], cmap='viridis')\n",
    "    ax.set_xlabel(feature, fontsize=12)\n",
    "    ax.set_ylabel(\"Preço (x$100k)\", fontsize=12)\n",
    "    ax.set_title(f\"{feature} vs Preço\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Compreendendo o Algoritmo**\n",
    "\n",
    "### **1. A Equação Linear**\n",
    "\n",
    "O modelo cria uma equação que combina todas as features:\n",
    "\n",
    "$$\\text{Preço} = \\beta_0 + \\beta_1 \\times \\text{MedInc} + \\beta_2 \\times \\text{AveRooms} + \\beta_3 \\times \\text{HouseAge}$$\n",
    "\n",
    "Onde:\n",
    "- $\\beta_0$ é o **intercepto** (preço base)\n",
    "- $\\beta_1, \\beta_2, \\beta_3$ são os **coeficientes** (peso de cada feature)\n",
    "\n",
    "### **2. Encontrando os Melhores Coeficientes**\n",
    "\n",
    "O algoritmo tenta encontrar os valores de $\\beta$ que minimizam o **erro quadrático médio** (MSE):\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Onde:\n",
    "- $y_i$ é o preço real\n",
    "- $\\hat{y}_i$ é o preço previsto\n",
    "- $n$ é o número de amostras\n",
    "\n",
    "### **3. Interpretação dos Coeficientes**\n",
    "\n",
    "Cada coeficiente nos diz: \"Se eu aumentar essa feature em 1 unidade (mantendo todas as outras constantes), quanto o preço vai mudar?\"\n",
    "\n",
    "- Se $\\beta_1 = 40.000$: Aumentar a renda média em 1 unidade aumenta o preço em $40k\n",
    "- Se $\\beta_3 = -2.000$: Cada ano a mais de idade diminui o preço em $2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos treinar um modelo simples para ver os coeficientes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simples, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Coeficientes\n",
    "print(\"Coeficientes do modelo:\")\n",
    "print(f\"Intercepto (β₀): ${modelo.intercept_ * 100000:.2f}\")\n",
    "print()\n",
    "for nome, coef in zip(features, modelo.coef_):\n",
    "    print(f\"  {nome} (β): ${coef * 100000:.2f}\")\n",
    "    print(f\"    - Aumentar {nome} em 1 unidade muda o preço em ${abs(coef * 100000):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Fazer previsão para nossa casa\n",
    "preco_previsto = modelo.predict([nova_casa])[0]\n",
    "print(\"=\" * 60)\n",
    "print(f\"Preço estimado para nossa casa: ${preco_previsto * 100000:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio: Parece contraintuitivo que aumentar a quantidade de quartos diminua o preço da casa, levante hipóteses do porque isso está acontecendo**\n",
    "\n",
    "**Resposta e Análise:**\n",
    "\n",
    "Este é um ótimo exemplo de **multicolinearidade**! Aqui estão as possíveis causas:\n",
    "\n",
    "1. **Multicolinearidade entre MedInc e AveRooms:**\n",
    "   - Em regiões de alta renda, casas têm mais cômodos\n",
    "   - Em regiões de baixa renda, casas têm menos cômodos\n",
    "   - O modelo tem dificuldade em separar qual é realmente o efeito\n",
    "\n",
    "2. **Captura de correlações espúrias:**\n",
    "   - AveRooms está **inversamente correlacionado com densidade** (casas menores têm menos quartos)\n",
    "   - Regiões com casas menores (menos quartos) podem ter outros fatores que aumentam o preço (proximidade do centro)\n",
    "\n",
    "3. **Interpretação errada dos coeficientes:**\n",
    "   - O coeficiente de AveRooms não significa \"adicionar um quarto sempre diminui o preço\"\n",
    "   - Significa: \"Mantendo MedInc e HouseAge constantes, adicionar quartos está associado a redução de preço\"\n",
    "   - Mas na realidade, se você adiciona um quarto, a renda e idade também mudam!\n",
    "\n",
    "**Para confirmar:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando a multicolinearidade\n",
    "print(\"ANÁLISE DE MULTICOLINEARIDADE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Matriz de Correlação entre as features selecionadas:\")\n",
    "print()\n",
    "\n",
    "# Criar dataframe com as features simplificadas\n",
    "df_features = pd.DataFrame(X_simples, columns=features)\n",
    "df_features['Price'] = y\n",
    "\n",
    "print(df_features.corr().round(3))\n",
    "\n",
    "print(\"\\n2. Correlação entre as features independentes:\")\n",
    "print()\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        corr = np.corrcoef(X_simples[:, i], X_simples[:, j])[0, 1]\n",
    "        print(f\"   {features[i]} vs {features[j]}: {corr:+.4f}\")\n",
    "\n",
    "print(\"\\n3. Interpretação:\")\n",
    "if abs(np.corrcoef(X_simples[:, 0], X_simples[:, 1])[0, 1]) > 0.5:\n",
    "    print(f\"    MedInc e AveRooms têm correlação forte!\")\n",
    "    print(f\"   Isso explica por que o coeficiente de AveRooms é negativo!\")\n",
    "    print(f\"   Quando MedInc é mantido constante, AveRooms acaba capturando\")\n",
    "    print(f\"   a relação inversa com densidade (menores casas = mais pobreza).\")\n",
    "else:\n",
    "    print(f\"   As features têm correlação fraca. Coeficientes são confiáveis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização: Valores reais vs Previstos\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Previsão Perfeita')\n",
    "plt.xlabel(\"Preço Real (x$100k)\", fontsize=12)\n",
    "plt.ylabel(\"Preço Previsto (x$100k)\", fontsize=12)\n",
    "plt.title(\"Preços Reais vs Previstos - Regressão Linear Múltipla\", \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas de desempenho\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MÉTRICAS DE DESEMPENHO DO MODELO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  MSE (Erro Quadrático Médio):      {mse:.6f}\")\n",
    "print(f\"  RMSE (Raiz do MSE):               {rmse:.6f} (≈ ${rmse * 100000:,.0f})\")\n",
    "print(f\"  MAE (Erro Absoluto Médio):        {mae:.6f} (≈ ${mae * 100000:,.0f})\")\n",
    "print(f\"  R² (Coeficiente de Determinação): {r2:.6f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nINTERPRETAÇÃO DOS RESULTADOS:\\n\")\n",
    "\n",
    "print(f\"1. **RMSE (≈ ${rmse * 100000:,.0f})**\")\n",
    "print(f\"   → Em média, nossas previsões erraram por ${rmse * 100000:,.0f}\")\n",
    "print(f\"   → Para uma casa de preço médio (~$200k), isso é um erro de ~{(rmse*100000/200000)*100:.1f}%\")\n",
    "print(f\"   → Razoável para um modelo linear com apenas 3 features\\n\")\n",
    "\n",
    "print(f\"2. **MAE (≈ ${mae * 100000:,.0f})**\")\n",
    "print(f\"   → Erro absoluto: em média erramos por ${mae * 100000:,.0f} (sem penalizar outliers)\")\n",
    "print(f\"   → Comparação: MAE < RMSE significa que não há outliers extremos\\n\")\n",
    "\n",
    "print(f\"3. **R² = {r2:.4f} ({r2*100:.2f}%)**\")\n",
    "if r2 >= 0.7:\n",
    "    print(f\"   EXCELENTE! O modelo explica {r2*100:.1f}% da variação nos preços\")\n",
    "    print(f\"   - Significa: 70%+ da variação é capturada pelas 3 features escolhidas\")\n",
    "elif r2 >= 0.5:\n",
    "    print(f\"    BOM! O modelo explica {r2*100:.1f}% da variação nos preços\")\n",
    "    print(f\"   - Há ainda {(1-r2)*100:.1f}% de variação inexplicada (outras features, randomness)\")\n",
    "elif r2 >= 0.3:\n",
    "    print(f\"    MODERADO. Explicamos {r2*100:.1f}% da variação\")\n",
    "    print(f\"   - Modelo não é ótimo, mas melhor que adivinhar a média\")\n",
    "else:\n",
    "    print(f\"   FRACO. Explicamos apenas {r2*100:.1f}% da variação\")\n",
    "    print(f\"   - Outras features/relações não-lineares são importantes\")\n",
    "\n",
    "print(f\"\\n4. **Conclusão Prática:**\")\n",
    "print(f\"   - Com 3 features simples, nosso modelo é {r2*100:.1f}% confiável\")\n",
    "print(f\"   - Se quisermos melhorar: usar mais features, relações não-lineares, ou\")\n",
    "print(f\"     regularização (Ridge/Lasso) para lidar com multicolinearidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formal'></a>\n",
    "## **Formulação Formal**\n",
    "\n",
    "**Modelo Matemático**\n",
    "\n",
    "Dado um conjunto de dados com $n$ amostras e $p$ features, a regressão linear múltipla modela a relação entre as variáveis independentes $X$ e a variável dependente $y$ através da equação:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i$$\n",
    "\n",
    "Ou em forma vetorial:\n",
    "\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "\n",
    "Onde:\n",
    "- $y$ é o vetor de valores observados (target)\n",
    "- $X$ é a matriz de features ($n \\times p$)\n",
    "- $\\beta$ é o vetor de coeficientes ($p \\times 1$)\n",
    "- $\\epsilon$ é o vetor de erros (resíduos)\n",
    "\n",
    "**Objetivo: Método dos Mínimos Quadrados**\n",
    "\n",
    "Encontrar $\\beta$ que minimize a soma dos quadrados dos resíduos (SSR):\n",
    "\n",
    "$$\\text{SSR} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2$$\n",
    "\n",
    "Ou em forma matricial:\n",
    "\n",
    "$$\\text{SSR} = (y - X\\beta)^T(y - X\\beta)$$\n",
    "\n",
    "**Solução Analítica (Equação Normal)**\n",
    "\n",
    "A solução que minimiza SSR é:\n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Esta é a **Equação Normal** - a solução de forma fechada para regressão linear.\n",
    "\n",
    "**Premissas do Modelo**\n",
    "\n",
    "1. **Linearidade**: A relação entre X e y é linear\n",
    "2. **Independência**: As observações são independentes\n",
    "3. **Homocedasticidade**: A variância dos erros é constante\n",
    "4. **Normalidade**: Os erros seguem distribuição normal\n",
    "5. **Não-colinearidade**: As features não são altamente correlacionadas entre si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metricas'></a>\n",
    "## **Métricas de Avaliação**\n",
    "\n",
    "Para problemas de regressão, usamos métricas diferentes das de classificação:\n",
    "\n",
    "### **1. MSE (Mean Squared Error)**\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Penaliza erros grandes mais fortemente (devido ao quadrado).\n",
    "\n",
    "### **2. RMSE (Root Mean Squared Error)**\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}}$$\n",
    "\n",
    "Tem a mesma unidade que a variável target (mais interpretável).\n",
    "\n",
    "### **3. MAE (Mean Absolute Error)**\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "Menos sensível a outliers que o MSE.\n",
    "\n",
    "### **4. R² (Coeficiente de Determinação)**\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "Indica a proporção da variância explicada pelo modelo (0 a 1).\n",
    "- $R^2 = 1$: Modelo perfeito\n",
    "- $R^2 = 0$: Modelo não melhor que a média"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='from-scratch'></a>\n",
    "## **Implementação do Zero**\n",
    "\n",
    "Agora que você entende como funciona a Regressão Linear Múltipla, tente implementar o algoritmo do zero, usando apenas NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressaoLinearMultipla:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa o modelo de Regressão Linear Múltipla\n",
    "        \"\"\"\n",
    "        self.coef_ = None  # Coeficientes β\n",
    "        self.intercept_ = None  # Intercepto β₀\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Treina o modelo usando a Equação Normal\n",
    "        \n",
    "        Parâmetros:\n",
    "        X (array-like): Matriz de features (n_samples, n_features)\n",
    "        y (array-like): Vetor target (n_samples,)\n",
    "        \"\"\"\n",
    "        # Adicionar coluna de 1s para o intercepto\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # [1, x1, x2, ..., xp]\n",
    "        \n",
    "        # Aplicar a Equação Normal: β = (X^T X)^(-1) X^T y\n",
    "        beta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        \n",
    "        # Separar intercepto dos coeficientes\n",
    "        self.intercept_ = beta[0]\n",
    "        self.coef_ = beta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Faz previsões para novos dados\n",
    "        \n",
    "        Parâmetros:\n",
    "        X (array-like): Matriz de features (n_samples, n_features)\n",
    "        \n",
    "        Retorna:\n",
    "        array: Previsões (n_samples,)\n",
    "        \"\"\"\n",
    "        # y_pred = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcula o R² (coeficiente de determinação)\n",
    "        \n",
    "        Parâmetros:\n",
    "        X (array-like): Matriz de features\n",
    "        y (array-like): Vetor target verdadeiro\n",
    "        \n",
    "        Retorna:\n",
    "        float: R² score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # R² = 1 - (SS_res / SS_tot)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)  # Soma dos quadrados dos resíduos\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)  # Soma total dos quadrados\n",
    "        \n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "    \n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcula o MSE\n",
    "        \n",
    "        Parâmetros:\n",
    "        y_true (array-like): Valores verdadeiros\n",
    "        y_pred (array-like): Valores previstos\n",
    "        \n",
    "        Retorna:\n",
    "        float: MSE\n",
    "        \"\"\"\n",
    "        # MSE = (1/n) Σ(y_i - ŷ_i)²\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos testar nossa implementação com os dados do California Housing\n",
    "# (já carregados nas células anteriores)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTANDO NOSSA IMPLEMENTAÇÃO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dividindo os dados (já temos X_simples e y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simples, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDados preparados!\")\n",
    "print(f\"Tamanho do treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Tamanho do teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"Número de features: {X_train.shape[1]}\")\n",
    "print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Usando Scikit-learn e Comparando com Nossa Implementação**\n",
    "\n",
    "Agora vamos usar a implementação pronta do scikit-learn e comparar com nossa implementação do zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando nossa implementação\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTANDO NOSSA IMPLEMENTAÇÃO (do zero com NumPy)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "modelo_nosso = RegressaoLinearMultipla()\n",
    "modelo_nosso.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nCoeficientes da nossa implementação:\")\n",
    "print(f\"  Intercepto: ${modelo_nosso.intercept_ * 100000:.2f}\")\n",
    "for feat, coef in zip(features, modelo_nosso.coef_):\n",
    "    print(f\"  {feat}: ${coef * 100000:.2f}\")\n",
    "\n",
    "# Previsões\n",
    "y_pred_nosso = modelo_nosso.predict(X_test)\n",
    "r2_nosso = modelo_nosso.score(X_test, y_test)\n",
    "rmse_nosso = np.sqrt(modelo_nosso.mean_squared_error(y_test, y_pred_nosso))\n",
    "\n",
    "print(f\"\\nR² (nossa impl): {r2_nosso:.6f}\")\n",
    "print(f\"RMSE (nossa impl): ${rmse_nosso * 100000:,.0f}\")\n",
    "\n",
    "# Comparando com scikit-learn\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARANDO COM SCIKIT-LEARN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "modelo_sklearn = LinearRegression()\n",
    "modelo_sklearn.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nCoeficientes do scikit-learn:\")\n",
    "print(f\"  Intercepto: ${modelo_sklearn.intercept_ * 100000:.2f}\")\n",
    "for feat, coef in zip(features, modelo_sklearn.coef_):\n",
    "    print(f\"  {feat}: ${coef * 100000:.2f}\")\n",
    "\n",
    "y_pred_sklearn = modelo_sklearn.predict(X_test)\n",
    "r2_sklearn = modelo_sklearn.score(X_test, y_test)\n",
    "rmse_sklearn = np.sqrt(mean_squared_error(y_test, y_pred_sklearn))\n",
    "\n",
    "print(f\"\\nR² (scikit-learn): {r2_sklearn:.6f}\")\n",
    "print(f\"RMSE (scikit-learn): ${rmse_sklearn * 100000:,.0f}\")\n",
    "\n",
    "# Comparação\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARAÇÃO: Nossa Implementação vs Scikit-learn\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDiferença no R²:        {abs(r2_nosso - r2_sklearn):.9f} (praticamente zero!)\")\n",
    "print(f\"Diferença no RMSE:      ${abs(rmse_nosso - rmse_sklearn) * 100000:.2f}\")\n",
    "print(f\"\\nDiferença de coeficientes:\")\n",
    "for feat, coef_nosso, coef_sk in zip(features, modelo_nosso.coef_, modelo_sklearn.coef_):\n",
    "    diff = abs(coef_nosso - coef_sk)\n",
    "    print(f\"  {feat}: ${diff * 100000:.6f}\")\n",
    "\n",
    "print(f\"\\nDiferença de intercepto: ${abs(modelo_nosso.intercept_ - modelo_sklearn.intercept_) * 100000:.6f}\")\n",
    "\n",
    "print(\"\\nCONCLUSÃO:\")\n",
    "print(\"   Nossa implementação está consistente.\")\n",
    "print(\"   Os resultados são equivalentes aos do scikit-learn (diferenças muito pequenas).\")\n",
    "print(\"   Agora você tem segurança na matemática por trás da Regressão Linear Múltipla.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fechando a conversa:**\n",
    "- Regressão linear múltipla é um bom primeiro passo quando você quer explicar o impacto de cada variável.\n",
    "- Funciona melhor quando a relação é aproximadamente linear e as features não brigam entre si (pouca multicolinearidade).\n",
    "- Se os erros parecerem grandes ou os coeficientes estiverem esquisitos, teste alternativas como SVR, Árvores ou Ensembles.\n",
    "- Antes de trocar de modelo, tente padronizar as features e usar regularização (Ridge/Lasso) para estabilizar os coeficientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<-- [**Anterior: Módulo 3 - Classificação**](../03_Classificacao/06_ensemble_classificacao.ipynb) | [**Próximo: SVR**](02_svr.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
