{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Regression (SVR) — Módulo 4, Notebook 2/4**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução](#introducao)\n",
    "2. [Por que SVR? Intuição e Motivação](#intuicao)\n",
    "3. [Cenários e Dados](#cenarios)\n",
    "4. [Visualização Inicial](#visualizacao)\n",
    "5. [SVR Linear e Tubo ε](#svr-linear)\n",
    "6. [Kernels e Comparações](#kernels)\n",
    "7. [Hiperparâmetros e Ajustes](#hiperparametros)\n",
    "8. [Conclusões e Dicas](#conclusoes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução**\n",
    "\n",
    "Depois da regressão linear múltipla, queremos algo mais flexível para capturar curvas sem perder controle sobre a complexidade. O SVR nasce do SVM de classificação, mas troca a margem de separação por um “tubo” de tolerância aos erros.\n",
    "\n",
    "- Objetivo: encontrar uma função suave que ignore pequenos erros (dentro de ε) e só penalize desvios maiores.\n",
    "- Por que agora? Porque na regressão linear vimos limitações em relações não lineares e sensibilidade a outliers. O SVR lida melhor com essas curvas sem depender apenas de polinômios de alto grau.\n",
    "- O que veremos: intuição, cenários linear vs não linear, efeito de C/ε/kernel, número de vetores de suporte como indicador de complexidade, e comparação entre kernels.\n",
    "\n",
    "Vamos construir a intuição, visualizar, testar e comparar com sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Por que SVR? Intuição e Motivação**\n",
    "\n",
    "Recapitulando rapidamente o SVM de classificação:\n",
    "\n",
    "- **Hiperplano**: Fronteira de decisão que separa as classes no espaço de features.\n",
    "- **Margem**: Distância entre o hiperplano e os pontos mais próximos de cada classe (maximizada no SVM).\n",
    "- **Vetores de suporte**: Pontos que definem a margem e são críticos para posicionar o hiperplano.\n",
    "\n",
    "![](https://www.researchgate.net/publication/359343222/figure/fig1/AS:1182277103042573@1658888235641/SVM-and-SVR-modeling-In-SVM-left-a-hyperplane-with-maximal-margin-is-constructed-to.png?fit=600%)\n",
    "\n",
    "Para regressão, não queremos separar classes — queremos ajustar uma função $f(x)$ que aproxima $y$.\n",
    "\n",
    "- Em vez de minimizar o erro quadrático direto, o SVR ignora erros pequenos: se $|y - f(x)| \\le \\epsilon$, o custo é zero. Só quando $|y - f(x)| > \\epsilon$ ativamos penalização.\n",
    "- Hiperparâmetros:\n",
    "  - **ε (epsilon)**: largura do tubo de tolerância. Pequeno → mais SVs (risco overfitting). Grande → menos SVs (risco underfitting).\n",
    "  - **C**: penalidade para pontos fora do tubo. Alto → prioriza ajuste; baixo → prioriza suavidade.\n",
    "  - **Kernel**: linear ou não linear (RBF, polinomial) para mapear o espaço e capturar curvaturas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cenarios'></a>\n",
    "## **Quando usar SVR? Cenários típicos**\n",
    "\n",
    "- **Relações não lineares moderadas**: Quando a regressão linear não captura curvaturas, mas não queremos um modelo muito complexo.\n",
    "- **Robustez a ruído moderado**: A zona de tolerância $\\epsilon$ ignora pequenos desvios; outliers fortes ainda contam via $C$.\n",
    "- **Dados de média/alta dimensionalidade**: Kernels (RBF, polinomial) capturam padrões sem engenharia manual de features.\n",
    "- **Poucos dados com controle de capacidade**: $C$ e $\\epsilon$ ajudam a equilibrar viés/variância em amostras menores.\n",
    "- **Necessidade de previsões suaves**: Útil quando queremos ignorar flutuações pequenas, mas reagir a erros maiores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTANDO AS BIBLIOTECAS\n",
    "# =======================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# DATASETS (Linear e Não Linear)\n",
    "# =======================================================\n",
    "\n",
    "# 1. Dataset Linear\n",
    "X_lin, y_lin = make_regression(n_samples=400, n_features=1, noise=12, random_state=42)\n",
    "y_lin = y_lin / 100  # escala mais comportada\n",
    "\n",
    "# 2. Dataset Não Linear (seno)\n",
    "X_non = np.linspace(-3, 3, 400).reshape(-1, 1)\n",
    "np.random.seed(42)\n",
    "y_non = np.sin(X_non[:, 0]) + np.random.normal(0, 0.15, size=X_non.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualizacao'></a>\n",
    "## **Visualização Inicial**\n",
    "\n",
    "Vamos primeiro observar a natureza dos nossos datasets antes de aplicar qualquer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# VISUALIZAÇÃO INICIAL\n",
    "# =======================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "axes[0].scatter(X_lin, y_lin, s=15, alpha=0.6)\n",
    "axes[0].set_title('Dataset Linear')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "axes[1].scatter(X_non, y_non, s=15, alpha=0.6, c='tab:orange')\n",
    "axes[1].set_title('Dataset Não Linear (seno + ruído)')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svr-linear'></a>\n",
    "#### Visualizando o Tubo ε\n",
    "\n",
    "Vamos ajustar um SVR linear simples e desenhar o tubo ε para ver quais pontos ficam dentro e fora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SVR Linear: Tubo epsilon\n",
    "# =======================================================\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_lin, y_lin, test_size=0.2, random_state=42)\n",
    "\n",
    "svr_lin = SVR(kernel='linear', C=1.0, epsilon=0.2) \n",
    "# kernel = 'linear' para regressão linear\n",
    "# C = penalidade por erro fora do tubo (quanto maior, mais rígido)\n",
    "# epsilon = largura do tubo (quanto maior, mais tolerante)\n",
    "\n",
    "svr_lin.fit(Xtr, ytr) # Treinamento do modelo\n",
    "\n",
    "y_pred_lin = svr_lin.predict(Xte) # Predições no conjunto de teste\n",
    "rmse_lin = sqrt(mean_squared_error(yte, y_pred_lin)) # RMSE\n",
    "r2_lin = r2_score(yte, y_pred_lin) # R²\n",
    "\n",
    "# Linha de predição e tubos\n",
    "X_plot = np.linspace(X_lin.min(), X_lin.max(), 300).reshape(-1,1)\n",
    "y_line = svr_lin.predict(X_plot)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_lin, y_lin, s=15, alpha=0.6, label='Dados')\n",
    "plt.plot(X_plot, y_line, 'r', label='Função f(x)')\n",
    "plt.plot(X_plot, y_line + svr_lin.epsilon, 'k--', linewidth=1, label='+ε') # Tubo epsilon\n",
    "plt.plot(X_plot, y_line - svr_lin.epsilon, 'k--', linewidth=1, label='-ε')\n",
    "\n",
    "# Destacar pontos fora do tubo\n",
    "outside_mask = np.abs(y_lin - svr_lin.predict(X_lin)) > svr_lin.epsilon\n",
    "plt.scatter(X_lin[outside_mask], y_lin[outside_mask], c='gold', edgecolors='k', s=40, label='Vetores de Suporte (fora)')\n",
    "\n",
    "plt.title(f'SVR Linear (ε={svr_lin.epsilon}) - Tubo')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"RMSE: {rmse_lin:.3f} | R²: {r2_lin:.3f}\")\n",
    "print(f\"Total de vetores de suporte: {len(svr_lin.support_)} ({len(svr_lin.support_)/len(Xtr)*100:.1f}% do treino)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulação Formal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Formulação Formal**\n",
    "\n",
    "Agora vamos formalizar a intuição.\n",
    "\n",
    "Suponha que temos $n$ pares $(x^{(i)}, y^{(i)})$ com $i = 1, \\ldots, n$ e queremos encontrar uma função $f(x) = \\mathbf{w}^T \\phi(x) + b$ que seja tão simples quanto possível (baixa norma $\\|\\mathbf{w}\\|$) e que ao mesmo tempo incorra em erro zero ou mínimo sobre os dados.\n",
    "\n",
    "#### Objetivo: Minimizar Erro fora do Tubo + Complexidade\n",
    "\n",
    "O SVR resolve o problema convexo (forma primal):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\xi, \\xi^*} \\quad \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "sujeito a:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y^{(i)} - \\mathbf{w}^T \\phi(x^{(i)}) - b \\le \\epsilon + \\xi_i \\\\\n",
    "\\mathbf{w}^T \\phi(x^{(i)}) + b - y^{(i)} \\le \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Interpretação:**\n",
    "\n",
    "- **$\\epsilon$ (epsilon)-tubo**: Zona de tolerância onde o custo é zero (espaçamento máximo de $\\pm \\epsilon$ em torno de $f(x)$).\n",
    "- **$\\xi_i, \\xi_i^*$**: Variáveis de folga que permitem violações do tubo. Se um ponto cai fora, paga $C \\times$ (distância além de $\\epsilon$).\n",
    "- **$C$**: Penalidade. Alto $C$ >> ajuste fino aos dados (risco overfitting). Baixo $C$ >> função mais suave (risco underfitting).\n",
    "- **$\\frac{1}{2}\\|\\mathbf{w}\\|^2$**: Regularização (preferência por funções suaves). Minimizar isso = maximizar a margem.\n",
    "\n",
    "#### Forma Dual: Representação em Termos de Vetores de Suporte\n",
    "\n",
    "Através da teoria de otimização convexa (multiplicadores de Lagrange), transformamos o problema primal em sua **forma dual**:\n",
    "\n",
    "$$\n",
    "f(x)=\\sum_{i=1}^n (\\alpha_i - \\alpha_i^*)\\, K(x^{(i)}, x) + b\n",
    "$$\n",
    "\n",
    "onde $K(x^{(i)}, x) = \\phi(x^{(i)})^T \\phi(x)$ é a **função kernel** (atalho para calcular produto interno em espaço mapeado sem explicitar $\\phi$).\n",
    "\n",
    "**Implicações práticas:**\n",
    "\n",
    "1. **Dependência só do kernel**: $\\mathbf{w}$ não aparece explicitamente; a solução depende de $K(\\cdot, \\cdot)$ e dos pesos $\\alpha_i - \\alpha_i^*$.\n",
    "2. **Solução esparsa automática**: A maioria dos $\\alpha_i - \\alpha_i^* = 0$\n",
    "\n",
    "**Significado dos coeficientes $\\alpha$:**\n",
    "\n",
    "- **$\\alpha_i = 0$ e $\\alpha_i^* = 0$**: Ponto dentro do tubo >> não influencia a solução\n",
    "- **$\\alpha_i > 0$ ou $\\alpha_i^* > 0$**: Ponto é um **vetor de suporte** >> define a função\n",
    "\n",
    "3. **Número de vetores de suporte como indicador de complexidade**: Poucos SVs = modelo simples e bem generalizado. Muitos SVs = modelo complexo ou dados ruidosos/outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando SKlearn\n",
    "\n",
    "A implementação oficial já resolve a otimização dual internamente. Vamos comparar kernels e hiperparâmetros rapidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTAÇÕES\n",
    "# =======================================================\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# =======================================================\n",
    "# SEÇÃO: COMPARAÇÃO LINEAR VS RBF (DADOS LINEARES)\n",
    "# =======================================================\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "Xtr_l, Xte_l, ytr_l, yte_l = train_test_split(X_lin, y_lin, test_size=0.2, random_state=0)\n",
    "\n",
    "# Treinar SVR Linear\n",
    "model_linear = SVR(kernel='linear')\n",
    "model_linear.fit(Xtr_l, ytr_l)\n",
    "pred_linear = model_linear.predict(Xte_l)\n",
    "\n",
    "# Treinar SVR RBF\n",
    "model_rbf = SVR(kernel='rbf')\n",
    "model_rbf.fit(Xtr_l, ytr_l)\n",
    "pred_rbf = model_rbf.predict(Xte_l)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\n=== Desempenho SVR: Linear vs RBF (dados lineares) ===\")\n",
    "for name, pred, m in [('Linear', pred_linear, model_linear), ('RBF', pred_rbf, model_rbf)]:\n",
    "    mse = mean_squared_error(yte_l, pred)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(yte_l, pred)\n",
    "    r2 = r2_score(yte_l, pred)\n",
    "    n_support_vectors = len(m.support_)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  MAE:  {mae:.3f}\")\n",
    "    print(f\"  R²:   {r2:.3f}\")\n",
    "    print(f\"  Vetores de Suporte: {n_support_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernels'></a>\n",
    "#### Kernel Trick na Regressão\n",
    "\n",
    "Vamos comparar ajuste de kernel linear vs RBF no dataset não linear (seno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# COMPARAÇÃO: KERNELS LINEAR VS RBF (DADOS NÃO LINEARES)\n",
    "# =======================================================\n",
    "\n",
    "# Usar dados não lineares (seno)\n",
    "Xtr_n, Xte_n, ytr_n, yte_n = train_test_split(X_non, y_non, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar SVR Linear\n",
    "svr_lin_n = SVR(kernel='linear', C=1.0, epsilon=0.05)\n",
    "svr_lin_n.fit(Xtr_n, ytr_n)\n",
    "\n",
    "# Treinar SVR RBF (kernel não linear)\n",
    "svr_rbf_n = SVR(kernel='rbf', C=1.0, epsilon=0.05, gamma='scale')\n",
    "svr_rbf_n.fit(Xtr_n, ytr_n)\n",
    "\n",
    "# Gerar dados para visualização\n",
    "X_plot_n = np.linspace(-3, 3, 400).reshape(-1, 1)\n",
    "lin_pred_curve = svr_lin_n.predict(X_plot_n)\n",
    "rbf_pred_curve = svr_rbf_n.predict(X_plot_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hiperparametros'></a>\n",
    "#### Hiperparâmetros Importantes\n",
    "\n",
    "- C: Controle de penalização (alto >> mais ajuste, baixo >> mais suavidade).\n",
    "- ε: Largura do tubo (alto >> ignora muitos erros pequenos >> pode subajustar; baixo >> tubo fino >> mais SVs >> risco overfitting).\n",
    "- kernel: 'linear', 'rbf', 'poly', 'sigmoid'.\n",
    "- gamma (RBF / poly / sigmoid): Influência local (alto >> captura detalhes / risco overfit; baixo >> mais suave).\n",
    "- degree (poly): Grau do polinômio.\n",
    "\n",
    "Vamos ver sensibilidade rápida em grade reduzida (dataset seno):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# MINI GRID SEARCH (DEMONSTRAÇÃO DE SENSIBILIDADE)\n",
    "# =======================================================\n",
    "\n",
    "print(\"\\n=== Mini Grid Search: Sensibilidade a Hiperparâmetros ===\")\n",
    "print(\"Testando combinações de C, epsilon e gamma...\\n\")\n",
    "\n",
    "param_C = [0.1, 1, 10]\n",
    "param_eps = [0.01, 0.05, 0.2]\n",
    "param_gamma = ['scale', 0.5, 2]\n",
    "\n",
    "results = []\n",
    "for C in param_C:\n",
    "    for eps in param_eps:\n",
    "        for gamma in param_gamma:\n",
    "            svr_tmp = SVR(kernel='rbf', C=C, epsilon=eps, gamma=gamma)\n",
    "            svr_tmp.fit(Xtr_n, ytr_n)\n",
    "            pred = svr_tmp.predict(Xte_n)\n",
    "            r2 = r2_score(yte_n, pred)\n",
    "            rmse = sqrt(mean_squared_error(yte_n, pred))\n",
    "            results.append({'C': C, 'epsilon': eps, 'gamma': gamma, 'R2': r2, 'RMSE': rmse, 'SVs': len(svr_tmp.support_)})\n",
    "\n",
    "# Mostrar top 10 configurações\n",
    "res_df = pd.DataFrame(results).sort_values('R2', ascending=False)\n",
    "print(\"Top 10 configurações:\")\n",
    "print(res_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*O que a saída nos diz sobre a sensibilidade aos hiperparâmetros?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otimização de Hiperparâmetros com Grid Search\n",
    "\n",
    "Para encontrar a melhor combinação de hiperparâmetros de forma sistemática, utilizaremos `GridSearchCV` junto com a validação cruzada. Isso nos permitirá testar diferentes kernels, valores de `C`, `epsilon` e `gamma` para identificar a configuração que proporciona o melhor desempenho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTAÇÕES ADICIONAIS\n",
    "# =======================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =======================================================\n",
    "# GRID SEARCH: OTIMIZAR HIPERPARÂMETROS\n",
    "# =======================================================\n",
    "\n",
    "# Usar dados já criados nas células 3-4 (X_non, y_non)\n",
    "# Dividir em treino e teste\n",
    "Xtr_n, Xte_n, ytr_n, yte_n = train_test_split(X_non, y_non, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar pipeline: Escalar dados antes do SVR\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "# Definir espaço de busca de hiperparâmetros\n",
    "param_grid = {\n",
    "    'svr__kernel': ['rbf', 'linear'],\n",
    "    'svr__C': [0.1, 1, 10, 100],\n",
    "    'svr__epsilon': [0.01, 0.05, 0.1, 0.2],\n",
    "    'svr__gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "# Executar Grid Search com validação cruzada 5-fold\n",
    "print(\"\\n=== Grid Search: Otimização de Hiperparâmetros ===\")\n",
    "print(\"Executando GridSearchCV (pode demorar alguns segundos)...\\n\")\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xtr_n, ytr_n)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n=== Resultados do Grid Search ===\")\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Melhor score (R²) CV: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "best_svr = grid_search.best_estimator_\n",
    "y_pred_best = best_svr.predict(Xte_n)\n",
    "test_r2 = r2_score(yte_n, y_pred_best)\n",
    "test_rmse = np.sqrt(mean_squared_error(yte_n, y_pred_best))\n",
    "\n",
    "print(f\"R² no teste: {test_r2:.3f}\")\n",
    "print(f\"RMSE no teste: {test_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline e Normalização\n",
    "\n",
    "Assim como no SVM de classificação, escalar features normalmente melhora estabilidade (principalmente com RBF / polinomial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# PIPELINE MANUAL: ESCALAR + SVR RBF\n",
    "# =======================================================\n",
    "\n",
    "print(\"\\n=== Pipeline Manual: Escalar + SVR RBF ===\")\n",
    "pipe_svr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, epsilon=0.05))\n",
    "])\n",
    "pipe_svr.fit(Xtr_n, ytr_n)\n",
    "pred_pipe = pipe_svr.predict(Xte_n)\n",
    "\n",
    "print(f\"R² no teste: {r2_score(yte_n, pred_pipe):.3f}\")\n",
    "print(f\"RMSE no teste: {sqrt(mean_squared_error(yte_n, pred_pipe)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnóstico: Resíduos\n",
    "\n",
    "Mesmas análises que regressão linear: queremos resíduos sem padrão, centrados em 0. Aqui mostramos rapidamente para o modelo RBF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# ANÁLISE DE RESÍDUOS\n",
    "# =======================================================\n",
    "\n",
    "print(\"\\n=== Diagnóstico: Análise de Resíduos ===\")\n",
    "residuals_test = yte_n - svr_rbf_n.predict(Xte_n)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Gráfico 1: Resíduos vs Preditos\n",
    "axes[0].scatter(svr_rbf_n.predict(Xte_n), residuals_test, s=25, alpha=0.6)\n",
    "axes[0].axhline(0, color='r', linestyle='--')\n",
    "axes[0].set_title('Resíduos vs Preditos')\n",
    "axes[0].set_xlabel('Valores Preditos')\n",
    "axes[0].set_ylabel('Resíduos')\n",
    "\n",
    "# Gráfico 2: Histograma de Resíduos\n",
    "axes[1].hist(residuals_test, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Distribuição dos Resíduos')\n",
    "axes[1].set_xlabel('Resíduos')\n",
    "axes[1].set_ylabel('Frequência')\n",
    "\n",
    "# Gráfico 3: Q-Q Plot (Normalidade)\n",
    "from scipy import stats\n",
    "stats.probplot(residuals_test, dist='norm', plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Teste de Normalidade)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Média dos resíduos: {residuals_test.mean():.6f}\")\n",
    "print(f\"Desvio padrão: {residuals_test.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusoes'></a>\n",
    "### **Conclusões e Dicas Práticas**\n",
    "\n",
    "Agora que você dominou os conceitos fundamentais do SVR, aqui estão as orientações práticas para aplicação em seus projetos:\n",
    "\n",
    "1. **Começar simples:** Ajuste primeiro um SVR linear como baseline para estabelecer ponto de partida\n",
    "2. **Avaliar padrão:** Se o linear não captura bem o padrão, teste kernels não lineares (RBF primeiro)\n",
    "3. **Normalizar dados:** Sempre normalize features antes de usar kernels não lineares\n",
    "4. **Ajustar ε:** Comece pequeno (0.05) e aumente conforme necessário. Observe % de vetores de suporte\n",
    "5. **Controlar C:** Se detectar overfitting (muitos SVs + ruído), reduza C para aumentar suavidade\n",
    "6. **Tunar gamma (RBF):** Use 'scale' como ponto de partida; ajuste se resultado insatisfatório\n",
    "7. **Grid Search:** Aplique apenas após validar configurações básicas (economiza tempo)\n",
    "8. **Comparar modelos:** Sempre compare com regressão linear para justificar complexidade adicional\n",
    "\n",
    "#### **Referências Principais**\n",
    "\n",
    "Para aprofundar seus conhecimentos em SVR:\n",
    "\n",
    "- **[Documentação Oficial SVR do Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)**\n",
    "  — Referência técnica completa com todos os parâmetros, exemplos e boas práticas\n",
    "  \n",
    "- **[Kernel Trick Explicado](https://www.geeksforgeeks.org/machine-learning/kernel-trick-in-support-vector-classification/)**\n",
    "  — Revisar o conceito matemático fundamental que possibilita SVR não linear\n",
    "  \n",
    "- **[Diagnóstico em Regressão](https://lamfo-unb.github.io/2019/04/13/Diagnostico-em-Regressao/)**\n",
    "  — Técnicas avançadas para análise de resíduos e diagnóstico de modelos de regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<-- [**Anterior: Regressão Linear Múltipla**](01_regressao_linear_multipla.ipynb) | [**Próximo: Árvores de Regressão**](03_arvores_regressao.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
