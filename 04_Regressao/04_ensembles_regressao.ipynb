{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensembles para Regressão — Módulo 4, Notebook Extra 2**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao Ensembles](#introducao)\n",
    "2. [Bagging](#bagging)\n",
    "3. [Formalização Matemática](#formalizacao)\n",
    "4. [Random Forest](#random-forest)\n",
    "5. [Gradient Boosting](#gradient-boosting)\n",
    "6. [Combinando Modelos Diferentes](#voting-stacking)\n",
    "7. [Entendendo Bias vs Variância](#bias-variancia)\n",
    "8. [Comparando Todos os Modelos](#comparacao)\n",
    "9. [Quando Usar Cada Método](#quando-usar)\n",
    "10. [Considerações Importantes](#consideracoes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao Ensembles**\n",
    "\n",
    "Você já parou para pensar na sabedoria coletiva? Quando você quer tomar uma grande decisão, frequentemente consulta várias pessoas. Um conselho de especialistas tende a tomar decisões melhores do que um especialista isolado.\n",
    "\n",
    "O mesmo princípio aplica-se a machine learning. **Ensembles** são algoritmos que combinam múltiplos modelos para fazer previsões. A ideia central é simples: múltiplos modelos, cada um cometendo erros de forma ligeiramente diferente, quando combinados, tendem a dar melhores resultados.\n",
    "\n",
    "Você já aprendeu sobre **Regressão Linear**, **SVR** e **Árvores de Decisão** para regressão. Todos têm suas forças e fraquezas. Agora vamos descobrir como juntá-los de forma inteligente!\n",
    "\n",
    "### **A Situação Prática**\n",
    "\n",
    "Imagine que você precisa prever o **preço de casas** na Califórnia. Você tem dados sobre:\n",
    "- Onde a casa fica (latitude/longitude)\n",
    "- Quantos quartos tem em média\n",
    "- A renda das pessoas na região\n",
    "- Quão velhas são as construções\n",
    "\n",
    "Se cada modelo fosse uma pessoa dando uma opinião:\n",
    "\n",
    "- **Regressão Linear**: \"É simples, casa maior = maior preço, relação é direta\"\n",
    "- **SVR**: \"Depende do contexto, às vezes o preço sobe rápido, às vezes devagar\"\n",
    "- **Árvore de Regressão**: \"Se a casa está em tal região com tantos quartos, o preço é X, senão é Y\"\n",
    "\n",
    "**Ensembles fazem exatamente isso:** juntam essas interpretações complementares para chegar a uma previsão melhor. Em vez de votar em classes (como em classificação), agora fazemos a **média de predições numéricas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging'></a>\n",
    "## **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "Se você leu o capítulo de Ensembles para Classificação, já sabe como Bagging funciona. A mecânica é idêntica, mas em vez de **votar** (\"8 de 10 árvores dizem que sobreviveu\"), agora fazemos **média**:\n",
    "\n",
    "### **Os Três Passos do Bagging**\n",
    "\n",
    "1. **Pega** o dataset de treino\n",
    "2. **Cria** várias versões dele usando bootstrap (amostragem com reposição)\n",
    "3. **Treina** um modelo em cada versão\n",
    "4. **Combina** as previsões fazendo a média\n",
    "\n",
    "Exemplo prático:\n",
    "- Árvore 1 prevê: $2.5M\n",
    "- Árvore 2 prevê: $2.8M\n",
    "- Árvore 3 prevê: $2.3M\n",
    "- **Previsão final**: média = $(2.5 + 2.8 + 2.3) / 3 = 2.53M$\n",
    "\n",
    "### **Por que Funciona?**\n",
    "\n",
    "Cada árvore erra de um jeito diferente. Uma superestima o preço, outra subestima. Quando você tira a média, esses erros tendem a se cancelar mutuamente, reduzindo a variância geral.\n",
    "\n",
    "### **Detalhe Importante: OOB Score**\n",
    "\n",
    "Lembre-se que ~37% dos dados ficam de fora de cada bootstrap (Out-of-Bag). Podemos usar esses dados para avaliar o modelo **de graça**, sem precisar separar um conjunto de validação! Em regressão, o OOB score é basicamente o R² calculado nessas amostras não vistas.\n",
    "\n",
    "### **Vendo Isso na Prática**\n",
    "\n",
    "Vamos usar o **California Housing Dataset** (preços de casas na Califórnia) para observar como Bagging melhora a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o BaggingRegressor do Sklearn\n",
    "\n",
    "Agora vamos usar a implementação pronta do sklearn. Ativaremos `oob_score=True` para obter o \"score grátis\" nos dados Out-of-Bag, funcionando como um conjunto de validação sem precisar separar dados!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTS INICIAIS\n",
    "# =======================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Carregar dataset California Housing\n",
    "california = fetch_california_housing()\n",
    "X_full = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y_full = california.target  # MedHouseVal\n",
    "\n",
    "# Usar subconjunto para velocidade (opcional)\n",
    "X = X_full.sample(n=6000, random_state=42)\n",
    "y = y_full[X.index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Árvore única de regressão (baseline)\n",
    "# =======================================================\n",
    "base_tree = DecisionTreeRegressor(random_state=42, max_depth=None)\n",
    "base_tree.fit(X_train, y_train)\n",
    "\n",
    "pred_base = base_tree.predict(X_test)\n",
    "rmse_base = mean_squared_error(y_test, pred_base)\n",
    "mae_base = mean_absolute_error(y_test, pred_base)\n",
    "r2_base = r2_score(y_test, pred_base)\n",
    "\n",
    "print(f\"Árvore Única - RMSE: {rmse_base:.3f} | MAE: {mae_base:.3f} | R²: {r2_base:.3f}\")\n",
    "\n",
    "# Visualização real vs predito\n",
    "plt.scatter(y_test, pred_base, alpha=0.4)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predito')\n",
    "plt.title('Árvore Única - Real vs Predito')\n",
    "lims = [min(y_test.min(), pred_base.min()), max(y_test.max(), pred_base.max())]\n",
    "plt.plot(lims, lims, 'r--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Várias Árvores bootstrap (manual) para observar variância\n",
    "# =======================================================\n",
    "B = 12 \n",
    "metrics = []\n",
    "for b in range(B):\n",
    "    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_boot = X_train[indices]\n",
    "    y_boot = y_train[indices]\n",
    "    tree_b = DecisionTreeRegressor(random_state=b)\n",
    "    tree_b.fit(X_boot, y_boot)\n",
    "    pred_b = tree_b.predict(X_test)\n",
    "    r2_b = r2_score(y_test, pred_b)\n",
    "    rmse_b = mean_squared_error(y_test, pred_b)\n",
    "    metrics.append((r2_b, rmse_b))\n",
    "    print(f\"Árvore {b+1:02d} -> R²: {r2_b:.3f} | RMSE: {rmse_b:.3f}\")\n",
    "\n",
    "r2_vals = [m[0] for m in metrics]\n",
    "print(f\"\\nR² Médio: {np.mean(r2_vals):.3f} | Desvio: {np.std(r2_vals):.3f} | Min: {min(r2_vals):.3f} | Max: {max(r2_vals):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(r2_vals, marker='o')\n",
    "plt.axhline(np.mean(r2_vals), color='red', linestyle='--', label='Média R²')\n",
    "plt.title('Distribuição de R² entre Árvores Bootstrap')\n",
    "plt.xlabel('Árvore')\n",
    "plt.ylabel('R²')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**\n",
    "\n",
    "Agora que você entendeu intuitivamente como os ensembles funcionam, vamos formalizar os conceitos matematicamente.\n",
    "\n",
    "### **Bagging**\n",
    "\n",
    "A predição do Bagging é a média simples das predições de todos os modelos treinados:\n",
    "\n",
    "$$\\hat{y}_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^{B}f_b(x)$$\n",
    "\n",
    "Onde:\n",
    "- $B$: número de modelos (árvores)\n",
    "- $f_b(x)$: predição do modelo $b$ para a entrada $x$\n",
    "- $\\hat{y}_{\\text{bag}}(x)$: predição final do ensemble\n",
    "\n",
    "### **Out-of-Bag Score**\n",
    "\n",
    "Para cada observação no conjunto de treino, aproximadamente 37% não foram selecionadas em cada bootstrap. Podemos usar essas amostras para calcular o R² sem necessidade de separar um conjunto de validação:\n",
    "\n",
    "$$R^2_{\\text{OOB}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_{OOB,i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "### **Gradient Boosting**\n",
    "\n",
    "Gradient Boosting é sequencial: cada modelo novo tenta corrigir os erros do anterior. A predição é feita acumulativamente:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta f_m(x)$$\n",
    "\n",
    "Onde:\n",
    "- $F_{m-1}(x)$: predição acumulada até o modelo anterior\n",
    "- $f_m(x)$: predição do modelo novo (que tenta prever os resíduos)\n",
    "- $\\eta$: learning rate (taxa de aprendizado) que controla o peso de cada correção\n",
    "- $F_m(x)$: predição final acumulada\n",
    "\n",
    "A ideia é treinar cada novo modelo para prever os **resíduos** do anterior:\n",
    "\n",
    "$$r_{i,m} = y_i - F_{m-1}(x_i)$$\n",
    "\n",
    "E depois adicionar a predição deste modelo à anterior.\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "Random Forest combina Bagging com seleção aleatória de features. A predição é similar ao Bagging, mas cada árvore considera apenas um subset aleatório de features em cada divisão, aumentando a diversidade:\n",
    "\n",
    "$$\\hat{y}_{\\text{RF}}(x) = \\frac{1}{B}\\sum_{b=1}^{B}f_b(x) \\quad \\text{onde cada } f_b \\text{ usa features aleatórias}$$\n",
    "\n",
    "<a id='random-forest'></a>\n",
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(random_state=0),\n",
    "    n_estimators=50,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "pred_bag = bag.predict(X_test)\n",
    "rmse_bag = mean_squared_error(y_test, pred_bag)\n",
    "mae_bag = mean_absolute_error(y_test, pred_bag)\n",
    "r2_bag = r2_score(y_test, pred_bag)\n",
    "print(f\"Bagging - RMSE: {rmse_bag:.3f} | MAE: {mae_bag:.3f} | R² Teste: {r2_bag:.3f} | OOB R²: {bag.oob_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest é um Bagging que além de criar versões diferentes dos dados (bootstrap), também escolhe **features aleatórias** em cada divisão da árvore.\n",
    "\n",
    "### **Por que Isso?**\n",
    "\n",
    "No Bagging normal, se uma feature é muito forte (ex: \"renda mediana\" em preços de casas), **todas** as árvores vão usá-la na primeira divisão. Resultado? As árvores ficam muito parecidas (correlacionadas) e a redução de variância é menor.\n",
    "\n",
    "Random Forest força cada árvore a considerar apenas um **subset aleatório de features** em cada divisão. Isso deixa as árvores mais diferentes umas das outras, e quando você tira a média, ganha ainda mais estabilidade.\n",
    "\n",
    "### **Quando Usar Random Forest**\n",
    "\n",
    "Random Forest costuma ser o **primeiro modelo que você deve tentar** em problemas com dados tabulares (tabelas). Funciona bem out-of-the-box e raramente falha de forma catastrófica.\n",
    "\n",
    "### **Hiperparâmetros Principais**\n",
    "\n",
    "- `n_estimators`: quantas árvores treinar (mais = melhor, até um ponto de saturação)\n",
    "- `max_features`: quantas features considerar por split (default `'sqrt'` funciona bem na prática)\n",
    "- `max_depth`: profundidade máxima de cada árvore (controla complexidade)\n",
    "- `min_samples_leaf`: número mínimo de amostras em uma folha (evita sobreajuste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred_rf = rf.predict(X_test)\n",
    "rmse_rf = mean_squared_error(y_test, pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, pred_rf)\n",
    "r2_rf = r2_score(y_test, pred_rf)\n",
    "print(f\"RandomForest - RMSE: {rmse_rf:.3f} | MAE: {mae_rf:.3f} | R²: {r2_rf:.3f}\")\n",
    "\n",
    "# Importância de features\n",
    "imp = rf.feature_importances_\n",
    "cols = california.feature_names\n",
    "order = np.argsort(imp)[::-1]\n",
    "print(\"\\nImportância das features:\")\n",
    "for i in order:\n",
    "    print(f\"{cols[i]:15}: {imp[i]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar([cols[i] for i in order], imp[order])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Importância')\n",
    "plt.title('Random Forest - Importância de Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gradient-boosting'></a>\n",
    "## **Gradient Boosting**\n",
    "\n",
    "Bagging e Random Forest são **paralelos**: todas as árvores são treinadas independentemente, cada uma vendo um subset de dados.\n",
    "\n",
    "Gradient Boosting é **sequencial**: cada árvore nova tenta **corrigir os erros** da anterior, refinando a predição iterativamente.\n",
    "\n",
    "### **A Ideia Fundamental**\n",
    "\n",
    "1. Treina a primeira árvore (faz uma predição inicial dos preços)\n",
    "2. Calcula os **resíduos** (erros: real - previsto)\n",
    "3. Treina a segunda árvore **para prever os resíduos**, não os valores originais\n",
    "4. Adiciona essa correção à predição anterior\n",
    "5. Calcula novos resíduos\n",
    "6. Repete até ter $N$ árvores\n",
    "\n",
    "É como ir refinando a estimativa aos poucos. Cada árvore é fraca (pequena, com poucas divisões), mas juntas formam um modelo extremamente forte.\n",
    "\n",
    "### **Learning Rate (Taxa de Aprendizado)**\n",
    "\n",
    "O parâmetro $\\eta$ (eta) controla o \"peso\" de cada correção. \n",
    "\n",
    "- Se for muito alto (ex: $\\eta = 0.5$), você aprende rápido mas pode overfitar facilmente\n",
    "- Se for muito baixo (ex: $\\eta = 0.01$), você precisa de muitas árvores mas generaliza melhor\n",
    "\n",
    "**Regra de bolso:** learning_rate baixo (~0.05) + muitas árvores (~300) costuma funcionar bem.\n",
    "\n",
    "### **Early Stopping**\n",
    "\n",
    "Um recurso crítico do Gradient Boosting é o **early stopping**. Você treina monitorizando o desempenho em um conjunto de validação. Quando o desempenho para de melhorar (ou piora), você para de adicionar árvores.\n",
    "\n",
    "```python\n",
    "gb = GradientBoostingRegressor(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,  # pode parecer alto, mas...\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    validation_fraction=0.2,  # 20% para validação\n",
    "    n_iter_no_change=10  # para se não melhorar em 10 iterações\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Isso previne overfitting automático e economiza tempo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "lrs = [0.05, 0.1, 0.2]\n",
    "results_gb = []\n",
    "for lr in lrs:\n",
    "    gb = GradientBoostingRegressor(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=300,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "    pred_gb = gb.predict(X_test)\n",
    "    r2_gb = r2_score(y_test, pred_gb)\n",
    "    rmse_gb = mean_squared_error(y_test, pred_gb)\n",
    "    results_gb.append((lr, r2_gb, rmse_gb))\n",
    "    print(f\"GB (lr={lr}) -> R²: {r2_gb:.3f} | RMSE: { rmse_gb:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([r[0] for r in results_gb],[r[1] for r in results_gb], marker='o')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Gradient Boosting - efeito do learning_rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='voting-stacking'></a>\n",
    "## **Combinando Modelos Diferentes: Voting e Stacking**\n",
    "\n",
    "Até agora combinamos várias **versões do mesmo modelo** (várias árvores). E se combinarmos **modelos completamente diferentes**?\n",
    "\n",
    "### **VotingRegressor: Média Simples**\n",
    "\n",
    "Você treina vários modelos diferentes (Linear, SVR, RandomForest) e faz a **média simples** das predições.\n",
    "\n",
    "Exemplo:\n",
    "- Regressão Linear prevê: $2.3M (captura tendência geral linear)\n",
    "- SVR prevê: $2.7M (captura não-linearidades)\n",
    "- Random Forest prevê: $2.5M (captura interações complexas)\n",
    "- **Voting**: $(2.3 + 2.7 + 2.5) / 3 = 2.5M$\n",
    "\n",
    "**Vantagem:** Cada modelo contribui sua \"expertise\" complementar. A média suaviza outliers individuais.\n",
    "\n",
    "### **StackingRegressor: Aprendendo a Combinar**\n",
    "\n",
    "Em vez de só tirar a média, Stacking treina um **modelo meta** que aprende a melhor forma de combinar as predições. É como ter vários especialistas dando opinião e um coordenador inteligente que sabe quanto peso dar para cada um.\n",
    "\n",
    "**Processo:**\n",
    "1. Treina vários modelos base (Linear, SVR, RF) em subset dos dados\n",
    "2. Faz predições desses modelos em outro subset\n",
    "3. Usa essas predições como **features** para treinar um modelo meta (ex: Gradient Boosting)\n",
    "4. O modelo meta aprende a melhor combinação\n",
    "\n",
    "**Quando usar?** Quando você quer extrair os últimos 0.5-1% de performance. Mas cuidado: mais complexidade = mais chance de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "lin = LinearRegression()\n",
    "svr = SVR(kernel='rbf', C=10, epsilon=0.1)\n",
    "small_rf = RandomForestRegressor(n_estimators=120, random_state=7, n_jobs=-1)\n",
    "\n",
    "vote = VotingRegressor([\n",
    "    ('lin', lin),\n",
    "    ('svr', svr),\n",
    "    ('rf', small_rf)\n",
    "])\n",
    "vote.fit(X_train, y_train)\n",
    "pred_vote = vote.predict(X_test)\n",
    "print(f\"VotingRegressor R²: {r2_score(y_test, pred_vote):.3f}\")\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[('lin', LinearRegression()), ('svr', SVR(kernel='rbf', C=10, epsilon=0.1)), ('rf', RandomForestRegressor(n_estimators=150, random_state=11))],\n",
    "    final_estimator=GradientBoostingRegressor(learning_rate=0.05, n_estimators=200, max_depth=3, random_state=42),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack.fit(X_train, y_train)\n",
    "pred_stack = stack.predict(X_test)\n",
    "print(f\"StackingRegressor R²: {r2_score(y_test, pred_stack):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bias-variancia'></a>\n",
    "## **Entendendo Bias vs Variância com Ensembles**\n",
    "\n",
    "Lembra do dilema **bias-variância** que aprendemos? É crucial para entender por que ensembles funcionam.\n",
    "\n",
    "- **Bias alto**: modelo é simples demais, não captura padrões (underfitting)\n",
    "- **Variância alta**: modelo muda muito com pequenas alterações nos dados (overfitting)\n",
    "\n",
    "### **Como Cada Ensemble Reduz Variância e Bias**\n",
    "\n",
    "| Modelo | Bias | Variância | Abordagem |\n",
    "|--------|------|-----------|-----------|\n",
    "| **Árvore única profunda** | Baixo ✅ | Alta ❌ | Captura tudo, mas instável |\n",
    "| **Bagging** | Baixo ✅ | Baixa ✅ | Média estabiliza predições |\n",
    "| **Random Forest** | Baixo ✅ | Muito Baixa ✅✅ | Bagging + diversidade |\n",
    "| **Gradient Boosting** | Muito Baixo ✅✅ | Média ⚠️ | Reduz bias iterativamente |\n",
    "\n",
    "### **A Estratégia de Cada Ensemble**\n",
    "\n",
    "**Bagging / Random Forest:**\n",
    "- Reduz **variância** drasticamente (média estabiliza)\n",
    "- Mantém bias relativamente baixo\n",
    "- **Resultado:** Melhor para a maioria dos casos\n",
    "\n",
    "**Gradient Boosting:**\n",
    "- Reduz **bias** progressivamente (corrige erros de forma sequencial)\n",
    "- Se exagerar, pode aumentar variância (overfit)\n",
    "- **Resultado:** Precisa de tuning cuidadoso, mas consegue performance muito alta\n",
    "\n",
    "### **Diagnóstico: Como Saber Se Está Overfitting?**\n",
    "\n",
    "Se seu modelo está overfitting (treino ótimo, teste ruim):\n",
    "\n",
    "- ↓ `max_depth` (árvores mais rasas)\n",
    "- ↑ `min_samples_leaf` (folhas maiores)\n",
    "- ↓ `learning_rate` (boosting mais conservador)\n",
    "- ↑ `min_samples_split` (divisões mais conservadoras)\n",
    "\n",
    "Se seu modelo está underfitting (treino e teste ruins):\n",
    "\n",
    "- ↑ `n_estimators` (mais árvores/iterações)\n",
    "- ↑ `max_depth` (mais profundidade)\n",
    "- ↑ `learning_rate` (aprender mais rápido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparacao'></a>\n",
    "## **Comparando Todos os Modelos**\n",
    "\n",
    "Agora que treinamos vários ensembles, vamos colocá-los lado a lado e ver quem performou melhor no nosso dataset de casas da Califórnia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantir que já existem variáveis dos modelos (base_tree, bag, rf, results_gb, vote, stack)\n",
    "# Pegar melhor Gradient Boosting (maior R²)\n",
    "best_gb_lr, best_gb_r2, best_gb_rmse = sorted(results_gb, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "comparacao = []\n",
    "# Regressor base\n",
    "comparacao.append(['Árvore Única', r2_base, rmse_base])\n",
    "comparacao.append(['Bagging', r2_bag, rmse_bag])\n",
    "comparacao.append(['RandomForest', r2_rf, rmse_rf])\n",
    "comparacao.append([f'GradientBoosting(lr={best_gb_lr})', best_gb_r2, best_gb_rmse])\n",
    "comparacao.append(['Voting', r2_score(y_test, pred_vote), mean_squared_error(y_test, pred_vote)])\n",
    "comparacao.append(['Stacking', r2_score(y_test, pred_stack), mean_squared_error(y_test, pred_stack)])\n",
    "\n",
    "res_df = pd.DataFrame(comparacao, columns=['Modelo','R2','RMSE']).sort_values('R2', ascending=False)\n",
    "print(res_df)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(res_df['Modelo'], res_df['R2'])\n",
    "plt.xticks(rotation=40, ha='right')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Comparação de Modelos - Ensembles Regressão')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quando-usar'></a>\n",
    "## **Quando Usar Cada Método**\n",
    "\n",
    "Agora que você conhece todos os ensembles, como escolher qual usar?\n",
    "\n",
    "### **Tabela de Decisão Prática**\n",
    "\n",
    "| Cenário | Recomendação | Razão |\n",
    "|---------|--------------|-------|\n",
    "| **Primeiro modelo a testar** | Random Forest | Melhor out-of-the-box, sem tuning |\n",
    "| **Dados pequenos (<1000 amostras)** | Random Forest | Bagging funciona bem com poucos dados |\n",
    "| **Dados grandes (>100k amostras)** | Gradient Boosting ou LightGBM | Mais control fine-tuning permite melhor performance |\n",
    "| **Features correlacionadas** | Gradient Boosting | GB lida melhor com redundância |\n",
    "| **Precisa interpretabilidade** | Random Forest + Feature Importance | RF é mais fácil de explicar |\n",
    "| **Busca máxima performance** | Gradient Boosting + Stacking | Estes conseguem os últimos % de melhoria |\n",
    "| **Computação limitada** | Random Forest | Menos intensivo que GB/Stacking |\n",
    "| **Dados desbalanceados** | Gradient Boosting | Pode ajustar class_weight |\n",
    "\n",
    "### **Seu Workflow Prático**\n",
    "\n",
    "1. **Baseline rápido**: Testa um RandomForest com configurações padrão\n",
    "2. **Tuning**: Ajusta `n_estimators`, `max_depth`, `min_samples_leaf`\n",
    "3. **Upgrade**: Experimenta GradientBoosting com learning_rate baixo e early stopping\n",
    "4. **Ensemble heterogêneo**: Se ainda não tá bom, combina modelos diferentes com Voting/Stacking\n",
    "5. **Próximo nível**: Explora XGBoost, LightGBM ou CatBoost (versões turbinadas)\n",
    "\n",
    "<a id='consideracoes'></a>\n",
    "## **Considerações Importantes**\n",
    "\n",
    "### **1. Custo Computacional**\n",
    "\n",
    "Ensembles são **significativamente mais lentos** que modelos únicos. Você está treinando dezenas ou centenas de árvores. Para datasets grandes, isso importa:\n",
    "\n",
    "- **Random Forest**: Pode paralelizar facilmente (`n_jobs=-1`)\n",
    "- **Gradient Boosting**: Sequencial por natureza, mas mais rápido por árvore\n",
    "- **Stacking**: Muito lento (treina múltiplos modelos base + modelo meta)\n",
    "\n",
    "### **2. Interpretabilidade vs Performance**\n",
    "\n",
    "Ensembles sacrificam interpretabilidade para ganhar performance:\n",
    "\n",
    "- ❌ Difícil explicar \"por que o modelo decidiu isso\"\n",
    "- ✅ Use feature importance para entender quais features importam\n",
    "- ✅ Use SHAP values para explicações mais detalhadas\n",
    "- ⚠️ Em contextos regulatórios (ex: crédito), isso pode ser crítico\n",
    "\n",
    "### **3. Overfitting em Gradient Boosting**\n",
    "\n",
    "Gradient Boosting agressivo pode overfitar facilmente:\n",
    "\n",
    "- ✅ Use **early stopping** para monitorar validação\n",
    "- ✅ Mantenha `learning_rate` baixo (~0.05)\n",
    "- ✅ Mantenha árvores rasas (`max_depth=3` a `max_depth=5`)\n",
    "- ✅ Aumente `min_samples_leaf`\n",
    "\n",
    "### **4. Quando Ir para XGBoost/LightGBM**\n",
    "\n",
    "Se você chegou ao máximo com Gradient Boosting sklearn, a próxima evolução natural é:\n",
    "\n",
    "- **XGBoost**: Versão otimizada de Gradient Boosting com regularização\n",
    "- **LightGBM**: Mais rápido, usa growth by leaf (menos profundo)\n",
    "- **CatBoost**: Excelente com features categóricas\n",
    "\n",
    "Esses modelos ganham competições de Kaggle, mas a base teórica é a mesma que você aprendeu aqui!\n",
    "\n",
    "---\n",
    "\n",
    "<-- [**Anterior: Árvores de Decisão para Regressão**](03_arvores_regressao.ipynb) | [**Próximo: Módulo 5: Introdução**](../05_Unsupervised_Learning/01_introducao.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
