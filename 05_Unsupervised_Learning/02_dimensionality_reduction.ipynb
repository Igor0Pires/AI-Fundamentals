{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418486f4",
   "metadata": {},
   "source": [
    "# **Redução de Dimensionalidade - Módulo 5, Notebook 2/3**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução](#introducao)\n",
    "2. [A Maldição da Dimensionalidade](#maldicao)\n",
    "3. [Projeção vs. Manifold Learning](#projecao-manifold)\n",
    "4. [PCA - Principal Component Analysis](#pca)\n",
    "   - [Definição e Intuição](#pca-intuicao)\n",
    "   - [Formulação Matemática](#pca-matematica)\n",
    "   - [Implementação com Scikit-Learn](#pca-implementacao)\n",
    "5. [LLE - Locally Linear Embedding](#lle)\n",
    "   - [Definição e Intuição](#lle-intuicao)\n",
    "   - [Implementação e Comparação com PCA](#lle-implementacao)\n",
    "6. [Outros Métodos de Redução de Dimensionalidade](#outros-metodos)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c2b51",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## 1. Introdução\n",
    "\n",
    "Quando trabalhamos com dados do mundo real, frequentemente nos deparamos com datasets que possuem dezenas, centenas ou até milhares de features. Embora mais informação pareça sempre melhor, há um preço a pagar: a **maldição da dimensionalidade**.\n",
    "\n",
    "Neste tópico, vamos explorar:\n",
    "- Por que muitas features podem prejudicar nossos modelos\n",
    "- Como reduzir dimensionalidade preservando informação relevante\n",
    "- Dois métodos: PCA (projeção) e LLE (manifold learning)\n",
    "- Quando usar cada abordagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= IMPORTS =======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_digits, make_swiss_roll\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configuração visual\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cacec",
   "metadata": {},
   "source": [
    "<a id='maldicao'></a>\n",
    "## 2. A Maldição da Dimensionalidade\n",
    "\n",
    "Quando o número de features cresce, o espaço de dados se torna esparso e a noção de vizinhança se dissolve: as distâncias ficam parecidas entre si e perdem poder explicativo. Isso acontece porque cada nova dimensão adiciona um critério extra para que dois pontos sejam considerados próximos. Se as variáveis extras carregam pouco ou nenhum sinal, elas diluem a influência das variáveis úteis e fazem o ruído dominar a métrica de distância, prejudicando modelos baseados em proximidade.\n",
    "\n",
    "Pense no problema de prever renda. Com poucas variáveis relevantes, como escolaridade, é fácil encontrar indivíduos comparáveis. Ao acrescentar atributos pouco relacionados, como número de cachorros ou estilo musical, a distância entre essas mesmas pessoas aumenta - não porque o fenômeno mudou, mas porque exigimos semelhança em critérios que não trazem informação. Com dezenas ou centenas de variáveis irrelevantes, todos os pontos passam a parecer igualmente distantes.\n",
    "\n",
    "Uma resposta intuitiva seria sugerir simplesmente “coletar mais dados”. O problema é que, em alta dimensão, a quantidade de observações necessária para manter uma densidade mínima cresce exponencialmente com o número de features. Para preservar a mesma resolução do espaço - isto é, para continuar encontrando pontos minimamente próximos - o volume a ser preenchido explode. Em dimensões elevadas, o número de amostras exigido rapidamente ultrapassa qualquer possibilidade prática, tornando-se astronômico. Por isso, a solução não é apenas mais dados, é necessário reduzir a dimensionalidade do problema, seja por meio da seleção de variáveis realmente informativas, seja pela construção de representações mais compactas que preservem o sinal relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386970a",
   "metadata": {},
   "source": [
    "<a id='projecao-manifold'></a>\n",
    "## 3. Projeção vs. Manifold Learning\n",
    "\n",
    "Existem duas abordagens principais para reduzir dimensionalidade: **projeção** e **manifold learning**. A primeira busca subespaços lineares que capturam máxima variância; a segunda tenta desenrolar estruturas não-lineares preservando relações locais.\n",
    "\n",
    "### 3.1 Projeção\n",
    "\n",
    "A ideia é encontrar direções no espaço original ao longo das quais os dados variam mais e descartar as direções com pouca variação. Imagine uma nuvem de pontos 3D que na prática forma um disco achatado: podemos projetá-la em um plano 2D sem perder muito, pois a terceira dimensão carrega pouca informação.\n",
    "\n",
    "O método mais conhecido concerteza é o **PCA (Principal Component Analysis)**. Ele é simples, rápido e funciona bem quando os dados variam principalmente em direções lineares. A transformação é explícita e reversível, permitindo aplicar a mesma projeção a novos dados. A limitação é que assume relações lineares e falha quando a estrutura subjacente é complexa ou não-linear.\n",
    "\n",
    "Vamos visualizar essa ideia com um exemplo simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc779bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= EXEMPLO: PROJEÇÃO - MAIOR VS MENOR VARIÂNCIA =======\n",
    "\n",
    "# Gerar dados 3D com tendência clara e variâncias muito diferentes\n",
    "np.random.seed(42)\n",
    "n = 400\n",
    "\n",
    "# Criar dados com forte tendência linear em uma direção\n",
    "t = np.linspace(0, 4*np.pi, n)\n",
    "X_proj = np.zeros((n, 3))\n",
    "X_proj[:, 0] = 3 * t + np.random.randn(n) * 0.8      # Forte variância + tendência no eixo X\n",
    "X_proj[:, 1] = 1.2 * np.sin(t) + np.random.randn(n) * 0.5  # Variância média no eixo Y\n",
    "X_proj[:, 2] = 0.15 * np.cos(t*2) + np.random.randn(n) * 0.15  # Pouca variância no eixo Z\n",
    "\n",
    "# Criar cores para visualização (baseado na posição ao longo da tendência)\n",
    "colors_proj = t\n",
    "\n",
    "# Criar visualização\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# ===== LINHA SUPERIOR: DADOS 3D COM PLANOS DE PROJEÇÃO =====\n",
    "\n",
    "# Plot 1: Dados 3D com plano XY destacado (MAIOR VARIÂNCIA)\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "ax1.scatter(X_proj[:, 0], X_proj[:, 1], X_proj[:, 2], \n",
    "           c=colors_proj, cmap='viridis', s=25, alpha=0.7, edgecolors='k', linewidth=0.3)\n",
    "# Desenhar plano XY (z=0)\n",
    "x_range = [X_proj[:, 0].min()-2, X_proj[:, 0].max()+2]\n",
    "y_range = [X_proj[:, 1].min()-1, X_proj[:, 1].max()+1]\n",
    "xx, yy = np.meshgrid(np.linspace(*x_range, 10), np.linspace(*y_range, 10))\n",
    "zz = np.zeros_like(xx)\n",
    "ax1.plot_surface(xx, yy, zz, alpha=0.25, color='red', edgecolor='none')\n",
    "ax1.set_xlabel('X (forte tendência)', fontsize=11, labelpad=8)\n",
    "ax1.set_ylabel('Y (var. média)', fontsize=11, labelpad=8)\n",
    "ax1.set_zlabel('Z (var. mínima)', fontsize=11, labelpad=8)\n",
    "ax1.set_title('Plano XY - Captura Tendência Principal', fontsize=13, pad=10)\n",
    "ax1.view_init(elev=18, azim=35)\n",
    "\n",
    "# Plot 2: Dados 3D com plano YZ destacado (MENOR VARIÂNCIA)\n",
    "ax2 = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "ax2.scatter(X_proj[:, 0], X_proj[:, 1], X_proj[:, 2], \n",
    "           c=colors_proj, cmap='viridis', s=25, alpha=0.7, edgecolors='k', linewidth=0.3)\n",
    "# Desenhar plano YZ (x=0)\n",
    "y_range = [X_proj[:, 1].min()-1, X_proj[:, 1].max()+1]\n",
    "z_range = [X_proj[:, 2].min()-0.5, X_proj[:, 2].max()+0.5]\n",
    "yy, zz = np.meshgrid(np.linspace(*y_range, 10), np.linspace(*z_range, 10))\n",
    "xx = np.zeros_like(yy)\n",
    "ax2.plot_surface(xx, yy, zz, alpha=0.25, color='orange', edgecolor='none')\n",
    "ax2.set_xlabel('X (forte tendência)', fontsize=11, labelpad=8)\n",
    "ax2.set_ylabel('Y (var. média)', fontsize=11, labelpad=8)\n",
    "ax2.set_zlabel('Z (var. mínima)', fontsize=11, labelpad=8)\n",
    "ax2.set_title('Plano YZ - Perde Tendência Principal', fontsize=13, pad=10)\n",
    "ax2.view_init(elev=18, azim=35)\n",
    "\n",
    "# ===== LINHA INFERIOR: PROJEÇÕES 2D =====\n",
    "\n",
    "# Calcular variâncias\n",
    "var_x, var_y, var_z = np.var(X_proj[:, 0]), np.var(X_proj[:, 1]), np.var(X_proj[:, 2])\n",
    "var_total = var_x + var_y + var_z\n",
    "var_xy = (var_x + var_y) / var_total\n",
    "var_yz = (var_y + var_z) / var_total\n",
    "\n",
    "# Plot 3: Projeção no plano XY (PRESERVA MÁXIMA VARIÂNCIA)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "scatter3 = ax3.scatter(X_proj[:, 0], X_proj[:, 1], c=colors_proj, \n",
    "                      cmap='viridis', s=25, alpha=0.7, edgecolors='k', linewidth=0.3)\n",
    "ax3.set_xlabel('X (forte tendência)', fontsize=11)\n",
    "ax3.set_ylabel('Y (variância média)', fontsize=11)\n",
    "ax3.set_title(f'Projeção XY - {var_xy*100:.2f}% da variância preservada', fontsize=13)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 4: Projeção no plano YZ (PERDE MÁXIMA VARIÂNCIA)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "scatter4 = ax4.scatter(X_proj[:, 1], X_proj[:, 2], c=colors_proj, \n",
    "                      cmap='viridis', s=25, alpha=0.7, edgecolors='k', linewidth=0.3)\n",
    "ax4.set_xlabel('Y (variância média)', fontsize=11)\n",
    "ax4.set_ylabel('Z (variância mínima)', fontsize=11)\n",
    "ax4.set_title(f'Projeção YZ - {var_yz*100:.2f}% da variância preservada', fontsize=13)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.suptitle('Projeção 3D → 2D: Escolha do Plano Determina Informação Preservada', \n",
    "             fontsize=15, y=0.98, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0a258",
   "metadata": {},
   "source": [
    "### 3.2 Manifold Learning\n",
    "\n",
    "Aqui a premissa é diferente: os dados de alta dimensão vivem em (ou próximos de) uma variedade de dimensão menor, mas essa variedade pode estar dobrada, torcida ou enrolada no espaço maior. A tarefa é \"desenrolar\" essa estrutura, preservando vizinhanças locais.\n",
    "\n",
    "A analogia clássica é o rolo de papel higiênico: a superfície é 2D, mas está enrolada no espaço 3D. Métodos de manifold learning tentam \"abrir\" esse rolo e revelar a estrutura plana subjacente.\n",
    "\n",
    "Exemplos incluem **LLE (Locally Linear Embedding)**, **t-SNE** e **Isomap**. Esses métodos capturam relações não-lineares e preservam estruturas locais, mas têm custo computacional maior, sensibilidade a hiperparâmetros e, em geral, não oferecem transformação explícita para novos dados.\n",
    "\n",
    "A seguir, visualizamos ambos os cenários para consolidar a intuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= EXEMPLO: MANIFOLD LEARNING - SWISS ROLL =======\n",
    "\n",
    "# Gerar Swiss Roll (papel higiênico enrolado)\n",
    "X_swiss_demo, color_swiss_demo = make_swiss_roll(n_samples=1000, noise=0.05, random_state=42)\n",
    "\n",
    "# Plotar o manifold 3D\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Visualização 3D do Swiss Roll\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "scatter1 = ax1.scatter(X_swiss_demo[:, 0], X_swiss_demo[:, 1], X_swiss_demo[:, 2], \n",
    "                      c=color_swiss_demo, cmap='Spectral', s=20, alpha=0.7)\n",
    "ax1.set_xlabel('X', fontsize=10)\n",
    "ax1.set_ylabel('Y', fontsize=10)\n",
    "ax1.set_zlabel('Z', fontsize=10)\n",
    "ax1.set_title('Swiss Roll 3D - Superfície 2D Enrolada', fontsize=12)\n",
    "ax1.view_init(elev=15, azim=70)\n",
    "plt.colorbar(scatter1, ax=ax1, label='Posição ao longo do rolo', shrink=0.6)\n",
    "\n",
    "# Visualização de outro ângulo\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "scatter2 = ax2.scatter(X_swiss_demo[:, 0], X_swiss_demo[:, 1], X_swiss_demo[:, 2], \n",
    "                      c=color_swiss_demo, cmap='Spectral', s=20, alpha=0.7)\n",
    "ax2.set_xlabel('X', fontsize=10)\n",
    "ax2.set_ylabel('Y', fontsize=10)\n",
    "ax2.set_zlabel('Z', fontsize=10)\n",
    "ax2.set_title('Visão Lateral - Estrutura Enrolada Evidente', fontsize=12)\n",
    "ax2.view_init(elev=0, azim=0)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Posição ao longo do rolo', shrink=0.6)\n",
    "\n",
    "# Projeção simples (esmagamento) vs ideal (desenrolamento)\n",
    "# Usar a coordenada original do swiss roll que representa o \"desenrolamento\"\n",
    "ax3 = fig.add_subplot(133)\n",
    "scatter3 = ax3.scatter(color_swiss_demo, X_swiss_demo[:, 1], \n",
    "                      c=color_swiss_demo, cmap='Spectral', s=20, alpha=0.7)\n",
    "ax3.set_xlabel('Posição desenrolada (ideal)', fontsize=10)\n",
    "ax3.set_ylabel('Altura (Y)', fontsize=10)\n",
    "ax3.set_title('Estrutura 2D Revelada Após \"Desenrolar\"', fontsize=12)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Manifold Learning: Objetivo é \"Desenrolar\" Estruturas Não-Lineares', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b6d07",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "## 4. PCA - Principal Component Analysis\n",
    "\n",
    "<a id='pca-intuicao'></a>\n",
    "### 4.1 Definição e Intuição\n",
    "\n",
    "**PCA (Principal Component Analysis)** é a ferramenta mais popular e prática para redução de dimensionalidade linear. Sua ideia é simples: encontrar as direções no espaço original ao longo das quais os dados variam mais e descartar as direções onde há pouca variação.\n",
    "\n",
    "Pense em um conjunto de pontos 3D formando uma elipse alongada. O PCA identifica o eixo maior da elipse como a **primeira direção principal** - a que captura a máxima variância. Depois, encontra uma segunda direção, perpendicular à primeira, que captura a máxima variância restante. E assim por diante.\n",
    "\n",
    "Ao manter apenas os primeiros $k$ componentes principais, reduzimos a dimensionalidade de $n$ para $k$, preservando tipicamente 80-95% da informação original. Os algoritmos baseados em distância (como KNN ou K-Means) funcionam muito melhor neste espaço reduzido, onde apenas o sinal relevante domina.\n",
    "\n",
    "O objetivo de PCA é transformar features correlacionadas (e muitas vezes redundantes) em features não-correlacionadas (ortogonais) que capturam máxima variância. Este é um achado fundamental: em um novo sistema de eixos bem escolhido, podemos descartar dimensões inteiras sem perder informação essencial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9f9f3",
   "metadata": {},
   "source": [
    "<a id='pca-matematica'></a>\n",
    "### 4.2 Formulação Matemática\n",
    "\n",
    "O algoritmo de PCA segue uma sequência clara e bem-definida. Dado um dataset $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ com $m$ amostras e $n$ features:\n",
    "\n",
    "**Passo 1 - Centralizar os dados**  \n",
    "Subtraímos a média de cada feature, garantindo que o espaço esteja centrado na origem:\n",
    "$$\\mathbf{X}_{\\text{centered}} = \\mathbf{X} - \\boldsymbol{\\mu}$$\n",
    "\n",
    "**Passo 2 - Calcular a matriz de covariância**  \n",
    "A matriz de covariância captura como cada par de features varia em conjunto:\n",
    "$$\\mathbf{C} = \\frac{1}{m-1} \\mathbf{X}_{\\text{centered}}^T \\mathbf{X}_{\\text{centered}}$$\n",
    "\n",
    "**Passo 3 - Decomposição espectral**  \n",
    "Encontramos autovalores $\\lambda_i$ e autovetores $\\mathbf{v}_i$ de $\\mathbf{C}$. Cada autovetor representa uma direção principal, e o correspondente autovalor representa a variância capturada por essa direção:\n",
    "$$\\mathbf{C} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "\n",
    "**Passo 4 - Selecionar componentes principais**  \n",
    "Ordenamos os autovetores por autovalores em ordem decrescente. Os $k$ primeiros autovetores - aqueles com maiores autovalores - formam a matriz de projeção $\\mathbf{W} \\in \\mathbb{R}^{n \\times k}$.\n",
    "\n",
    "**Passo 5 - Projetar dados**  \n",
    "Finalmente, aplicamos a transformação ao dataset centralizado, gerando a representação reduzida:\n",
    "$$\\mathbf{X}_{\\text{reduzido}} = \\mathbf{X}_{\\text{centered}} \\mathbf{W}$$\n",
    "\n",
    "**Métrica importante - Variância explicada**  \n",
    "O percentual de variância capturado pelo componente $i$ é:\n",
    "$$\\frac{\\lambda_i}{\\sum_{j=1}^{n} \\lambda_j}$$\n",
    "\n",
    "Esta métrica ajuda a decidir quantos componentes são suficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3c914",
   "metadata": {},
   "source": [
    "<a id='pca-implementacao'></a>\n",
    "### 4.3 Implementação com Scikit-Learn\n",
    "\n",
    "Vamos aplicar PCA no dataset **MNIST de dígitos** (8×8 pixels = 64 features) e reduzi-lo para 2D para visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= CARREGAR DATASET DE DÍGITOS =======\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Shape original: {X_digits.shape}\")\n",
    "print(f\"Número de classes: {len(np.unique(y_digits))}\")\n",
    "print(f\"Pixels por imagem: {X_digits.shape[1]} (8x8)\")\n",
    "\n",
    "# Visualizar algumas imagens\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f'Dígito: {y_digits[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= APLICAR PCA: 64D → 2D =======\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_digits)\n",
    "\n",
    "print(f\"Shape após PCA: {X_pca.shape}\")\n",
    "print(f\"\\nVariância explicada por componente:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "print(f\"\\nVariância total explicada: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bca70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= VISUALIZAÇÃO: PROJEÇÃO PCA 2D =======\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_digits, cmap='tab10', \n",
    "                     alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(scatter, label='Dígito')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variância)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variância)', fontsize=12)\n",
    "plt.title('PCA: Projeção de Dígitos de 64D para 2D', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5f829",
   "metadata": {},
   "source": [
    "#### Análise: Scree Plot e Variância Acumulada\n",
    "\n",
    "Um aspecto crucial do PCA é decidir **quantos componentes manter**. Duas ferramentas comuns:\n",
    "\n",
    "1. **Scree Plot**: Visualiza a variância explicada por cada componente\n",
    "2. **Variância Acumulada**: Mostra quanta variância total é preservada ao manter os primeiros $k$ componentes\n",
    "\n",
    "Geralmente buscamos manter 80-95% da variância original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baf275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= ANÁLISE DE VARIÂNCIA: TODOS OS COMPONENTES =======\n",
    "\n",
    "# Ajustar PCA com todos os componentes\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_digits)\n",
    "\n",
    "# Calcular variância acumulada\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Criar subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Scree Plot\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "            pca_full.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Componente Principal', fontsize=12)\n",
    "axes[0].set_ylabel('Variância Explicada', fontsize=12)\n",
    "axes[0].set_title('Scree Plot: Variância por Componente', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Variância Acumulada\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "             marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% variância', linewidth=2)\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% variância', linewidth=2)\n",
    "axes[1].set_xlabel('Número de Componentes', fontsize=12)\n",
    "axes[1].set_ylabel('Variância Acumulada', fontsize=12)\n",
    "axes[1].set_title('Variância Acumulada vs. Número de Componentes', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Encontrar número de componentes para 95% variância\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nComponentes necessários para 95% variância: {n_components_95}/{len(cumulative_variance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb77854",
   "metadata": {},
   "source": [
    "#### Impacto da Redução no Desempenho de Modelos\n",
    "\n",
    "Vamos treinar um classificador com diferentes números de componentes para ver como a redução afeta a acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96527e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= EXPERIMENTO: PCA + CLASSIFICADOR =======\n",
    "\n",
    "# Split dos dados originais\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Testar diferentes números de componentes\n",
    "n_components_list = [2, 5, 10, 15, 20, 30, 40, 50, 64]\n",
    "accuracies = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    if n_comp <= X_train.shape[1]:\n",
    "        # Aplicar PCA\n",
    "        pca_temp = PCA(n_components=n_comp)\n",
    "        X_train_pca = pca_temp.fit_transform(X_train)\n",
    "        X_test_pca = pca_temp.transform(X_test)\n",
    "        \n",
    "        # Treinar classificador\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Avaliar\n",
    "        acc = accuracy_score(y_test, clf.predict(X_test_pca))\n",
    "        accuracies.append(acc)\n",
    "        print(f\"{n_comp:2d} componentes: {acc:.4f} (variância: {pca_temp.explained_variance_ratio_.sum():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b051d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= VISUALIZAÇÃO: ACURÁCIA VS COMPONENTES =======\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_components_list, accuracies, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "plt.xlabel('Número de Componentes PCA', fontsize=12)\n",
    "plt.ylabel('Acurácia no Teste', fontsize=12)\n",
    "plt.title('Desempenho do Classificador vs. Dimensionalidade Reduzida (PCA)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=max(accuracies), color='r', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e46f5",
   "metadata": {},
   "source": [
    "#### Visualização dos Componentes Principais\n",
    "\n",
    "Os componentes principais são na verdade **direções no espaço original**. Podemos visualizá-los como \"imagens\" para entender o que cada componente captura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ce08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= VISUALIZAR PRIMEIROS COMPONENTES COMO IMAGENS =======\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(pca_full.components_):\n",
    "        ax.imshow(pca_full.components_[i].reshape(8, 8), cmap='RdBu_r')\n",
    "        ax.set_title(f'PC{i+1} ({pca_full.explained_variance_ratio_[i]*100:.1f}%)')\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Primeiros 8 Componentes Principais (Autovetores)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b502fb",
   "metadata": {},
   "source": [
    "<a id='lle'></a>\n",
    "## 5. LLE - Locally Linear Embedding\n",
    "\n",
    "<a id='lle-intuicao'></a>\n",
    "### 5.1 Definição e Intuição\n",
    "\n",
    "**LLE (Locally Linear Embedding)** representa uma mudança fundamental de perspectiva. Enquanto PCA busca direções globais de máxima variância, LLE assume que os dados de alta dimensão residem próximos a uma variedade (manifold) não-linear e desconhecida. Sua estratégia é preservar a **estrutura local** dos dados, mantendo vizinhanças intactas durante a redução.\n",
    "\n",
    "A premissa é elegante: se dois pontos estão próximos no espaço original, eles devem permanecer próximos após a redução. Mais precisamente, cada ponto pode ser representado como uma combinação linear de seus vizinhos mais próximos. LLE tira vantagem disso: primeiro aprende os pesos que melhor reconstroem cada ponto a partir de seus vizinhos no espaço original; depois encontra uma projeção de baixa dimensão que preserva esses mesmos pesos.\n",
    "\n",
    "O resultado é uma abordagem capaz de \"desenrolar\" manifolds complexos, capturando estruturas não-lineares que PCA jamais conseguiria explorar.\n",
    "\n",
    "**O Algoritmo em Três Fases**:\n",
    "\n",
    "**Fase 1 - Encontrar vizinhanças**  \n",
    "Para cada ponto $\\mathbf{x}_i$, identificamos seus $k$ vizinhos mais próximos baseado em distância euclidiana.\n",
    "\n",
    "**Fase 2 - Aprender pesos locais**  \n",
    "Encontramos os pesos $w_{ij}$ que melhor reconstroem cada ponto a partir de seus vizinhos, minimizando:\n",
    "$$\\min_{\\mathbf{W}} \\sum_i \\left\\| \\mathbf{x}_i - \\sum_j w_{ij} \\mathbf{x}_j \\right\\|^2$$\n",
    "\n",
    "**Fase 3 - Projetar preservando pesos**  \n",
    "Encontramos projeções $\\mathbf{y}_i$ em dimensão reduzida que mantêm esses mesmos pesos:\n",
    "$$\\min_{\\mathbf{Y}} \\sum_i \\left\\| \\mathbf{y}_i - \\sum_j w_{ij} \\mathbf{y}_j \\right\\|^2$$\n",
    "\n",
    "**Quando LLE brilha**: Em estruturas altamente não-lineares (como manifolds enrolados), LLE captura a geometria intrínseca melhor que qualquer método linear. Em estruturas lineares, ambos funcionam bem, mas PCA é mais rápido.\n",
    "\n",
    "**Limitações práticas**: A escolha de $k$ (número de vizinhos) é crítica e sensível; não há transformação explícita, dificultando a aplicação a novos dados; é computacionalmente mais custoso que PCA.\n",
    "- Mais lento que PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aa2c0",
   "metadata": {},
   "source": [
    "<a id='lle-implementacao'></a>\n",
    "### 5.2 Implementação e Comparação com PCA\n",
    "\n",
    "Vamos comparar PCA e LLE em dois cenários:\n",
    "1. **Dataset de Dígitos**: Estrutura relativamente linear\n",
    "2. **Swiss Roll**: Manifold não-linear clássico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e5890",
   "metadata": {},
   "source": [
    "#### Experimento 1: Dígitos (Estrutura Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= APLICAR LLE NOS DÍGITOS =======\n",
    "\n",
    "# Usar apenas subset para acelerar (LLE é mais lento)\n",
    "n_samples_subset = 1000\n",
    "indices = np.random.choice(len(X_digits), n_samples_subset, replace=False)\n",
    "X_subset = X_digits[indices]\n",
    "y_subset = y_digits[indices]\n",
    "\n",
    "# PCA\n",
    "pca_digits = PCA(n_components=2)\n",
    "X_pca_digits = pca_digits.fit_transform(X_subset)\n",
    "\n",
    "# LLE\n",
    "lle_digits = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_lle_digits = lle_digits.fit_transform(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5454fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= COMPARAÇÃO VISUAL: PCA VS LLE (DÍGITOS) =======\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_digits[:, 0], X_pca_digits[:, 1], c=y_subset, \n",
    "                          cmap='tab10', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[0].set_xlabel('Componente 1', fontsize=12)\n",
    "axes[0].set_ylabel('Componente 2', fontsize=12)\n",
    "axes[0].set_title('PCA - Projeção Linear', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Dígito')\n",
    "\n",
    "# LLE\n",
    "scatter2 = axes[1].scatter(X_lle_digits[:, 0], X_lle_digits[:, 1], c=y_subset, \n",
    "                          cmap='tab10', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[1].set_xlabel('Dimensão 1', fontsize=12)\n",
    "axes[1].set_ylabel('Dimensão 2', fontsize=12)\n",
    "axes[1].set_title('LLE - Manifold Learning (k=10 vizinhos)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Dígito')\n",
    "\n",
    "plt.suptitle('Comparação PCA vs LLE: Dataset de Dígitos', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef393a1",
   "metadata": {},
   "source": [
    "#### Experimento 2: Swiss Roll (Manifold Não-Linear)\n",
    "\n",
    "O Swiss Roll é um dataset clássico em forma de \"rolo suíço\" - uma estrutura 2D enrolada no espaço 3D. É o caso ideal para demonstrar a superioridade de métodos de manifold learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e226dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= GERAR E VISUALIZAR SWISS ROLL =======\n",
    "\n",
    "# Gerar Swiss Roll 3D\n",
    "X_swiss, color_swiss = make_swiss_roll(n_samples=1500, noise=0.1, random_state=42)\n",
    "\n",
    "# Visualizar em 3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], \n",
    "                    c=color_swiss, cmap='viridis', s=20, alpha=0.7)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Swiss Roll: Manifold 2D no Espaço 3D', fontsize=14)\n",
    "plt.colorbar(scatter, label='Posição ao longo do rolo')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= APLICAR PCA E LLE NO SWISS ROLL =======\n",
    "\n",
    "# PCA\n",
    "pca_swiss = PCA(n_components=2)\n",
    "X_pca_swiss = pca_swiss.fit_transform(X_swiss)\n",
    "\n",
    "# LLE\n",
    "lle_swiss = LocallyLinearEmbedding(n_components=2, n_neighbors=12, random_state=42)\n",
    "X_lle_swiss = lle_swiss.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd40b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= COMPARAÇÃO VISUAL: PCA VS LLE (SWISS ROLL) =======\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_swiss[:, 0], X_pca_swiss[:, 1], c=color_swiss, \n",
    "                          cmap='viridis', alpha=0.7, s=20)\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title('PCA - Projeção Linear (Falha em \"Desenrolar\")', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Posição')\n",
    "\n",
    "# LLE\n",
    "scatter2 = axes[1].scatter(X_lle_swiss[:, 0], X_lle_swiss[:, 1], c=color_swiss, \n",
    "                          cmap='viridis', alpha=0.7, s=20)\n",
    "axes[1].set_xlabel('Dimensão 1', fontsize=12)\n",
    "axes[1].set_ylabel('Dimensão 2', fontsize=12)\n",
    "axes[1].set_title('LLE - Desenrola o Manifold com Sucesso', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Posição')\n",
    "\n",
    "plt.suptitle('Comparação PCA vs LLE: Swiss Roll (Manifold Não-Linear)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9507c41",
   "metadata": {},
   "source": [
    "### Interpretação das Comparações\n",
    "\n",
    "**Dataset de Dígitos**: Ambos PCA e LLE funcionam razoavelmente bem, pois a estrutura é relativamente linear. PCA é preferível por ser mais rápido e ter transformação explícita.\n",
    "\n",
    "**Swiss Roll**: \n",
    "- **PCA falha** em \"desenrolar\" o manifold - vemos apenas uma projeção achatada onde pontos distantes ao longo do rolo aparecem próximos\n",
    "- **LLE consegue** preservar a estrutura local e desenrolar o rolo, revelando a estrutura 2D verdadeira\n",
    "\n",
    "**Quando usar cada método**:\n",
    "- **PCA**: Dados com variações principalmente lineares, necessidade de velocidade, aplicação a novos dados\n",
    "- **LLE**: Dados com estrutura não-linear conhecida, foco em preservar relações locais, visualização exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34139890",
   "metadata": {},
   "source": [
    "<a id='outros-metodos'></a>\n",
    "## 6. Outros Métodos de Redução de Dimensionalidade\n",
    "\n",
    "Além de PCA e LLE, o ecossistema oferece diversos outros métodos especializados, cada um adequado para cenários e dados específicos. Aqui apresentamos um panorama dos mais relevantes.\n",
    "\n",
    "### 6.1 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**t-SNE** é especialista em **visualização exploratória**. Ao contrário de PCA e LLE que buscam preservar variância ou vizinhança em geral, t-SNE otimiza especificamente para renderizar estruturas de cluster em 2D ou 3D de forma legível ao olho humano.\n",
    "\n",
    "O algoritmo preserva as relações locais de proximidade com fidelidade notável: pontos próximos no espaço original tendemos a permanecer próximos na visualização, e clusters bem separados aparecem isolados visualmente. Isso o torna excelente para análise exploratória inicial de datasets.\n",
    "\n",
    "**Limitações importantes**: t-SNE não é determinístico (resultados variam entre execuções), é computacionalmente muito lento para grandes datasets e não oferece transformação explícita, impossibilitando sua aplicação direto a novos dados. Também é sensível a hiperparâmetros como taxa de aprendizado e \"perplexidade\".\n",
    "\n",
    "**Extra**: [`sklearn.manifold.TSNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "### 6.2 Isomap\n",
    "\n",
    "**Isomap** estende o conceito clássico de MDS (Multidimensional Scaling) para dados em manifolds não-lineares. Sua inovação principal é usar **distâncias geodésicas** (caminho mais curto *ao longo* da variedade) em vez de distâncias euclidianas diretas. Isto permite capturar a geometria intrínseca de manifolds complexos.\n",
    "\n",
    "O algoritmo: (1) constrói um grafo de vizinhança dos pontos; (2) calcula distâncias geodésicas via shortest paths no grafo; (3) aplica MDS clássico nessas distâncias. É particularmente eficaz em manifolds bem-definidos sem \"furos\" ou topologias complexas.\n",
    "\n",
    "**Limitação**: Sensível a ruído e descontinuidades nos dados; a qualidade depende criticamente de encontrar a vizinhança correta.\n",
    "\n",
    "**Extra**: [`sklearn.manifold.Isomap`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html)\n",
    "\n",
    "### 6.4 Kernel PCA\n",
    "\n",
    "**Kernel PCA** transporta o \"kernel trick\" - familiar de SVM - para o contexto de PCA. Ao invés de trabalhar no espaço original de features, Kernel PCA implicitamente aplica uma transformação não-linear (via kernel) e então executa PCA nesse novo espaço transformado.\n",
    "\n",
    "Isso permite capturar estruturas não-lineares que PCA linear seria incapaz de revelar. A escolha do kernel (RBF, polinomial, sigmoid) e seus hiperparâmetros é crítica e requer validação cuidadosa. Em muitos cenários, oferece melhor compromisso entre simplicidade de PCA e flexibilidade de métodos manifold.\n",
    "C\n",
    "**Extra**: [`sklearn.decomposition.KernelPCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)\n",
    "\n",
    "### 6.5 Autoencoders (Deep Learning)\n",
    "\n",
    "**Autoencoders** são redes neurais que aprendem representações comprimidas de forma completamente não-supervisionada. A arquitetura é simétrica: um encoder que comprime dados em dimensão reduzida, seguido de um decoder que reconstrói os dados originais. A perda de reconstrução força a aprendizagem de representações informativas.\n",
    "\n",
    "O poder dos autoencoders está em sua flexibilidade: variantes como variational autoencoders (VAE) ou convolutional autoencoders (para imagens) permitem capturar estruturas altamente não-lineares e complexas. São especialmente valiosos em domínios como imagens, áudio e texto.\n",
    "\n",
    "**Limitações**: Requerem muito mais dados e poder computacional que métodos clássicos; ajuste de hiperparâmetros (arquitetura, taxa de aprendizado) é mais envolvido; interpretação das dimensões latentes é menos clara."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a2d2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<-- [**Anterior: Introdução ao Unsupervised Learning**](01_introducao.ipynb) | [**Próximo: Clustering**](03_clustering.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
