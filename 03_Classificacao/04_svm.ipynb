{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM (Support Vector Machines) — Módulo 3, Notebook 4/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao SVM](#introducao)\n",
    "2. [Como o SVM Funciona?](#como-funciona)\n",
    "3. [Intuição Prática](#intuicao)\n",
    "4. [Kernel Trick: Além do Linear](#kernel-trick)\n",
    "5. [Formalização Matemática](#formalizacao)\n",
    "6. [Implementação com Scikit-Learn](#sklearn)\n",
    "7. [Vantagens e Desvantagens](#vantagens-desvantagens)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao SVM**\n",
    "\n",
    "Support Vector Machines (SVM) são algoritmos de aprendizado supervisionado utilizados para tarefas de classificação e regressão. O objetivo principal do SVM é encontrar o hiperplano ótimo que separa diferentes classes de dados, maximizando a margem entre elas.\n",
    "\n",
    "Enquanto existem infinitas fronteiras de decisão possíveis que podem separar duas classes, o SVM busca aquela que oferece a maior distância entre os pontos mais próximos de cada classe - uma propriedade que confere maior robustez ao modelo.\n",
    "\n",
    "**Comparação com outros algoritmos de classificação:**\n",
    "\n",
    "- **KNN**: Baseado em proximidade, armazena todos os dados de treinamento\n",
    "- **Naive Bayes**: Abordagem probabilística com premissa de independência entre features\n",
    "- **Regressão Logística**: Modelo linear que estima probabilidades através da função logística\n",
    "- **SVM**: Encontra hiperplano ótimo que maximiza a margem entre classes\n",
    "\n",
    "**Características principais:**\n",
    "\n",
    "- **Maximização de margem**: Otimiza a separação entre classes\n",
    "- **Kernel trick**: Permite resolver problemas não-lineares através de transformações implícitas\n",
    "- **Alta dimensionalidade**: Eficaz mesmo quando o número de features excede o número de amostras\n",
    "- **Eficiência**: Decisão baseada apenas nos vetores de suporte\n",
    "\n",
    "**Conteúdo do notebook:**\n",
    "\n",
    "1. Como o SVM funciona (hiperplanos e margens)\n",
    "2. Intuição prática (classificação de flores Iris)\n",
    "3. Kernel trick (tratamento de não-linearidade)\n",
    "4. Formalização matemática (otimização e teoria)\n",
    "5. Implementação com Scikit-Learn (aplicação prática)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Como o SVM Funciona?**\n",
    "\n",
    "O SVM resolve o problema de classificação através de três etapas fundamentais:\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapa 1: Identificação do Hiperplano Separador**\n",
    "\n",
    "O primeiro passo consiste em identificar um hiperplano que separa as classes:\n",
    "- Em **2D**: uma linha\n",
    "- Em **3D**: um plano  \n",
    "- Em **N dimensões**: um hiperplano\n",
    "\n",
    "Existem infinitas opções de hiperplanos capazes de separar os dados.\n",
    "\n",
    "<img src=\"https://uk.mathworks.com/discovery/support-vector-machine/_jcr_content/mainParsys/band_1231704498_copy/mainParsys/lockedsubnav/mainParsys/columns/4d6875cb-8556-43eb-9393-53bcec9e3682/columns/29ee2f91-358a-4aca-8f04-e0f536feb8f2/image_2128876021_cop.adapt.full.medium.jpg/1759842383498.jpg\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapa 2: Maximização da Margem**\n",
    "\n",
    "O SVM seleciona o hiperplano que maximiza a margem:\n",
    "\n",
    "$$\\text{Margem} = \\text{distância mínima entre o hiperplano e os pontos mais próximos de cada classe}$$\n",
    "\n",
    "**Razões para maximização da margem:**\n",
    "\n",
    "- Maior robustez na generalização para novos dados\n",
    "- Redução do risco de overfitting\n",
    "- Melhoria na capacidade de generalização do modelo\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapa 3: Definição dos Vetores de Suporte**\n",
    "\n",
    "Os pontos mais próximos ao hiperplano são denominados vetores de suporte (support vectors).\n",
    "\n",
    "**Propriedades dos vetores de suporte:**\n",
    "\n",
    "- São os únicos pontos que determinam a posição do hiperplano\n",
    "- A remoção de outros pontos não altera a fronteira de decisão\n",
    "- Definem completamente a solução do problema de otimização\n",
    "\n",
    "---\n",
    "\n",
    "### **Tratamento de Dados Não-Linearmente Separáveis**\n",
    "\n",
    "Quando os dados não podem ser separados por um hiperplano linear, o SVM utiliza o kernel trick:\n",
    "\n",
    "1. Mapeia os dados para um espaço de maior dimensão\n",
    "2. Nesse novo espaço, os dados tornam-se linearmente separáveis\n",
    "3. Encontra o hiperplano ótimo no espaço transformado\n",
    "\n",
    "**Kernels mais utilizados:**\n",
    "\n",
    "- **Linear**: Dados já linearmente separáveis\n",
    "- **Polinomial**: Relações polinomiais entre features\n",
    "- **RBF (Radial Basis Function)**: Fronteiras complexas e não-lineares (mais utilizado)\n",
    "- **Sigmoid**: Casos específicos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição Prática**\n",
    "\n",
    "Para ilustrar o funcionamento do SVM, utilizaremos o dataset Iris para classificação.\n",
    "\n",
    "### **Contexto do Problema**\n",
    "\n",
    "Dadas as medidas de uma flor íris:\n",
    "- Comprimento da pétala (cm)\n",
    "- Largura da pétala (cm)\n",
    "\n",
    "Objetivo: Classificar a espécie entre:\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica\n",
    "\n",
    "### **Análise Exploratória**\n",
    "\n",
    "Primeiro, visualizaremos a distribuição das flores utilizando comprimento e largura da pétala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_circles\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 1. Análise Exploratória\n",
    "# =================================================\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [2, 3]] \n",
    "y = iris.target\n",
    "\n",
    "print(f\"Iris data shape: {X.shape}\")\n",
    "print(f\"\\n Classes:\")\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    count = np.sum(y == i)\n",
    "    print(f\" {i}: {name} ({count} samples)\")\n",
    "\n",
    "# Visualização dos dados\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "color = ['red', 'blue', 'green']\n",
    "marker = ['o', 's', '^']\n",
    "\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    indices = y == i\n",
    "    ax.scatter(X[indices, 0], X[indices, 1], \n",
    "    c=color[i], marker=marker[i], s=100, alpha=0.7, \n",
    "    edgecolors='k', linewidth=1.5, label=name.capitalize())\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[2].capitalize(), fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(iris.feature_names[3].capitalize(), fontsize=12, fontweight='bold')\n",
    "ax.set_title('Iris Dataset - Sepal Length vs Sepal Width', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# Visualização: Fronteiras de Decisão\n",
    "# =================================================\n",
    "\n",
    "# Transformar em problema binário para facilitar visualização\n",
    "# Setosa (classe 0) vs Não-Setosa (classes 1 e 2)\n",
    "y_binary = (y != 0).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('SVM: Encontrando a Fronteira Ótima', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# ===== GRÁFICO 1: Comparando Fronteiras =====\n",
    "ax1 = axes[0]\n",
    "\n",
    "setosa_mask = y_binary == 0\n",
    "not_setosa_mask = y_binary == 1\n",
    "\n",
    "ax1.scatter(X[setosa_mask, 0], X[setosa_mask, 1],\n",
    "            c='red', s=100, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Setosa', marker='o')\n",
    "\n",
    "ax1.scatter(X[not_setosa_mask, 0], X[not_setosa_mask, 1],\n",
    "            c='blue', s=100, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Não-Setosa', marker='s')\n",
    "\n",
    "x_line = np.linspace(0, 7, 100)\n",
    "\n",
    "# Fronteira ruim (muito próxima de uma classe)\n",
    "y_line1 = -0.3 * x_line + 1\n",
    "ax1.plot(x_line, y_line1, 'gray', linestyle='--', linewidth=2, alpha=0.5, label='Fronteira Ruim')\n",
    "\n",
    "# Fronteira OK (separa, mas não maximiza margem)\n",
    "y_line2 = 0.02 * x_line + 0.7\n",
    "ax1.plot(x_line, y_line2, 'orange', linestyle='--', linewidth=2, alpha=0.7, label='Fronteira OK')\n",
    "\n",
    "# Fronteira SVM (maximiza margem)\n",
    "y_line3 = -0.3 * x_line + 1.5\n",
    "ax1.plot(x_line, y_line3, 'green', linestyle='-', linewidth=3, label='Fronteira SVM (ótima)')\n",
    "\n",
    "ax1.set_xlabel('Comprimento da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Largura da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Comparando Fronteiras de Decisão', fontsize=12, fontweight='bold', pad=15)\n",
    "ax1.legend(fontsize=9, loc='upper left')\n",
    "ax1.set_xlim(0, 7)\n",
    "ax1.set_ylim(0, 2.8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== GRÁFICO 2: Margens e Hiperplanos =====\n",
    "ax2 = axes[1]\n",
    "\n",
    "ax2.scatter(X[setosa_mask, 0], X[setosa_mask, 1],\n",
    "            c='red', s=100, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Setosa', marker='o')\n",
    "\n",
    "ax2.scatter(X[not_setosa_mask, 0], X[not_setosa_mask, 1],\n",
    "            c='blue', s=100, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Não-Setosa', marker='s')\n",
    "\n",
    "y_hyperplane = -0.3 * x_line + 1.5\n",
    "ax2.plot(x_line, y_hyperplane, 'green', linestyle='-', linewidth=3,\n",
    "         label='Hiperplano de Decisão')\n",
    "\n",
    "# Desenhar margens\n",
    "margin = 0.4\n",
    "y_margin1 = y_hyperplane + margin\n",
    "y_margin2 = y_hyperplane - margin\n",
    "\n",
    "ax2.plot(x_line, y_margin1, 'green', linestyle=':', linewidth=2, alpha=0.6,\n",
    "         label='Margem Superior')\n",
    "ax2.plot(x_line, y_margin2, 'green', linestyle=':', linewidth=2, alpha=0.6,\n",
    "         label='Margem Inferior')\n",
    "\n",
    "# Preencher área da margem\n",
    "ax2.fill_between(x_line, y_margin1, y_margin2, color='green', alpha=0.1)\n",
    "\n",
    "ax2.set_xlabel('Comprimento da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Largura da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Margens Maximizadas', fontsize=12, fontweight='bold', pad=15)\n",
    "ax2.legend(fontsize=9, loc='upper left')\n",
    "ax2.set_xlim(0, 7)\n",
    "ax2.set_ylim(0, 2.8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== GRÁFICO 3: Support Vectors =====\n",
    "ax3 = axes[2]\n",
    "\n",
    "ax3.scatter(X[setosa_mask, 0], X[setosa_mask, 1],\n",
    "            c='red', s=100, alpha=0.3, edgecolors='k', linewidth=1.5,\n",
    "            label='Setosa (outros)', marker='o')\n",
    "\n",
    "ax3.scatter(X[not_setosa_mask, 0], X[not_setosa_mask, 1],\n",
    "            c='blue', s=100, alpha=0.3, edgecolors='k', linewidth=1.5,\n",
    "            label='Não-Setosa (outros)', marker='s')\n",
    "\n",
    "# Destacar Support Vectors (pontos críticos)\n",
    "support_vectors_setosa = np.array([[1.4, 0.2], [1.9, 0.4]])\n",
    "support_vectors_not_setosa = np.array([[3.0, 1.1], [4.0, 1.3]])\n",
    "\n",
    "ax3.scatter(support_vectors_setosa[:, 0], support_vectors_setosa[:, 1],\n",
    "            c='red', s=300, alpha=1.0, edgecolors='yellow', linewidth=4,\n",
    "            label='Support Vectors Setosa', marker='o')\n",
    "\n",
    "ax3.scatter(support_vectors_not_setosa[:, 0], support_vectors_not_setosa[:, 1],\n",
    "            c='blue', s=300, alpha=1.0, edgecolors='yellow', linewidth=4,\n",
    "            label='Support Vectors Não-Setosa', marker='s')\n",
    "\n",
    "ax3.plot(x_line, y_hyperplane, 'green', linestyle='-', linewidth=3, label='Hiperplano')\n",
    "ax3.plot(x_line, y_margin1, 'green', linestyle=':', linewidth=2, alpha=0.6)\n",
    "ax3.plot(x_line, y_margin2, 'green', linestyle=':', linewidth=2, alpha=0.6)\n",
    "ax3.fill_between(x_line, y_margin1, y_margin2, color='green', alpha=0.1)\n",
    "\n",
    "ax3.set_xlabel('Comprimento da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Largura da Pétala (cm)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Support Vectors (Pontos Críticos)', fontsize=12, fontweight='bold', pad=15)\n",
    "ax3.legend(fontsize=9, loc='upper left')\n",
    "ax3.set_xlim(0, 7)\n",
    "ax3.set_ylim(0, 2.8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservações:\")\n",
    "print(\"   • A fronteira SVM maximiza a distância entre as classes\")\n",
    "print(\"   • Os Support Vectors (destacados) são os únicos pontos que importam\")\n",
    "print(\"   • A remoção de outros pontos não altera a fronteira\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel-trick'></a>\n",
    "## **Kernel Trick: Tratamento de Não-Linearidade**\n",
    "\n",
    "### **Problema: Dados Não-Linearmente Separáveis**\n",
    "\n",
    "No exemplo anterior, conseguimos separar a classe Setosa das demais com uma fronteira linear. Porém, muitos problemas não apresentam separação linear.\n",
    "\n",
    "Considere dados dispostos em círculos concêntricos:\n",
    "- Classe Vermelha: Círculo interior\n",
    "- Classe Azul: Círculo exterior\n",
    "\n",
    "Nenhuma linha reta consegue separar essas classes adequadamente.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solução: Kernel Trick**\n",
    "\n",
    "O kernel trick resolve este problema através de:\n",
    "\n",
    "**Passo 1:** Mapeamento implícito dos dados para espaço de maior dimensão  \n",
    "**Passo 2:** Nesse novo espaço, os dados tornam-se linearmente separáveis  \n",
    "**Passo 3:** Identificação do hiperplano ótimo no espaço transformado  \n",
    "**Passo 4:** Projeção da fronteira de volta ao espaço original\n",
    "\n",
    "---\n",
    "\n",
    "### **Funções Kernel**\n",
    "\n",
    "| Kernel | Equação | Aplicação |\n",
    "|--------|---------|-------------|\n",
    "| **Linear** | $K(x, x') = x \\cdot x'$ | Dados linearmente separáveis |\n",
    "| **Polinomial** | $K(x, x') = (x \\cdot x' + c)^d$ | Relações polinomiais |\n",
    "| **RBF (Gaussian)** | $K(x, x') = e^{-\\gamma \\|x - x'\\|^2}$ | Fronteiras complexas |\n",
    "| **Sigmoid** | $K(x, x') = \\tanh(\\alpha x \\cdot x' + c)$ | Casos específicos |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# Demonstração: Kernel Trick com Círculos Concêntricos\n",
    "# =================================================\n",
    "\n",
    "# Criar dados em círculos concêntricos (não-linearmente separáveis)\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Kernel Trick: Transformando Problemas Não-Lineares', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# ===== GRÁFICO 1: Dados Originais (2D) - NÃO linearmente separáveis =====\n",
    "ax1 = axes[0]\n",
    "\n",
    "class_0 = X_circles[y_circles == 0]\n",
    "class_1 = X_circles[y_circles == 1]\n",
    "\n",
    "ax1.scatter(class_0[:, 0], class_0[:, 1], \n",
    "            c='red', s=80, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Classe 0 (Interior)', marker='o')\n",
    "\n",
    "ax1.scatter(class_1[:, 0], class_1[:, 1],\n",
    "            c='blue', s=80, alpha=0.7, edgecolors='k', linewidth=1.5,\n",
    "            label='Classe 1 (Exterior)', marker='s')\n",
    "\n",
    "# Tentar traçar linha reta (vai falhar!)\n",
    "ax1.plot([-1.5, 1.5], [0, 0], 'gray', linestyle='--', linewidth=2, \n",
    "         alpha=0.5, label='Linha Reta (não funciona)')\n",
    "\n",
    "ax1.set_xlabel('Feature X₁', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Feature X₂', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Espaço Original (2D)', \n",
    "              fontsize=12, fontweight='bold', pad=15)\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# ===== GRÁFICO 2: SVM com Kernel RBF - Fronteira NÃO-LINEAR =====\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Treinar SVM com kernel RBF\n",
    "svm_rbf = SVC(kernel='rbf', gamma=2, C=1, random_state=42)\n",
    "svm_rbf.fit(X_circles, y_circles)\n",
    "\n",
    "# Criar grid para visualizar fronteira de decisão\n",
    "xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 200),\n",
    "                     np.linspace(-1.5, 1.5, 200))\n",
    "\n",
    "Z = svm_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotar fronteira de decisão\n",
    "ax2.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.3)\n",
    "ax2.contour(xx, yy, Z, levels=[0], linewidths=3, colors='green', \n",
    "            linestyles='-', label='Fronteira SVM (RBF)')\n",
    "\n",
    "# Plotar pontos\n",
    "ax2.scatter(class_0[:, 0], class_0[:, 1],\n",
    "            c='red', s=80, alpha=0.8, edgecolors='k', linewidth=1.5,\n",
    "            label='Classe 0 (Interior)', marker='o')\n",
    "\n",
    "ax2.scatter(class_1[:, 0], class_1[:, 1],\n",
    "            c='blue', s=80, alpha=0.8, edgecolors='k', linewidth=1.5,\n",
    "            label='Classe 1 (Exterior)', marker='s')\n",
    "\n",
    "# Destacar Support Vectors\n",
    "sv_mask = svm_rbf.support_\n",
    "ax2.scatter(X_circles[sv_mask, 0], X_circles[sv_mask, 1],\n",
    "            s=200, facecolors='none', edgecolors='yellow', linewidth=3,\n",
    "            label=f'Support Vectors ({len(sv_mask)})')\n",
    "\n",
    "ax2.set_xlabel('Feature X₁', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Feature X₂', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Com Kernel RBF',\n",
    "              fontsize=12, fontweight='bold', pad=15)\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResultados do SVM com Kernel RBF:\")\n",
    "print(f\"   • Acurácia: {svm_rbf.score(X_circles, y_circles)*100:.1f}%\")\n",
    "print(f\"   • Support Vectors: {len(svm_rbf.support_)} de {len(X_circles)} pontos ({len(svm_rbf.support_)/len(X_circles)*100:.1f}%)\")\n",
    "print(f\"\\nO Kernel RBF transformou o problema não-linear em linear em um espaço de maior dimensão.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**\n",
    "\n",
    "Agora que você já entendeu a **intuição** do SVM, vamos formalizar matematicamente.\n",
    "\n",
    "Não se assuste com as equações - elas apenas descrevem o que já vimos visualmente. Se preferir, você pode pular esta seção e ir direto para a [implementação com Scikit-Learn](#sklearn).\n",
    "\n",
    "---\n",
    "\n",
    "### **Objetivo do SVM**\n",
    "\n",
    "Queremos encontrar o hiperplano que **maximiza a margem** entre as classes.\n",
    "\n",
    "**Matematicamente:** Encontrar $\\mathbf{w}$ (vetor de pesos) e $b$ (bias) que definem o hiperplano:\n",
    "\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x} + b = 0$$\n",
    "\n",
    "**Onde:**\n",
    "- $\\mathbf{w}$: Vetor perpendicular ao hiperplano (define direção)\n",
    "- $b$: Deslocamento do hiperplano em relação à origem\n",
    "- $\\mathbf{x}$: Ponto no espaço de features\n",
    "\n",
    "---\n",
    "\n",
    "### **Função de Decisão**\n",
    "\n",
    "Para classificar um novo ponto $\\mathbf{x}$:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "$$f(\\mathbf{x}) = \\begin{cases}\n",
    "+1 & \\text{se } \\mathbf{w} \\cdot \\mathbf{x} + b > 0 \\quad \\text{(Classe Positiva)} \\\\\n",
    "-1 & \\text{se } \\mathbf{w} \\cdot \\mathbf{x} + b < 0 \\quad \\text{(Classe Negativa)}\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Margem**\n",
    "\n",
    "A distância de um ponto $\\mathbf{x}_i$ ao hiperplano é:\n",
    "\n",
    "$$\\text{distância} = \\frac{|\\mathbf{w} \\cdot \\mathbf{x}_i + b|}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "A **margem** é a distância mínima entre o hiperplano e os pontos de cada classe:\n",
    "\n",
    "$$\\text{margem} = \\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "**Objetivo:** Maximizar margem = Minimizar $\\|\\mathbf{w}\\|$\n",
    "\n",
    "---\n",
    "\n",
    "### **Problema de Otimização**\n",
    "\n",
    "Queremos:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "**Sujeito a:** \n",
    "$$y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i$$\n",
    "\n",
    "Onde $y_i \\in \\{-1, +1\\}$ é o rótulo verdadeiro do ponto $\\mathbf{x}_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Soft Margin (Margem Flexível)**\n",
    "\n",
    "Na prática, dados podem ter **overlap** (não são perfeitamente separáveis). Usamos **variáveis de folga** $\\xi_i$ (slack variables):\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "**Sujeito a:**\n",
    "$$y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\quad \\text{e} \\quad \\xi_i \\geq 0$$\n",
    "\n",
    "**Parâmetro C:**\n",
    "- **C alto**: Penaliza muito erros → margem menor, menos erros\n",
    "- **C baixo**: Tolera erros → margem maior, mais erros permitidos\n",
    "\n",
    "---\n",
    "\n",
    "### **Kernel Trick (Formalização)**\n",
    "\n",
    "Para problemas não-lineares, mapeamos dados para espaço de maior dimensão usando função kernel $K(\\mathbf{x}_i, \\mathbf{x}_j)$:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)$$\n",
    "\n",
    "**Kernels comuns:**\n",
    "- **Linear**: $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$\n",
    "- **Polinomial**: $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d$\n",
    "- **RBF**: $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$\n",
    "\n",
    "**Parâmetro γ (gamma) no RBF:**\n",
    "- **γ alto**: Influência local (risco de overfitting)\n",
    "- **γ baixo**: Influência ampla (risco de underfitting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Implementação com Scikit-Learn**\n",
    "\n",
    "Nesta seção, aplicaremos o SVM utilizando a biblioteca Scikit-Learn no dataset Titanic para prever a sobrevivência de passageiros.\n",
    "\n",
    "### **Problema**\n",
    "\n",
    "Dadas informações de um passageiro:\n",
    "- **Pclass**: Classe do bilhete (1ª, 2ª ou 3ª)\n",
    "- **Sex**: Sexo (masculino/feminino)\n",
    "- **Age**: Idade\n",
    "\n",
    "**Objetivo:** Prever sobrevivência (0 = Não sobreviveu, 1 = Sobreviveu)\n",
    "\n",
    "---\n",
    "\n",
    "### **Principais Classes do Scikit-Learn**\n",
    "\n",
    "| Classe | Descrição |\n",
    "|--------|-----------|\n",
    "| `SVC` | Support Vector Classification - Classificação |\n",
    "| `SVR` | Support Vector Regression - Regressão |\n",
    "| `LinearSVC` | SVM linear otimizado (mais rápido que SVC com kernel='linear') |\n",
    "\n",
    "---\n",
    "\n",
    "### **Parâmetros Principais do `SVC`**\n",
    "\n",
    "| Parâmetro | Valores | Descrição |\n",
    "|-----------|---------|-----------|\n",
    "| **kernel** | `'linear'`, `'poly'`, `'rbf'`, `'sigmoid'` | Tipo de kernel a utilizar |\n",
    "| **C** | float (default=1.0) | Parâmetro de regularização (C alto = margem menor, menos erros permitidos) |\n",
    "| **gamma** | `'scale'`, `'auto'`, float | Coeficiente do kernel RBF/poly (gamma alto = influência local) |\n",
    "| **degree** | int (default=3) | Grau do kernel polinomial |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# Carregando Dataset Titanic\n",
    "# =================================================\n",
    "\n",
    "# Carregar dataset\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"yasserh/titanic-dataset\",\n",
    "  \"Titanic-Dataset.csv\",\n",
    "  pandas_kwargs={\"encoding\": \"latin\", \"usecols\": [1, 2, 4, 5]}\n",
    ")\n",
    "\n",
    "# Preparar os dados\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "X = df[['Pclass', 'Sex', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 1. Básico - SVM Linear\n",
    "\n",
    "# Ideia:\n",
    "# - Normalizar features (MUITO IMPORTANTE para SVM)\n",
    "# - Treinar o modelo\n",
    "# =================================================\n",
    "\n",
    "# Normalizar os dados (crucial para SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM básico com kernel linear\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Linear - Acurácia: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Informações sobre o modelo\n",
    "print(f\"Número de Support Vectors: {svm.n_support_}\")\n",
    "print(f\"Total de Support Vectors: {len(svm.support_)}\")\n",
    "print(f\"Proporção de Support Vectors: {len(svm.support_)/len(X_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 2. Testando Diferentes Kernels\n",
    "\n",
    "# Kernels disponíveis no sklearn:\n",
    "# - 'linear': para dados linearmente separáveis\n",
    "# - 'poly': kernel polinomial\n",
    "# - 'rbf': Radial Basis Function (Gaussiano) - default\n",
    "# - 'sigmoid': kernel sigmoide\n",
    "# =================================================\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Treinar SVM com kernel específico\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Avaliar\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append((kernel, accuracy, len(svm.support_)))\n",
    "    print(f\"Kernel {kernel:8}: Acurácia = {accuracy*100:5.2f}%, Support Vectors = {len(svm.support_):3d}\")\n",
    "\n",
    "print(f\"\\nMelhor kernel: {max(results, key=lambda x: x[1])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 3. Parâmetros Importantes do SVM\n",
    "\n",
    "# C (default=1.0): Parâmetro de regularização\n",
    "# - C alto: Margem menor, menos tolerância a erros (pode causar overfitting)\n",
    "# - C baixo: Margem maior, mais tolerância a erros (pode causar underfitting)\n",
    "\n",
    "# gamma (para kernels RBF, poly, sigmoid): Define a influência de cada exemplo\n",
    "# - gamma alto: Influência apenas de pontos próximos (pode causar overfitting)  \n",
    "# - gamma baixo: Influência mais ampla (pode causar underfitting)\n",
    "# - 'scale' (default): 1 / (n_features * X.var())\n",
    "# - 'auto': 1 / n_features\n",
    "\n",
    "# degree (para kernel poly): Grau do polinômio\n",
    "\n",
    "# kernel: Tipo de kernel a ser usado\n",
    "# =================================================\n",
    "\n",
    "# Testar diferentes valores de C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "print(\"Testando parâmetro C (com kernel RBF):\")\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='rbf', C=C, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    accuracy = svm.score(X_test_scaled, y_test)\n",
    "    print(f\"C = {C:5.1f}: Acurácia = {accuracy*100:5.2f}%, Support Vectors = {len(svm.support_):3d}\")\n",
    "\n",
    "print(f\"\\nTestando parâmetro gamma (com kernel RBF, C=1):\")\n",
    "for gamma in gamma_values:\n",
    "    svm = SVC(kernel='rbf', C=1, gamma=gamma, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    accuracy = svm.score(X_test_scaled, y_test)\n",
    "    print(f\"gamma = {str(gamma):5}: Acurácia = {accuracy*100:5.2f}%, Support Vectors = {len(svm.support_):3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 4. Otimização de Hiperparâmetros com Grid Search\n",
    "# =================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Definir espaço de busca\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Grid search com validação cruzada\n",
    "svm_grid = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm_grid, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Melhor score CV: {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_scaled)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Acurácia no teste: {accuracy_best*100:.2f}%\")\n",
    "\n",
    "# Relatório detalhado\n",
    "print(f\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 5. Interpretação dos Resultados\n",
    "\n",
    "# Vamos analisar as predições para alguns casos específicos\n",
    "# =================================================\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Casos de teste específicos\n",
    "test_cases = [\n",
    "    [1, 1, 25],  # 1ª classe, feminino, 25 anos - deveria sobreviver\n",
    "    [3, 0, 22],  # 3ª classe, masculino, 22 anos - provavelmente não sobrevive\n",
    "    [2, 1, 35],  # 2ª classe, feminino, 35 anos - provavelmente sobrevive\n",
    "    [3, 1, 5],   # 3ª classe, feminino, 5 anos - criança, provavelmente sobrevive\n",
    "]\n",
    "\n",
    "print(\"Predições para casos específicos:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    # Normalizar o caso de teste\n",
    "    case_scaled = scaler.transform([case])\n",
    "    \n",
    "    # Fazer predição\n",
    "    prediction = best_svm.predict(case_scaled)[0]\n",
    "    confidence = best_svm.decision_function(case_scaled)[0]\n",
    "    \n",
    "    # Mapear características\n",
    "    class_names = {1: \"1ª classe\", 2: \"2ª classe\", 3: \"3ª classe\"}\n",
    "    sex_names = {0: \"Masculino\", 1: \"Feminino\"}\n",
    "    result_names = {0: \"Não Sobrevive\", 1: \"Sobrevive\"}\n",
    "    \n",
    "    print(f\"Caso {i+1}:\")\n",
    "    print(f\"  Classe: {class_names[case[0]]}\")\n",
    "    print(f\"  Sexo: {sex_names[case[1]]}\")\n",
    "    print(f\"  Idade: {case[2]} anos\")\n",
    "    print(f\"  Predição: {result_names[prediction]}\")\n",
    "    print(f\"  Confiança: {abs(confidence):.3f}\")\n",
    "    print(f\"  {'*' * 40}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Não Sobrevive', 'Sobrevive'],\n",
    "            yticklabels=['Não Sobrevive', 'Sobrevive'])\n",
    "plt.title('Matriz de Confusão - SVM Otimizado')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predição')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vantagens-desvantagens'></a>\n",
    "## **Vantagens e Desvantagens**\n",
    "\n",
    "### **Vantagens**\n",
    "\n",
    "1. **Maximização de margem**: Otimiza separação entre classes, resultando em fronteiras robustas\n",
    "2. **Alta dimensionalidade**: Eficaz mesmo quando número de features excede número de amostras\n",
    "3. **Eficiência computacional**: Utiliza apenas vetores de suporte, não todos os dados de treinamento\n",
    "4. **Versatilidade**: Diferentes kernels permitem capturar relacionamentos complexos\n",
    "5. **Robustez**: Menos propenso a overfitting em alta dimensionalidade\n",
    "6. **Fundamento teórico**: Baseado em teoria de otimização e aprendizado estatístico\n",
    "\n",
    "### **Desvantagens**\n",
    "\n",
    "1. **Sensibilidade à escala**: Features em escalas diferentes afetam resultado - normalizar com `StandardScaler`\n",
    "2. **Complexidade computacional**: $O(n^2)$ a $O(n^3)$ em datasets grandes - usar `LinearSVC` ou `SGDClassifier`\n",
    "3. **Ajuste de hiperparâmetros**: Requer busca extensa para C, γ, kernel - utilizar `GridSearchCV`\n",
    "4. **Probabilidades**: Não retorna probabilidades diretamente - usar `probability=True` (custo computacional)\n",
    "5. **Classificação multiclasse**: SVM é binário nativamente - Sklearn implementa one-vs-one automaticamente\n",
    "6. **Interpretabilidade**: Dificuldade em entender importância de features - usar `LinearSVC` com atributo `coef_`\n",
    "\n",
    "---\n",
    "\n",
    "### **Quando usar SVM?**\n",
    "\n",
    "**Casos recomendados:**\n",
    "- Fronteiras de decisão não-lineares complexas\n",
    "- Dados de alta dimensionalidade (muitas features)\n",
    "- Precisão prioritária sobre interpretabilidade\n",
    "- Datasets pequenos a médios (< 10.000 amostras)\n",
    "- Necessidade de robustez contra overfitting\n",
    "\n",
    "**Casos não recomendados:**\n",
    "- Necessidade de probabilidades calibradas - preferir Regressão Logística\n",
    "- Datasets muito grandes (> 100.000 amostras) - preferir Naive Bayes ou Regressão Logística\n",
    "- Requisitos de interpretabilidade - preferir Árvores de Decisão ou Regressão Logística\n",
    "- Features em escalas muito diferentes sem possibilidade de normalização\n",
    "\n",
    "---\n",
    "\n",
    "### **Aplicações**\n",
    "\n",
    "| Área | Exemplos |\n",
    "|------|----------|\n",
    "| Processamento de texto | Classificação de sentimentos, detecção de spam, categorização de documentos |\n",
    "| Visão computacional | Reconhecimento facial, detecção de objetos, OCR |\n",
    "| Bioinformática | Classificação de proteínas, predição de genes |\n",
    "| Finanças | Detecção de fraude, análise de crédito |\n",
    "| Medicina | Diagnóstico de doenças, classificação de tumores |\n",
    "\n",
    "---\n",
    "\n",
    "### **Recomendações Práticas**\n",
    "\n",
    "1. Normalizar features com `StandardScaler` antes do treinamento\n",
    "2. Iniciar com kernel RBF (funcionalidade adequada para maioria dos problemas)\n",
    "3. Utilizar Grid Search para otimização de hiperparâmetros:\n",
    "   ```python\n",
    "   param_grid = {\n",
    "       'C': [0.1, 1, 10, 100],\n",
    "       'gamma': ['scale', 0.001, 0.01, 0.1, 1],\n",
    "       'kernel': ['rbf', 'linear']\n",
    "   }\n",
    "   ```\n",
    "4. Para datasets grandes, preferir `LinearSVC`\n",
    "5. Para probabilidades, usar `SVC(probability=True)` considerando custo computacional adicional\n",
    "6. Em problemas multiclasse, SVM utiliza estratégia one-vs-one automaticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Recursos Adicionais**\n",
    "\n",
    "**Documentação Oficial:**\n",
    "- [Scikit-Learn: SVM](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [SVC API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- [SVR API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "**Tutoriais e Artigos:**\n",
    "- [The Kernel Trick](https://medium.com/data-science/the-kernel-trick-c98cdbcaeb3f)\n",
    "- [Understanding Support Vector Machine](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n",
    "- [SVM Tutorial](https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python)\n",
    "\n",
    "**Vídeos Recomendados:**\n",
    "- [StatQuest: Support Vector Machines](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "- [SVM with Polynomial Kernel Visualization](https://www.youtube.com/watch?v=3liCbRZPrZA)\n",
    "\n",
    "**Datasets:**\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumo**\n",
    "\n",
    "Neste notebook foram abordados:\n",
    "\n",
    "- Conceitos fundamentais do SVM (hiperplano, margem, vetores de suporte)\n",
    "- Funcionamento em 3 etapas (hiperplano, maximização de margem, support vectors)\n",
    "- Kernel trick para tratamento de não-linearidade\n",
    "- Formalização matemática (hiperplanos, margens, otimização)\n",
    "- Implementação prática com Scikit-Learn\n",
    "- Vantagens e limitações do algoritmo\n",
    "\n",
    "**Conceitos principais:**\n",
    "- **Hiperplano**: Fronteira que separa classes\n",
    "- **Margem**: Distância entre hiperplano e pontos mais próximos\n",
    "- **Support Vectors**: Pontos críticos que definem a fronteira\n",
    "- **Kernel Trick**: Mapeamento para espaço de maior dimensão\n",
    "- **Parâmetro C**: Controla trade-off entre margem e erros\n",
    "- **Parâmetro γ**: Controla influência de cada ponto (kernel RBF)\n",
    "\n",
    "---\n",
    "\n",
    "<-- [**Anterior: Regressão Logística**](03_regressao_logistica.ipynb) | [**Próximo: Árvores de Decisão**](05_arvores_classificacao.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
