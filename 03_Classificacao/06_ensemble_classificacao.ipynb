{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Métodos de Ensemble — Módulo 3, Notebook 6/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao Problema](#introducao)\n",
    "2. [Bagging: Bootstrap Aggregating](#bagging)\n",
    "   - [Intuição Prática](#bagging-intuicao)\n",
    "   - [Formulação Formal](#bagging-formal)\n",
    "   - [Implementação Manual](#bagging-manual)\n",
    "3. [Boosting: AdaBoost](#boosting)\n",
    "   - [Formulação Formal](#boosting-formal)\n",
    "   - [Implementação com Sklearn](#boosting-sklearn)\n",
    "4. [Stacking: Combinando Modelos Diversos](#stacking)\n",
    "   - [Implementação com Sklearn](#stacking-sklearn)\n",
    "   - [Desafio: Criar um Ensemble Híbrido](#stacking-desafio)\n",
    "5. [Comparação: Bagging vs Boosting vs Stacking](#comparacao)\n",
    "6. [Considerações Finais](#consideracoes)\n",
    "7. [Resumo e Próximos Passos](#resumo)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao Problema: Por Que Um Único Modelo Não É Suficiente?**\n",
    "\n",
    "Vamos fazer um experimento. Imagine que você está construindo um modelo para prever se um cliente vai cancelar sua assinatura (churn).\n",
    "\n",
    "**Cenário 1: Uma única Árvore de Decisão**\n",
    "\n",
    "Você treina uma árvore de decisão e obtém 85% de acurácia no conjunto de teste. Ótimo, certo?\n",
    "\n",
    "Mas espera... imagina que isso é o que acontece (e normalmente será) se treinar a mesma árvore com pequenas variações nos dados:\n",
    "\n",
    "- **Tentativa 1**: 85% de acurácia\n",
    "- **Tentativa 2**: 82% de acurácia (removemos 5% dos dados aleatoriamente)\n",
    "- **Tentativa 3**: 88% de acurácia (diferentes dados de treino)\n",
    "- **Tentativa 4**: 79% de acurácia\n",
    "- **Tentativa 5**: 86% de acurácia\n",
    "\n",
    "**Problemas:**\n",
    "\n",
    "1. **Alta Variância**: Pequenas mudanças nos dados causam grandes variações no modelo\n",
    "2. **Overfitting**: O modelo pode estar \"decorando\" os dados de treino\n",
    "3. **Instabilidade**: Não podemos confiar plenamente em suas previsões\n",
    "\n",
    "**A Solução dos Ensembles:**\n",
    "\n",
    "Em vez de confiar em uma única árvore, será que faz sentido treinar várias árvores diferentes e combinar suas previsões?\n",
    "\n",
    "- Se 8 de 10 árvores dizem \"cliente vai cancelar\", provavelmente ele vai..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging'></a>\n",
    "## **Bagging: Bootstrap Aggregating**\n",
    "\n",
    "**O que é Bagging?**\n",
    "\n",
    "Bagging (Bootstrap Aggregating) é como pedir a opinião de várias pessoas que estudaram o mesmo assunto, mas com materiais ligeiramente diferentes.\n",
    "\n",
    "**A Ideia:**\n",
    "\n",
    "1. Pegamos nosso conjunto de dados original\n",
    "2. Criamos várias \"versões\" diferentes dele (amostras bootstrap)\n",
    "3. Treinamos um modelo em cada versão\n",
    "4. Combinamos as previsões\n",
    "\n",
    "**Bootstrap?**\n",
    "\n",
    "Bootstrap é uma técnica de reamostragem com substituição. Imagine que você tem uma sacola com 100 bolas numeradas:\n",
    "\n",
    "- Você retira uma bola, anota o número, e **devolve a bola** na sacola\n",
    "- Repete isso 100 vezes\n",
    "- Resultado: alguns números aparecem várias vezes, outros não aparecem\n",
    "\n",
    "Isso cria uma versão \"similar, mas diferente\" do conjunto original! Retornaremos nisso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging-intuicao'></a>\n",
    "### **Intuição Prática: Bagging com o Dataset Titanic**\n",
    "\n",
    "Vamos usar o dataset do Titanic para entender o Bagging na prática.\n",
    "\n",
    "**Mesmo Problema**: Prever se um passageiro sobreviveu ao naufrágio.\n",
    "\n",
    "**Abordagem com uma única Árvore de Decisão:**\n",
    "- Treina em todos os dados\n",
    "- Pode fazer overfitting\n",
    "- Instável com mudanças nos dados\n",
    "\n",
    "**Abordagem com Bagging:**\n",
    "- Cria 10 versões diferentes do dataset (bootstrap)\n",
    "- Treina uma árvore em cada versão\n",
    "- Cada árvore vota: \"Sobreviveu\" ou \"Não sobreviveu\"\n",
    "- Decisão final: maioria dos votos\n",
    "\n",
    "Vamos ver isso acontecer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# IMPORTANDO AS BIBLIOTECAS\n",
    "# =======================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuração de visualização (ignora, estou testando novas visualizações)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Carregando o Dataset do Titanic\n",
    "# =======================================================\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"yasserh/titanic-dataset\",\n",
    "    \"Titanic-Dataset.csv\",\n",
    "    pandas_kwargs={\"encoding\": \"latin\", \"usecols\": [1, 2, 4, 5]}\n",
    ")\n",
    "\n",
    "# Pré-processamento\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "X = df[['Pclass', 'Sex', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(X_train)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Passo 1: Treinar uma ÚNICA Árvore de Decisão\n",
    "# =======================================================\n",
    "\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_single = single_tree.predict(X_test)\n",
    "metrics_single = classification_report(y_test, y_pred_single)\n",
    "\n",
    "print(f\"Métricas de uma única árvore:\\n{metrics_single}\")\n",
    "\n",
    "# Vamos ver como essa árvore se comporta com diferentes amostras\n",
    "print(\"\\nTestando estabilidade (treinando em diferentes subconjuntos):\")\n",
    "accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Criar uma amostra bootstrap\n",
    "    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_boot = X_train[indices]\n",
    "    y_boot = y_train[indices]\n",
    "    \n",
    "    # Treinar árvore\n",
    "    tree = DecisionTreeClassifier(random_state=i)\n",
    "    tree.fit(X_boot, y_boot)\n",
    "    \n",
    "    # Avaliar\n",
    "    acc = accuracy_score(y_test, tree.predict(X_test))\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Árvore {i+1}: {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nMédia: {np.mean(accuracies)*100:.2f}%\")\n",
    "print(f\"Desvio padrão: {np.std(accuracies)*100:.2f}%\")\n",
    "print(f\"\\nInterpretação: A acurácia varia entre {min(accuracies)*100:.2f}% e {max(accuracies)*100:.2f}%\")\n",
    "print(\"nota: Cada árvore 'aprendeu' algo diferente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Passo 2: Implementar Bagging Manualmente\n",
    "# =======================================================\n",
    "seeds = np.arange(10)\n",
    "\n",
    "# Armazenar os modelos\n",
    "models = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Treinando ensemble com Bagging...\")\n",
    "for i in seeds:\n",
    "    # 1. Criar amostra bootstrap\n",
    "    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_boot = X_train[indices]\n",
    "    y_boot = y_train[indices]\n",
    "    \n",
    "    # 2. Treinar modelo na amostra bootstrap\n",
    "    tree = DecisionTreeClassifier(random_state=i)\n",
    "    tree.fit(X_boot, y_boot)\n",
    "    models.append(tree)\n",
    "    \n",
    "    # 3. Fazer previsões\n",
    "    pred = tree.predict(X_test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    print(f\"Árvore {i+1} treinada em {len(np.unique(indices))} amostras únicas (de {len(X_train)} possíveis)\")\n",
    "\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(f\"\\nShape das previsões: {predictions.shape}\")\n",
    "print(f\"({seeds.size} árvores, {len(X_test)} amostras de teste)\")\n",
    "\n",
    "# =======================================================\n",
    "# Passo 3: Combinar previsões por votação majoritária\n",
    "# =======================================================\n",
    "\n",
    "# Para cada amostra de teste, contar os votos\n",
    "final_predictions = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    votes = predictions[:, i]  # Votos de todas as árvores para a amostra i\n",
    "    # Votação majoritária\n",
    "    prediction = np.bincount(votes).argmax()\n",
    "    final_predictions.append(prediction)\n",
    "\n",
    "final_predictions = np.array(final_predictions)\n",
    "\n",
    "# Avaliar\n",
    "metrics_bagging = classification_report(y_test, final_predictions)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTADOS:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Métricas com Bagging ({seeds.size} árvores):\\n{metrics_bagging}\")\n",
    "metric = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Acurácia com Bagging ({seeds.size} árvores): {metric*100:.2f}%\")\n",
    "print(f\"Melhoria: {(metric - accuracy_score(y_test, y_pred_single))*100:.2f} pontos percentuais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Visualização: Como cada árvore vota\n",
    "# =======================================================\n",
    "\n",
    "# Vamos ver as previsões para as primeiras 10 amostras de teste\n",
    "n_samples_to_show = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Criar matriz de votos\n",
    "vote_matrix = predictions[:, :n_samples_to_show].T\n",
    "\n",
    "# Plotar heatmap\n",
    "im = ax.imshow(vote_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Configurar eixos\n",
    "ax.set_xlabel('Árvore', fontsize=12)\n",
    "ax.set_ylabel('Amostra de Teste', fontsize=12)\n",
    "ax.set_title('Votação de Cada Árvore (0 = Não Sobreviveu, 1 = Sobreviveu)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(seeds.size))\n",
    "ax.set_xticklabels([f'Árvore {i+1}' for i in range(seeds.size)], rotation=45)\n",
    "ax.set_yticks(range(n_samples_to_show))\n",
    "ax.set_yticklabels([f'Amostra {i+1}' for i in range(n_samples_to_show)])\n",
    "ax.grid(False)\n",
    "\n",
    "# Adicionar valores nas células\n",
    "for i in range(n_samples_to_show):\n",
    "    for j in range(seeds.size):\n",
    "        text = ax.text(j, i, int(vote_matrix[i, j]), \n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "\n",
    "# Adicionar informação sobre a votação final\n",
    "for i in range(n_samples_to_show):\n",
    "    votes = vote_matrix[i]\n",
    "    final_vote = \"Sobreviveu\" if np.sum(votes) > seeds.size/2 else \"Não Sobreviveu\"\n",
    "    vote_count = int(np.sum(votes))\n",
    "    ax.text(seeds.size, i, f'{final_vote}\\n({vote_count}/{seeds.size})', \n",
    "           ha=\"left\", va=\"center\", fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretação:\")\n",
    "print(\"- Verde: Árvore previu 'Sobreviveu' (1)\")\n",
    "print(\"- Vermelho: Árvore previu 'Não Sobreviveu' (0)\")\n",
    "print(\"- Decisão final: maioria dos votos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging-formal'></a>\n",
    "### **Formulação Formal do Bagging**\n",
    "\n",
    "**Entrada:**\n",
    "\n",
    "1. Conjunto de treinamento: $D = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "2. Algoritmo de aprendizado base $\\mathcal{A}$ (e.g., Árvore de Decisão)\n",
    "\n",
    "3. Número de modelos no ensemble: $T$\n",
    "\n",
    "**Processamento:**\n",
    "\n",
    "Para cada modelo $t = 1, 2, ..., T$:\n",
    "\n",
    "1. **Criar amostra bootstrap** $D_t$:\n",
    "   - Amostrar $n$ exemplos de $D$ com substituição\n",
    "   - Aproximadamente 63.2% das amostras originais aparecem em $D_t$\n",
    "   - As amostras restantes (~36.8%) são chamadas de \"out-of-bag\" (OOB), (por quê?)\n",
    "\n",
    "2. **Treinar modelo base**:\n",
    "   - $h_t = \\mathcal{A}(D_t)$\n",
    "   - Cada modelo $h_t$ é treinado independentemente\n",
    "\n",
    "**Saída - Predição:**\n",
    "\n",
    "Para uma nova amostra $x^o$:\n",
    "\n",
    "- votação majoritária:\n",
    "\n",
    "$$\\hat{y} = \\text{moda}\\{h_1(x^o), h_2(x^o), ..., h_T(x^o)\\}$$\n",
    "\n",
    "**Propriedades Importantes:**\n",
    "\n",
    "1. **Redução de Variância**: \n",
    "   - Se os modelos base têm variância $\\sigma^2$, a variância do ensemble é aproximadamente $\\frac{\\sigma^2}{T}$ (assumindo independência)\n",
    "   - Na prática, $\\frac{\\sigma^2}{T}$ nunca acontece porque há correlação entre modelos, mas de fato a variância é reduzida\n",
    "\n",
    "2. **Out-of-Bag Error (OOB)**:\n",
    "   - Para cada amostra de treino, podemos usar os modelos que NÃO a viram no treino para estimar o erro\n",
    "   - Isso fornece uma estimativa de validação \"gratuita\" sem precisar de um conjunto de validação separado\n",
    "   e.g\n",
    "\n",
    "   Suponha $n=5$ amostras e $T=3$ modelos:\n",
    "\n",
    "   - Modelo 1 treinou com amostras: {1, 2, 3, 5} → amostra 4 está OOB\n",
    "   - Modelo 2 treinou com amostras: {1, 2, 4, 5} → amostra 3 está OOB  \n",
    "   - Modelo 3 treinou com amostras: {2, 3, 4, 5} → amostra 1 está OOB\n",
    "\n",
    "   Para avaliar a amostra 4:\n",
    "   - Use apenas Modelo 1 (que não viu a amostra 4)\n",
    "   - $\\hat{y}^{OOB}_4 = h_1(x^{(4)})$\n",
    "\n",
    "   Para avaliar a amostra 1:\n",
    "   - Use apenas Modelo 3 (que não viu a amostra 1)\n",
    "   - $\\hat{y}^{OOB}_1 = h_3(x^{(1)})$\n",
    "\n",
    "\n",
    "3. **Bias-Variance Tradeoff**:\n",
    "   - O grande dilema!\n",
    "   - Por isso, funciona melhor com modelos de baixo bias e alta variância (como árvores mais profundas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging-manual'></a>\n",
    "### **Desafio: Implementar Bagging do Zero**\n",
    "\n",
    "Esse é um pouco mais complicado, mas tente implementar uma classe `BaggingClassifier` que funcione com qualquer classificador base.\n",
    "\n",
    "**Requisitos:**\n",
    "\n",
    "1. Deve aceitar qualquer classificador como base (DecisionTree, KNN, etc.)\n",
    "2. Criar amostras bootstrap\n",
    "3. Treinar múltiplos modelos\n",
    "4. Combinar previsões por votação\n",
    "5. Calcular OOB error (desafio extra, se pra ti estiver facil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, base_estimator, n_estimators=10, random_state=None):\n",
    "        \"\"\"\n",
    "        Inicializa o BaggingClassifier\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        base_estimator: classificador base (e.g., DecisionTreeClassifier())\n",
    "        n_estimators: número de modelos no ensemble\n",
    "        random_state: seed para reprodutibilidade\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _create_bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Cria uma amostra bootstrap de X e y\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        X_boot, y_boot: amostra bootstrap\n",
    "        oob_indices: índices das amostras out-of-bag\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Treina o ensemble\n",
    "        \n",
    "        Processo:\n",
    "        1. Para cada estimador:\n",
    "           a. Criar amostra bootstrap\n",
    "           b. Clonar o estimador base\n",
    "           c. Treinar o clone na amostra bootstrap\n",
    "           d. Armazenar o modelo e os índices OOB\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Faz previsões usando votação majoritária\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        y_pred: array de previsões\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcula a acurácia do modelo\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def oob_score(self):\n",
    "        \"\"\"\n",
    "        DESAFIO EXTRA: Calcular o OOB error\n",
    "        \n",
    "        Para cada amostra de treino:\n",
    "        - Usar apenas os modelos que NÃO a viram no treino\n",
    "        - Fazer votação com esses modelos\n",
    "        - Comparar com o label verdadeiro\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        oob_accuracy: acurácia calculada nas amostras OOB\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting'></a>\n",
    "## **Boosting: AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "**O que é Boosting?**\n",
    "\n",
    "Boosting é muito diferente de Bagging. Em vez de treinar modelos independentemente, **cada modelo foca nos erros do modelo anterior**.\n",
    "\n",
    "Imagine um professor ajudando um aluno:\n",
    "- Sessão 1: Ensina os conceitos básicos\n",
    "- Sessão 2: Foca nos tópicos que o aluno errou\n",
    "- Sessão 3: Foca mais nos erros anteriores\n",
    "- ...E assim por diante, até que o aluno dominar\n",
    "\n",
    "**A Ideia do AdaBoost:**\n",
    "\n",
    "1. Treinamos um modelo fraco no dataset\n",
    "2. Aumentamos o peso das amostras que ele errou\n",
    "3. Treinamos outro modelo fraco com ênfase nos erros anteriores\n",
    "4. Repetimos e combinamos com votação ponderada\n",
    "\n",
    "**Modelo Fraco?**\n",
    "\n",
    "Um \"modelo fraco\" (weak learner) é um modelo que tem acurácia ligeiramente melhor que 50% (melhor que chute aleatório).\n",
    "\n",
    "- **Exemplo**: Uma árvore de decisão com profundidade 1 (decision stump)\n",
    "- **Não precisa** ser complexo, ao contrário! Modelos simples funcionam melhor com boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# =======================================================\n",
    "# 1. BaggingClassifier com Árvores de Decisão\n",
    "# =======================================================\n",
    "\n",
    "bagging_tree = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    oob_score=True  # Calcular OOB score\n",
    ")\n",
    "\n",
    "bagging_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BAGGING COM ÁRVORES DE DECISÃO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia no teste: {bagging_tree.score(X_test, y_test)*100:.2f}%\")\n",
    "print(f\"OOB Score: {bagging_tree.oob_score_*100:.2f}%\")\n",
    "print(f\"Número de estimadores: {bagging_tree.n_estimators}\")\n",
    "\n",
    "# =======================================================\n",
    "# 2. Random Forest (Bagging + Feature Randomness)\n",
    "# =======================================================\n",
    "\n",
    "# Random Forest é um tipo especial de Bagging que:\n",
    "# - Usa árvores de decisão como base\n",
    "# - Adiciona aleatoriedade na seleção de features\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST (Bagging + Feature Randomness)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia no teste: {random_forest.score(X_test, y_test)*100:.2f}%\")\n",
    "print(f\"OOB Score: {random_forest.oob_score_*100:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['Pclass', 'Sex', 'Age']\n",
    "importances = random_forest.feature_importances_\n",
    "print(\"\\nImportância das features:\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"  {name}: {importance*100:.2f}%\")\n",
    "\n",
    "# =======================================================\n",
    "# 3. Bagging com outros classificadores\n",
    "# =======================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAGGING COM DIFERENTES CLASSIFICADORES BASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# KNN\n",
    "bagging_knn = BaggingClassifier(\n",
    "    estimator=KNeighborsClassifier(n_neighbors=5),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_knn.fit(X_train, y_train)\n",
    "print(f\"\\nBagging + KNN: {bagging_knn.score(X_test, y_test)*100:.2f}%\")\n",
    "\n",
    "# Naive Bayes\n",
    "bagging_nb = BaggingClassifier(\n",
    "    estimator=GaussianNB(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_nb.fit(X_train, y_train)\n",
    "print(f\"Bagging + Naive Bayes: {bagging_nb.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Parâmetros Importantes do BaggingClassifier\n",
    "# =======================================================\n",
    "\n",
    "# n_estimators: número de modelos base\n",
    "# max_samples: número/proporção de amostras para cada bootstrap\n",
    "# max_features: número/proporção de features para cada modelo\n",
    "# bootstrap: se True, usa bootstrap; se False, usa todo dataset\n",
    "# oob_score: se True, calcula OOB score\n",
    "\n",
    "configs = [\n",
    "    {'n_estimators': 10, 'max_samples': 1.0, 'max_features': 1.0},\n",
    "    {'n_estimators': 50, 'max_samples': 0.8, 'max_features': 1.0},\n",
    "    {'n_estimators': 100, 'max_samples': 1.0, 'max_features': 0.8},\n",
    "    {'n_estimators': 50, 'max_samples': 0.7, 'max_features': 0.7},\n",
    "]\n",
    "\n",
    "print(\"Testando diferentes configurações:\\n\")\n",
    "for i, config in enumerate(configs, 1):\n",
    "    bag = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        random_state=42\n",
    "    )\n",
    "    bag.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"{i}. n_estimators={config['n_estimators']}, \"\n",
    "          f\"max_samples={config['max_samples']}, \"\n",
    "          f\"max_features={config['max_features']}\")\n",
    "    print(f\"   Acurácia: {bag.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Adicione o OOB score e veja o que acontece- Como você faria uma validação cruzada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica: Random Forest vs Bagging**\n",
    "\n",
    "- **BaggingClassifier com árvores**: Cada árvore vê TODAS as features em cada divisão\n",
    "- **RandomForest**: Cada divisão da árvore vê apenas um subconjunto aleatório de features\n",
    "\n",
    "Por que isso importa?\n",
    "- Random Forest cria árvores mais **decorrelacionadas**\n",
    "- Resultado: geralmente melhor performance que Bagging puro\n",
    "- Trade-off: um pouco mais de bias, mas muito menos variância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Boosting: Aprendizado Sequencial**\n",
    "\n",
    "**O que é Boosting?**\n",
    "\n",
    "- **Bagging**: Modelos treinados em **paralelo**, independentemente\n",
    "  - \"Todos vocês, estudem o material e me digam o que acham\"\n",
    "\n",
    "- **Boosting**: Modelos treinados em **sequência**, cada um focando nos erros do anterior\n",
    "  - \"Você 1, estude tudo. Você 2, foque no que o 1 errou. Você 3, foque no que os anteriores erraram...\"\n",
    "\n",
    "**A ideia:**\n",
    "\n",
    "1. Treinar um modelo \"fraco\" (weak learner)\n",
    "2. Identificar em quais amostras ele errou\n",
    "3. Dar mais **peso/atenção** a essas amostras difíceis\n",
    "4. Treinar um novo modelo focado nelas\n",
    "5. Repetir o processo\n",
    "6. Combinar todos os modelos, dando mais peso aos melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intuição do AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "AdaBoost foi um dos primeiros e mais populares algoritmos de boosting. Vamos entender como funciona!\n",
    "\n",
    "**O Processo:**\n",
    "\n",
    "1. **Início**: Todas as amostras têm peso igual (e.g., 1/n cada)\n",
    "\n",
    "2. **Treinar modelo 1**: \n",
    "   - Treina em todas as amostras\n",
    "   - Identifica quais errou\n",
    "\n",
    "3. **Ajustar pesos**:\n",
    "   - ↑ Aumenta peso das amostras que errou\n",
    "   - ↓ Diminui peso das que acertou\n",
    "\n",
    "4. **Treinar modelo 2**:\n",
    "   - Agora \"presta mais atenção\" nas amostras difíceis\n",
    "   - Tem que acertar as que o modelo 1 errou\n",
    "\n",
    "5. **Repetir** o processo T vezes\n",
    "\n",
    "6. **Combinar**: Cada modelo tem um peso baseado em sua acurácia\n",
    "   - Modelos melhores têm mais influência na decisão final\n",
    "\n",
    "Vamos ver isso na prática!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Simulação Visual do AdaBoost\n",
    "\n",
    "# ideia:\n",
    "# - Mostrar como os pesos das amostras mudam a cada iteração\n",
    "# - Mostrar como os erros são tratados\n",
    "# - Mostrar a importância (alpha) de cada modelo \n",
    "# * Não se preocupe com o cálculo exato ainda\n",
    "# =======================================================\n",
    "\n",
    "# Dataset 2D simples\n",
    "np.random.seed(42)\n",
    "n_samples = 40 \n",
    "X_viz = np.random.randn(n_samples, 2)  \n",
    "y_viz = (X_viz[:, 0] + X_viz[:, 1] > 0).astype(int)\n",
    "y_viz = 2 * y_viz - 1  # Converter para {-1, +1}\n",
    "\n",
    "# Inicializar\n",
    "weights = np.ones(n_samples) / n_samples\n",
    "n_iterations = 3\n",
    "models = []\n",
    "alphas = []\n",
    "all_weights = []\n",
    "\n",
    "# Treinar AdaBoost\n",
    "for t in range(n_iterations):\n",
    "    all_weights.append(weights.copy())\n",
    "    \n",
    "    # Treinar weak learner\n",
    "    weak_learner = DecisionTreeClassifier(max_depth=1, random_state=t)\n",
    "    weak_learner.fit(X_viz, y_viz, sample_weight=weights)\n",
    "    predictions = weak_learner.predict(X_viz)\n",
    "    \n",
    "    # Calcular erro e alpha\n",
    "    incorrect = predictions != y_viz\n",
    "    error = np.sum(weights * incorrect) / np.sum(weights)\n",
    "    alpha = 0.5 * np.log((1 - error) / (error))\n",
    "    \n",
    "    # Atualizar pesos\n",
    "    weights = weights * np.exp(-alpha * y_viz * predictions)\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    models.append(weak_learner)\n",
    "    alphas.append(alpha)\n",
    "\n",
    "# Visualização com 3 iterações\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for t in range(n_iterations):\n",
    "    ax = axes[t]\n",
    "    weights_t = all_weights[t]\n",
    "    predictions = models[t].predict(X_viz)\n",
    "    incorrect = predictions != y_viz\n",
    "\n",
    "    sizes = 500 * weights_t / weights_t.max()\n",
    "    \n",
    "    # corretos\n",
    "    correct = ~incorrect\n",
    "    ax.scatter(X_viz[correct, 0], X_viz[correct, 1], \n",
    "              c=y_viz[correct], s=sizes[correct], \n",
    "              alpha=0.7, cmap='seismic', edgecolors='k', linewidth=2)\n",
    "    \n",
    "    # errados\n",
    "    if incorrect.sum() > 0:\n",
    "        ax.scatter(X_viz[incorrect, 0], X_viz[incorrect, 1], \n",
    "                  s=sizes[incorrect]*1.5, marker='X', \n",
    "                  c=predictions[incorrect],\n",
    "                  cmap='seismic', edgecolors='r', linewidth=1, zorder=5, alpha=0.9)\n",
    "    \n",
    "    # Título\n",
    "    ax.set_title(f'Modelo {t+1}\\n(Alpha = {alphas[t]:.2f})', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.1)\n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    if t == 0:\n",
    "        ax.set_ylabel('Feature 2', fontsize=12)\n",
    "\n",
    "plt.suptitle('AdaBoost\\n(Tamanho = Peso)', \n",
    "            fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETAÇÃO:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Pontos MAIORES = amostras com MAIS peso\")\n",
    "print(\"X = erros -> cor = classe prevista\")\n",
    "print(\"  - X azul = modelo previu -1 (mas era +1)\")\n",
    "print(\"  - X vermelho = modelo previu +1 (mas era -1)\")\n",
    "print(\"A cada iteração, o modelo foca nos erros anteriores\")\n",
    "print(f\"\\nQuantidade de erros por modelo: {[int((models[t].predict(X_viz) != y_viz).sum()) for t in range(n_iterations)]}\")\n",
    "print(f\"\\nImportância dos modelos (alphas): {[f'{a:.2f}' for a in alphas]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting-formal'></a>\n",
    "### **Formulação Formal do AdaBoost**\n",
    "\n",
    "**Entrada:**\n",
    "\n",
    "1. Conjunto de treinamento: $D = \\{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\\}$ onde $y^{(i)} \\in \\{-1, +1\\}$\n",
    "\n",
    "2. Algoritmo de aprendizado fraco $\\mathcal{A}$\n",
    "\n",
    "3. Número de iterações: $T$\n",
    "\n",
    "**Processamento:**\n",
    "\n",
    "**Inicialização:**\n",
    "- Pesos uniformes: $w^{(1)}_i = \\frac{1}{n}$ para $i = 1, ..., n$\n",
    "\n",
    "**Para cada iteração** $t = 1, 2, ..., T$:\n",
    "\n",
    "1. **Treinar modelo fraco**:\n",
    "   - $h_t = \\mathcal{A}(D, w^{(t)})$\n",
    "   - Cada amostra $(x^{(i)}, y^{(i)})$ tem peso $w^{(t)}_i$\n",
    "\n",
    "2. **Calcular erro ponderado**:\n",
    "   \n",
    "   $$\\epsilon_t = \\sum_{i=1}^{n} w^{(t)}_i \\cdot \\mathbb{I}(h_t(x^{(i)}) \\neq y^{(i)})$$\n",
    "   \n",
    "   onde $\\mathbb{I}$ é a função indicadora (1 se erro, 0 se acerto)\n",
    "\n",
    "3. **Calcular peso do modelo** (influência na predição final):\n",
    "   \n",
    "   $$\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "   \n",
    "   - Se $\\epsilon_t$ é pequeno (modelo bom) → $\\alpha_t$ é grande\n",
    "   - Se $\\epsilon_t \\approx 0.5$ (chute aleatório) → $\\alpha_t \\approx 0$\n",
    "\n",
    "4. **Atualizar pesos das amostras**:\n",
    "   \n",
    "   $$w^{(t+1)}_i = w^{(t)}_i \\cdot \\exp(-\\alpha_t \\cdot y^{(i)} \\cdot h_t(x^{(i)}))$$\n",
    "   \n",
    "   Simplificando:\n",
    "   - Se $h_t$ acerta $(y^{(i)} = h_t(x^{(i)}) \\Rightarrow y^{(i)}\\cdot h_t(x^{(i)}) = 1)$: peso diminui (multiplicado por $e^{-\\alpha_t}$)\n",
    "   - Se $h_t$ erra $(y^{(i)} \\neq h_t(x^{(i)})\\Rightarrow y^{(i)}\\cdot h_t(x^{(i)}) = -1)$: peso aumenta (multiplicado por $e^{\\alpha_t}$)\n",
    "\n",
    "5. **Normalizar pesos**:\n",
    "   \n",
    "   $$w^{(t+1)}_i = \\frac{w^{(t+1)}_i}{\\sum_{j=1}^{n}w^{(t+1)}_j}$$\n",
    "\n",
    "**Saída - Predição:**\n",
    "\n",
    "Para uma nova amostra $x^o$:\n",
    "\n",
    "$$H(x^o) = \\text{sign}\\left(\\sum_{t=1}^{T}\\alpha_t \\cdot h_t(x^o)\\right)$$\n",
    "\n",
    "- Votação ponderada: modelos melhores ($\\alpha_t$ maior) têm mais influência\n",
    "- $\\text{sign}()$ retorna +1 ou -1 (a classe final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting-sklearn'></a>\n",
    "### **Implementação com Sklearn: AdaBoost e Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# =======================================================\n",
    "# 1. AdaBoost\n",
    "# =======================================================\n",
    "\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # Stumps\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADABOOST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia no teste: {ada_boost.score(X_test, y_test)*100:.2f}%\")\n",
    "\n",
    "# Pesos dos estimadores\n",
    "print(f\"\\nPrimeiros 5 pesos dos modelos (alpha):\")\n",
    "print(ada_boost.estimator_weights_[:5])\n",
    "\n",
    "# =======================================================\n",
    "# 2. Gradient Boosting\n",
    "# =======================================================\n",
    "\n",
    "# Gradient Boosting é uma generalização do AdaBoost\n",
    "# Em vez de ajustar pesos, ele ajusta os resíduos (erros) diretamente\n",
    "\n",
    "gradient_boost = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia no teste: {gradient_boost.score(X_test, y_test)*100:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['Pclass', 'Sex', 'Age']\n",
    "importances = gradient_boost.feature_importances_\n",
    "print(\"\\nImportância das features:\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"  {name}: {importance*100:.2f}%\")\n",
    "\n",
    "# =======================================================\n",
    "# 3. Comparação: Bagging vs Boosting\n",
    "# =======================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    'Single Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Bagging (Random Forest)': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARAÇÃO DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = model.score(X_test, y_test)\n",
    "    print(f\"{name:25s}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Parâmetros Importantes do Gradient Boosting\n",
    "# =======================================================\n",
    "\n",
    "# n_estimators: número de árvores\n",
    "# learning_rate: taxa de aprendizado (shrinkage)\n",
    "#   - Valores menores = mais árvores necessárias, mas melhor generalização\n",
    "# max_depth: profundidade máxima de cada árvore\n",
    "# subsample: fração de amostras usadas para treinar cada árvore\n",
    "#   - < 1.0 = Stochastic Gradient Boosting (reduz overfitting)\n",
    "\n",
    "configs = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 1.0},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 3, 'subsample': 1.0},\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 1.0},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8},\n",
    "]\n",
    "\n",
    "print(\"Testando diferentes configurações de Gradient Boosting:\\n\")\n",
    "for i, config in enumerate(configs, 1):\n",
    "    gb = GradientBoostingClassifier(**config, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"{i}. n_est={config['n_estimators']}, lr={config['learning_rate']}, \"\n",
    "          f\"depth={config['max_depth']}, subsample={config['subsample']}\")\n",
    "    print(f\"   Acurácia: {gb.score(X_test, y_test)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica: Learning Rate no Boosting**\n",
    "\n",
    "O `learning_rate` (ou shrinkage) controla o quanto cada árvore contribui para a predição final.\n",
    "\n",
    "- **learning_rate = 1.0**: Cada árvore contribui totalmente\n",
    "  -  Converge rápido\n",
    "  - Porém, Mais propenso a overfitting\n",
    "\n",
    "- **learning_rate = 0.1**: Cada árvore contribui apenas 10%\n",
    "  - Melhor generalização\n",
    "  - Porém, Precisa de mais árvores (maior n_estimators)\n",
    "\n",
    "**Regra:** learning_rate baixo + mais árvores = melhor performance (mas mais lento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stacking'></a>\n",
    "## **Stacking: Combinando Diferentes Modelos**\n",
    "\n",
    "**O que é Stacking?**\n",
    "\n",
    "Stacking (Stacked Generalization) é como além de uma **equipe de especialistas diferentes** ter um **coordenador** que aprende a melhor forma de combinar suas opiniões.\n",
    "\n",
    "**A Ideia:**\n",
    "\n",
    "1. **Nível 0 (Base)**: Treinar vários modelos diferentes\n",
    "   - KNN, Árvore de Decisão, Naive Bayes, SVM, etc.\n",
    "   - Cada um tem suas próprias forças e fraquezas\n",
    "\n",
    "2. **Gerar meta-features**: As previsões desses modelos base se tornam novas features\n",
    "\n",
    "3. **Nível 1 (Meta)**: Treinar um \"meta-modelo\" que aprende a combinar as previsões dos modelos base\n",
    "   - Geralmente um modelo simples (Regressão Logística)\n",
    "   - Aprende qual modelo confiar em cada situação\n",
    "\n",
    "**Diferença de Bagging/Boosting:**\n",
    "\n",
    "- **Bagging/Boosting**: Múltiplos modelos do MESMO tipo (várias árvores)\n",
    "- **Stacking**: Modelos DIFERENTES + meta-modelo que aprende a combiná-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intuição do Stacking**\n",
    "\n",
    "Imagine que você está decidindo se um e-mail é spam:\n",
    "\n",
    "**Modelos Base (Nível 0):**\n",
    "\n",
    "1. **Naive Bayes** diz: \"90% spam\" \n",
    "2. **KNN** diz: \"60% spam\"\n",
    "3. **Árvore de Decisão** diz: \"80% spam\" \n",
    "4. **SVM** diz: \"85% spam\" \n",
    "**Meta-Modelo (Nível 1):**\n",
    "\n",
    "Em vez de simplesmente fazer a média (78.75%), o meta-modelo aprende que:\n",
    "\n",
    "- \"Quando Naive Bayes está confiante (>85%), confie nele\"\n",
    "- \"Quando os modelos discordam muito, prefira o SVM (por quê?)\"\n",
    "- \"Se KNN está muito diferente dos outros, ignore-o\"\n",
    "\n",
    "Resultado final: **92% spam**\n",
    "\n",
    "Vamos entender na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Implementação Manual do Stacking\n",
    "# =======================================================\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Passo 1: Definir modelos base\n",
    "base_models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "# Passo 2: Treinar modelos base e obter previsões\n",
    "print(\"=\"*60)\n",
    "print(\"NÍVEL 0: Treinando modelos base\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Armazenar previsões dos modelos base\n",
    "base_predictions_train = []\n",
    "base_predictions_test = []\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    # Treinar no conjunto de treino\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazer previsões (probabilidades)\n",
    "    # Usamos cross-validation para evitar overfitting no meta-modelo\n",
    "    train_pred = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method='predict_proba'\n",
    "    )[:, 1]  # Probabilidade da classe 1\n",
    "    \n",
    "    # Previsões no teste\n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    base_predictions_train.append(train_pred)\n",
    "    base_predictions_test.append(test_pred)\n",
    "    \n",
    "    acc = model.score(X_test, y_test)\n",
    "    print(f\"{name:20s}: {acc*100:.2f}%\")\n",
    "\n",
    "# Converter para arrays numpy\n",
    "X_meta_train = np.column_stack(base_predictions_train)\n",
    "X_meta_test = np.column_stack(base_predictions_test)\n",
    "\n",
    "print(f\"\\nShape das meta-features:\")\n",
    "print(f\"  Treino: {X_meta_train.shape}\")\n",
    "print(f\"  Teste: {X_meta_test.shape}\")\n",
    "\n",
    "# Passo 3: Treinar meta-modelo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NÍVEL 1: Treinando meta-modelo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Fazer previsões finais\n",
    "y_pred_stacking = meta_model.predict(X_meta_test)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Acurácia do Stacking: {accuracy_stacking*100:.2f}%\")\n",
    "\n",
    "# Visualizar os coeficientes do meta-modelo\n",
    "print(\"\\nCoeficientes do meta-modelo (importância de cada modelo base):\")\n",
    "for name, coef in zip(base_models.keys(), meta_model.coef_[0]):\n",
    "    print(f\"  {name:20s}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESAFIO: Como seria a visualização para o Stacking?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Formulação Formal do Stacking**\n",
    "\n",
    "**Entrada:**\n",
    "\n",
    "1. Conjunto de treinamento: $D = \\{(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "2. Conjunto de $K$ algoritmos de nível 0 (base): $\\{\\mathcal{A}_1, \\mathcal{A}_2, ..., \\mathcal{A}_K\\}$\n",
    "\n",
    "3. Algoritmo de nível 1 (meta): $\\mathcal{A}_{meta}$\n",
    "\n",
    "**Processamento:**\n",
    "\n",
    "**Nível 0 - Treinar Modelos Base:**\n",
    "\n",
    "Para cada algoritmo $\\mathcal{A}_k$ ($k = 1, ..., K$):\n",
    "\n",
    "1. **Treinar modelo**: $h_k = \\mathcal{A}_k(D)$\n",
    "\n",
    "2. **Gerar meta-features** usando validação cruzada para evitar overfitting:\n",
    "   - Dividir $D$ em $V$ folds\n",
    "   - Para cada fold $v$:\n",
    "     - Treinar $h_k$ nos outros $V-1$ folds\n",
    "     - Prever no fold $v$\n",
    "   - Resultado: $\\tilde{h}_k(x^{(i)})$ para todo $i$ (previsões out-of-fold)\n",
    "\n",
    "**Criar Dataset de Meta-Features:**\n",
    "\n",
    "Para cada amostra $(x^{(i)}, y^{(i)})$:\n",
    "\n",
    "$$x_{meta}^{(i)} = [\\tilde{h}_1(x^{(i)}), \\tilde{h}_2(x^{(i)}), ..., \\tilde{h}_K(x^{(i)})]$$\n",
    "\n",
    "O novo dataset meta é: $D_{meta} = \\{(x_{meta}^{(1)}, y^{(1)}), ..., (x_{meta}^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "**Nível 1 - Treinar Meta-Modelo:**\n",
    "\n",
    "$$h_{meta} = \\mathcal{A}_{meta}(D_{meta})$$\n",
    "\n",
    "**Saída - Predição:**\n",
    "\n",
    "Para uma nova amostra $x^o$:\n",
    "\n",
    "1. **Obter previsões dos modelos base**:\n",
    "   \n",
    "   $$x_{meta}^o = [h_1(x^o), h_2(x^o), ..., h_K(x^o)]$$\n",
    "\n",
    "2. **Aplicar meta-modelo**:\n",
    "   \n",
    "   $$\\hat{y} = h_{meta}(x_{meta}^o)$$\n",
    "\n",
    "**Variações Importantes:**\n",
    "\n",
    "1. **Features Originais no Meta-Modelo**: \n",
    "   - Em vez de usar apenas as previsões dos modelos base, podemos concatenar com as features originais:\n",
    "   \n",
    "   $$x_{meta}^{(i)} = [x^{(i)}, \\tilde{h}_1(x^{(i)}), ..., \\tilde{h}_K(x^{(i)})]$$\n",
    "\n",
    "2. **Multi-Level Stacking**:\n",
    "   - Podemos ter múltiplos níveis: Nível 0 → Nível 1 → Nível 2 → ...\n",
    "   - Cada nível usa as previsões do nível anterior como features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stacking-sklearn'></a>\n",
    "### **Implementação com Sklearn: StackingClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# =======================================================\n",
    "# 1. Stacking Básico\n",
    "# =======================================================\n",
    "\n",
    "# Definir modelos base\n",
    "estimators = [\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('tree', DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
    "    ('nb', GaussianNB()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "]\n",
    "\n",
    "# Criar stacking com Regressão Logística como meta-modelo\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5  # Validação cruzada para gerar meta-features\n",
    ")\n",
    "\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STACKING CLASSIFIER (sklearn)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia: {stacking.score(X_test, y_test)*100:.2f}%\")\n",
    "\n",
    "# =======================================================\n",
    "# 2. Stacking com passthrough (features originais)\n",
    "# =======================================================\n",
    "\n",
    "# passthrough=True adiciona as features originais ao meta-modelo\n",
    "stacking_passthrough = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    passthrough=True  # Incluir features originais\n",
    ")\n",
    "\n",
    "stacking_passthrough.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STACKING COM PASSTHROUGH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Acurácia: {stacking_passthrough.score(X_test, y_test)*100:.2f}%\")\n",
    "\n",
    "# =======================================================\n",
    "# 3. Diferentes meta-modelos\n",
    "# =======================================================\n",
    "\n",
    "meta_models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTANDO DIFERENTES META-MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, meta_model in meta_models.items():\n",
    "    stack = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "    stack.fit(X_train, y_train)\n",
    "    acc = stack.score(X_test, y_test)\n",
    "    print(f\"{name:25s}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica: Escolhendo Modelos Base para Stacking**\n",
    "\n",
    "Para um bom Stacking, escolha modelos **diversos**:\n",
    "\n",
    "**Bons para Stacking:**\n",
    "- Modelos com princípios diferentes (KNN + Árvore + Naive Bayes)\n",
    "- Modelos lineares + não-lineares\n",
    "- Modelos que cometem erros diferentes\n",
    "\n",
    "**Ruins para Stacking:**\n",
    "- Múltiplas variações do mesmo modelo (3 árvores de decisão diferentes)\n",
    "- Modelos muito correlacionados\n",
    "- Modelos que sempre concordam\n",
    "\n",
    "*Se os modelos base sempre preveem a mesma coisa, o stacking não ajuda*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stacking-desafio'></a>\n",
    "### **Desafio: Criar um Ensemble Híbrido**\n",
    "\n",
    "Agora é sua vez! Combine Bagging, Boosting e Stacking em um único ensemble.\n",
    "\n",
    "**Objetivo:** Criar um StackingClassifier onde os modelos base sejam ensembles!\n",
    "\n",
    "**Estrutura sugerida:**\n",
    "\n",
    "Nível 0 (Modelos Base):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SEU CÓDIGO AQUI!\n",
    "# =======================================================\n",
    "\n",
    "# Dica: Use StackingClassifier com estimators que sejam\n",
    "# BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, etc.\n",
    "\n",
    "# Exemplo de estrutura:\n",
    "# hybrid_ensemble = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', RandomForestClassifier(...)),\n",
    "#         ('gb', GradientBoostingClassifier(...)),\n",
    "#         ('ada', AdaBoostClassifier(...)),\n",
    "#         ('bag_svm', BaggingClassifier(SVC(...), ...))\n",
    "#     ],\n",
    "#     final_estimator=...\n",
    "# )\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparacao'></a>\n",
    "## **Comparação: Bagging vs Boosting vs Stacking**\n",
    "\n",
    "**Como escolher qual usar?**\n",
    "\n",
    "Agora que estudamos os três principais métodos de ensemble, vamos comparar suas características e entender quando usar cada um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Idéia:\n",
    "# - Comparamos todos os modelos (single tree, bagging, boosting, stacking)\n",
    "# - Medimos tempo de treino, acurácia no teste, cross-validation score\n",
    "# - Analisamos a importância de cada modelo no ensemble\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir todos os modelos para comparação\n",
    "models_comparison = {\n",
    "    # Baseline\n",
    "    'Decision Tree (baseline)': DecisionTreeClassifier(random_state=42),\n",
    "    \n",
    "    # Bagging\n",
    "    'Bagging (Decision Tree)': BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Boosting\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Stacking\n",
    "    'Stacking': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "            ('tree', DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
    "            ('nb', GaussianNB()),\n",
    "            ('rf', RandomForestClassifier(n_estimators=30, random_state=42))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Avaliar todos os modelos\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARAÇÃO COMPLETA DE ENSEMBLE METHODS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Modelo':<30} {'Acurácia Teste':<15} {'CV Score (mean)':<20} {'Tempo (s)':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, model in models_comparison.items():\n",
    "    # Medir tempo de treino\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Acurácia no teste\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Tempo\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'test_acc': test_acc,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'time': elapsed_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<30} {test_acc*100:>6.2f}%        {cv_mean*100:>6.2f}% ± {cv_std*100:>4.2f}%    {elapsed_time:>6.2f}s\")\n",
    "\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='consideracoes'></a>\n",
    "## **Considerações Finais**\n",
    "\n",
    "### **Quando Usar Cada Método?**\n",
    "\n",
    "**Bagging:**\n",
    "-  Quando você quer um modelo **simples e robusto**\n",
    "-  Random Forest é geralmente a melhor escolha\n",
    "-  Reduz variância sem aumentar muito a complexidade\n",
    "-  Pode ser **paralelizado** → rápido em multicore\n",
    "-  **Exemplo**: Classificação de imagens, regressão\n",
    "\n",
    "**Boosting:**\n",
    "-  Quando você precisa de **máxima acurácia** (geralmente o melhor desempenho)\n",
    "-  Quando você tem modelos com **alto bias** (ex: stumps)\n",
    "-  Quando você pode **ajustar cuidadosamente os hiperparâmetros**\n",
    "-  Cuidado com overfitting em datasets pequenos\n",
    "-  **Exemplo**: Competições de ML (Kaggle), ranking de busca\n",
    "\n",
    "**Stacking:**\n",
    "-  Quando você quer **combinar modelos fundamentalmente diferentes**\n",
    "-  Quando você já tem **vários modelos bons** e quer espremer mais performance\n",
    "-  Em **competições** onde cada 0.1% de acurácia importa\n",
    "-  Mais complexo de implementar e manter\n",
    "-  **Exemplo**: Sistemas de recomendação, previsão de séries temporais\n",
    "\n",
    "### **Considerações Práticas**\n",
    "\n",
    "**1. Interpretabilidade:**\n",
    "\n",
    "Todos os métodos de ensemble reduzem a interpretabilidade:\n",
    "- Um modelo único é mais fácil de explicar\n",
    "- Ensembles são \"caixas-pretas\"\n",
    "- Use **feature importance** para ter alguma interpretação\n",
    "\n",
    "**2. Hiperparâmetros Críticos:**\n",
    "\n",
    "**Bagging:**\n",
    "- `n_estimators`: Quantos modelos (mais = melhor, mas com retorno decrescente)\n",
    "- `max_samples`: Tamanho das amostras bootstrap\n",
    "- `max_features`: Features aleatórias (para Random Forest)\n",
    "\n",
    "**Boosting:**\n",
    "- `n_estimators`: Número de modelos (cuidado com overfitting)\n",
    "- `learning_rate`: Taxa de aprendizado (menor = mais modelos necessários)\n",
    "- `max_depth`: Profundidade das árvores (geralmente shallow)\n",
    "\n",
    "**Stacking:**\n",
    "- Escolha dos **modelos base** (diversidade é chave!)\n",
    "- Escolha do **meta-modelo** (geralmente simples: LogisticRegression)\n",
    "- `cv`: Número de folds (importante para evitar overfitting)\n",
    "\n",
    "**3. Dicas Práticas**\n",
    "\n",
    "**Comece simples**: Random Forest é geralmente uma ótima escolha inicial\n",
    "- Robusto, fácil de usar, bom desempenho out-of-the-box\n",
    "\n",
    "**Para máxima performance**: Gradient Boosting (ou XGBoost/LightGBM)\n",
    "- Mas requer mais tuning de hiperparâmetros\n",
    "\n",
    "**Compute vs Accuracy**:\n",
    "- Bagging: Pode ser paralelizado → rápido\n",
    "- Boosting: Sequencial → mais lento\n",
    "- Stacking: Overhead adicional → mais lento ainda\n",
    "\n",
    "**Evite ensembles de ensembles (em produção)**:\n",
    "- Stacking de Boosting pode dar overfitting\n",
    "- Complexidade de manutenção explode\n",
    "- Trade-off entre 0.5% de acurácia vs simplicidade\n",
    "\n",
    "**4. Aplicações no Mundo Real**\n",
    "\n",
    "1. **Detecção de Spam**: Random Forest ou Gradient Boosting\n",
    "2. **Diagnóstico Médico**: Stacking \n",
    "3. **Previsão de Churn**: Gradient Boosting (maximiza acurácia)\n",
    "4. **Sistemas de Recomendação**: Stacking (combine collaborative filtering + content-based)\n",
    "5. **Detecção de Fraude**: Random Forest (lidar com desbalanceamento)\n",
    "\n",
    "**5. Bibliotecas para Aprofundamento**\n",
    "\n",
    "- **[XGBoost](https://xgboost.readthedocs.io/en/stable/)**: Gradient Boosting extremamente otimizado\n",
    "- **[LightGBM](https://lightgbm.readthedocs.io/en/stable/Python-Intro.html)**: Gradient Boosting ainda mais rápido (da Microsoft)\n",
    "- **[CatBoost](https://catboost.ai/docs/en/concepts/python-installation)**: Gradient Boosting para features categóricas (da Yandex)\n",
    "- **[VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)**: Ensemble simples por votação (sklearn)\n",
    "\n",
    "Todos são variações/melhorias dos métodos que estudamos.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='resumo'></a>\n",
    "## **Resumo e Próximos Passos**\n",
    "\n",
    "### **Aprendizados Principais**\n",
    "\n",
    "1. **Ensembles reduzem variância** combinando múltiplos modelos\n",
    "2. **Bagging** treina modelos independentemente em amostras bootstrap\n",
    "3. **Boosting** treina modelos sequencialmente, focando em erros anteriores\n",
    "4. **Stacking** usa um meta-modelo para aprender a combinar previsões\n",
    "5. **Random Forest** é geralmente a melhor escolha para começar\n",
    "6. **Gradient Boosting** oferece máxima acurácia com tuning adequado\n",
    "\n",
    "### **Próximos Tópicos**\n",
    "\n",
    "- Técnicas avançadas: XGBoost, LightGBM, CatBoost\n",
    "- Tuning de hiperparâmetros com Grid/Random Search\n",
    "- Interpretabilidade em modelos de ensemble (SHAP, etc.)\n",
    "- Ensembles em problemas de séries temporais\n",
    "- Ensembles para regressão vs classificação\n",
    "\n",
    "<-- [**Anterior: Árvores de Decisão**](05_arvores_classificacao.ipynb) | [**Próximo: Módulo 04 — Regressão**](../04_Regressao/01_regressao_linear_multipla.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
