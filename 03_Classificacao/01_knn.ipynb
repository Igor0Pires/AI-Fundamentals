{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN (K-Nearest Neighbors) — Módulo 3, Notebook 1/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao KNN](#introducao)\n",
    "2. [Como o KNN Funciona?](#como-funciona)\n",
    "3. [Intuição Prática](#intuicao)\n",
    "4. [Formalização Matemática](#formalizacao)\n",
    "5. [Implementação do Zero](#from-scratch)\n",
    "6. [Implementação com Scikit-Learn](#sklearn)\n",
    "7. [Vantagens e Desvantagens](#vantagens-desvantagens)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao KNN**\n",
    "\n",
    "Para compreender o funcionamento do KNN, vamos partir de um exemplo intuitivo. Imagine que somos novos usuários na Netflix. A plataforma precisa prever se vamos gostar de um filme específico que nunca assistimos, digamos, \"Crepúsculo\". Como ela faria isso?\n",
    "\n",
    "A estratégia é direta: **encontrar pessoas com gostos semelhantes aos nossos e verificar o que elas acharam do filme**.\n",
    "\n",
    "Se 8 de 10 usuários que apreciaram os mesmos filmes que nós também gostaram de \"Crepúsculo\", existe uma probabilidade alta de também gostarmos. Esse é precisamente o princípio do KNN — classificar novos dados com base nos exemplos mais \"próximos\" que já conhece.\n",
    "\n",
    "O KNN (K-Nearest Neighbors, ou K-Vizinhos Mais Próximos) é um dos algoritmos de machine learning mais simples e intuitivos. Não envolve fórmulas complexas ou treino sofisticado — o modelo memoriza os dados e, quando precisa fazer uma previsão, busca os exemplos mais semelhantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Como o KNN Funciona?**\n",
    "\n",
    "Retomando os conceitos fundamentais, vamos relembrar que trabalhamos com um **espaço de características**. Cada observação em nossa base de dados é representada por um **vetor de características** neste espaço. \n",
    "\n",
    "O algoritmo KNN opera em três etapas fundamentais:\n",
    "\n",
    "### **Passo 1: Armazenar os Dados**\n",
    "O algoritmo armazena todos os exemplos de treinamento. Cada observação corresponde a um ponto no espaço de características.\n",
    "\n",
    "### **Passo 2: Calcular Distâncias**\n",
    "Quando precisamos classificar uma nova observação, o KNN calcula a distância entre esse novo ponto e **todos** os pontos do conjunto de treino. A métrica mais comum é a **distância euclidiana** (distância em linha reta entre dois pontos).\n",
    "\n",
    "### **Passo 3: Votar pela Classe**\n",
    "O algoritmo identifica os K vizinhos mais próximos (daí o \"K\" no nome) e realiza uma votação: a classe mais frequente entre esses vizinhos é atribuída ao novo ponto.\n",
    "\n",
    "O gif abaixo ilustra esse processo de maneira visual:\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2018/08/KNN-Classification.gif\" width=600 height=400></img>\n",
    "</p>\n",
    "\n",
    "### **Parâmetros Importantes**\n",
    "\n",
    "Quando trabalhamos com KNN, dois parâmetros são fundamentais:\n",
    "\n",
    "**1. Número de vizinhos (K):**\n",
    "- K pequeno (ex: K=1): Modelo mais complexo, sensível a ruído\n",
    "- K grande (ex: K=50): Modelo mais simples, fronteiras mais suaves\n",
    "\n",
    "**2. Métrica de distância:**\n",
    "- **Distância Euclidiana** (mais comum): linha reta entre pontos\n",
    "- Distância de Manhattan: soma das diferenças absolutas\n",
    "- Outras métricas para casos específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição Prática: Classificando Flores Iris**\n",
    "\n",
    "Com a compreensão conceitual estabelecida, vamos consolidar esse conhecimento através de um exemplo prático. Utilizaremos o dataset mais clássico de machine learning: o **Iris Dataset**.\n",
    "\n",
    "### **O Problema**\n",
    "\n",
    "Considere que encontramos uma flor íris na natureza e medimos suas características:\n",
    "- Comprimento da pétala: 4.5 cm\n",
    "- Largura da pétala: 1.5 cm\n",
    "\n",
    "Sabemos que existem 3 espécies de íris: **Setosa**, **Versicolor** e **Virginica**.\n",
    "\n",
    "**Desafio:** Determinar a qual espécie essa flor pertence.\n",
    "\n",
    "Vamos aplicar o KNN para resolver esse problema! Implementaremos o algoritmo passo a passo para compreender exatamente como ele toma essa decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o Iris Dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # Features: 4 características das flores\n",
    "y = iris.target  # Labels: 3 classes (0, 1, 2) das flores\n",
    "\n",
    "print(\"Classes:\")\n",
    "for i, nome in enumerate(iris.target_names):\n",
    "    print(f\"  Classe {i}: {nome.capitalize()} ({np.sum(y == i)} amostras)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nFeatures (cm):\")\n",
    "for i, nome in enumerate(iris.feature_names):\n",
    "    print(f\"  {i+1}. {nome}\")\n",
    "print(f\"\\nFormato dos dados: {X.shape}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualizando os dados\n",
    "print(\"\\nVisualizando os dados:\")\n",
    "pd.DataFrame(X, columns=iris.feature_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplificando para visualização: utilizaremos apenas 2 features\n",
    "# Comprimento e largura da pétala (features mais discriminativas)\n",
    "X_2d = X[:, [2, 3]]\n",
    "\n",
    "print(\"Reduzimos o dataset para 2 features para facilitar a visualização:\")\n",
    "print(\"  - Comprimento da pétala\")\n",
    "print(\"  - Largura da pétala\")\n",
    "print(f\"\\nNovo formato: {X_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a flor que queremos classificar\n",
    "novo_ponto = np.array([4.5, 1.5])  # Comprimento e largura da pétala\n",
    "\n",
    "print(\"Nossa flor desconhecida:\")\n",
    "print(f\"  Comprimento da pétala: {novo_ponto[0]} cm\")\n",
    "print(f\"  Largura da pétala: {novo_ponto[1]} cm\")\n",
    "print(f\"  Classe: ???\")\n",
    "print(\"\\nVamos descobrir usando KNN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 1: CALCULAR DISTÂNCIAS\n",
    "# ===================================================================\n",
    "\n",
    "def euclidean_distance(X, point):\n",
    "    \"\"\"\n",
    "    Calcula a distância Euclidiana entre um ponto e todos os pontos em X.\n",
    "    \n",
    "    Fórmula: d = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "    \"\"\"\n",
    "    distance = np.sqrt(np.sum((X - point) ** 2, axis=1))\n",
    "    return distance\n",
    "\n",
    "# Calculando as distâncias\n",
    "distancias = euclidean_distance(X_2d, novo_ponto)\n",
    "\n",
    "print(\"Distâncias calculadas:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Distância até ponto {i+1}: {distancias[i]:.4f} cm\")\n",
    "print(\"...\")\n",
    "print(f\"  Distância até ponto 150: {distancias[-1]:.4f} cm\")\n",
    "print(f\"\\nTotal de distâncias calculadas: {len(distancias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 2: ENCONTRAR OS K VIZINHOS MAIS PRÓXIMOS\n",
    "# ===================================================================\n",
    "\n",
    "def find_knn(distancias, k):\n",
    "    \"\"\"\n",
    "    Retorna os índices dos k vizinhos mais próximos\n",
    "    \"\"\"\n",
    "    indices_dos_vizinhos = np.argsort(distancias)[:k]\n",
    "    return indices_dos_vizinhos\n",
    "\n",
    "k = 7\n",
    "k_vizinhos = find_knn(distancias, k)\n",
    "\n",
    "print(f\"Os {k} vizinhos mais próximos:\")\n",
    "for i, idx in enumerate(k_vizinhos):\n",
    "    classe = iris.target_names[y[idx]]\n",
    "    print(f\"  {i+1}º Vizinho: Flor #{idx+1} - Classe: {classe.capitalize()} - Distância: {distancias[idx]:.4f} cm\")\n",
    "\n",
    "print(\"\\nNote: Os vizinhos mais próximos têm as menores distâncias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 3: FAZER A PREVISÃO DA CLASSE\n",
    "# ===================================================================\n",
    "\n",
    "def predict_class(y_train, k_vizinhos_indices):\n",
    "    \"\"\"\n",
    "    Faz a previsão da classe com base nos k vizinhos mais próximos\n",
    "    \"\"\"\n",
    "    k_classes = y_train[k_vizinhos_indices]\n",
    "    contagem = Counter(k_classes)\n",
    "    most_common = contagem.most_common(1)\n",
    "    return most_common[0][0], contagem\n",
    "\n",
    "classe_prevista, contagem_classes = predict_class(y, k_vizinhos)\n",
    "\n",
    "print(\"Votação dos vizinhos:\")\n",
    "for classe_id in sorted(contagem_classes.keys()):\n",
    "    nome_classe = iris.target_names[int(classe_id)]\n",
    "    quantidade = contagem_classes[classe_id]\n",
    "    percentual = (quantidade / k) * 100\n",
    "    barra = '█' * quantidade\n",
    "    print(f\"  {nome_classe.capitalize():12s}: {quantidade}/{k} ({percentual:5.1f}%) {barra}\")\n",
    "\n",
    "resultado_nome = iris.target_names[int(classe_prevista)]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RESULTADO FINAL: A flor é da classe '{resultado_nome.upper()}'\")\n",
    "print(f\"   Confiança: {contagem_classes[classe_prevista]}/{k} ({(contagem_classes[classe_prevista]/k)*100:.1f}%)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Desafio**\n",
    "\n",
    "Agora que você entendeu os 3 passos do KNN com o exemplo das flores, consegue identificar como o mesmo algoritmo funcionaria com a recomendação da Netflix que mencionamos na introdução?\n",
    "\n",
    "**Desafio:** \n",
    "- Qual seria o \"espaço de características\"?\n",
    "- Como seria feita a votação final?\n",
    "- Qual seria o resultado previsto para você?\n",
    "\n",
    "Pense sobre isso antes de continuar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizando o Processo**\n",
    "\n",
    "Vamos agora visualizar graficamente como o KNN tomou essa decisão. A figura a seguir ilustra as três etapas do algoritmo de forma sequencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando visualização dos 3 passos do KNN\n",
    "np.random.seed(42)\n",
    "\n",
    "# Criando amostra para o gráfico\n",
    "amostra_indices = []\n",
    "for classe in range(3):\n",
    "    indices_classe = np.where(y == classe)[0]\n",
    "    amostra_classe = np.random.choice(indices_classe, 10, replace=False)\n",
    "    amostra_indices.extend(amostra_classe)\n",
    "\n",
    "X_amostra = X_2d[amostra_indices]\n",
    "y_amostra = y[amostra_indices]\n",
    "\n",
    "# Recalculando para a amostra\n",
    "novo_ponto_viz = np.array([4.5, 1.5])\n",
    "dist_viz = euclidean_distance(X_amostra, novo_ponto_viz)\n",
    "k_viz = 5\n",
    "vizinhos_viz = find_knn(dist_viz, k_viz)\n",
    "classe_viz, contagem_viz = predict_class(y_amostra, vizinhos_viz)\n",
    "\n",
    "# Cores e labels\n",
    "cores = ['r', 'g', 'b']\n",
    "labels = iris.target_names\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "fig.suptitle(\"Classificação KNN - Visualização dos 3 Passos do Algoritmo\", fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRÁFICO 1: Calcular Distâncias\n",
    "# -----------------------------------------------------\n",
    "ax1 = axes[0]\n",
    "\n",
    "for i in range(3):\n",
    "    indices = y_amostra == i\n",
    "    ax1.scatter(X_amostra[indices, 0], X_amostra[indices, 1], \n",
    "                c=cores[i], s=150, alpha=0.7, label=labels[i].capitalize(),\n",
    "                edgecolors='k', linewidth=1.5, zorder=3)\n",
    "    \n",
    "ax1.scatter(novo_ponto_viz[0], novo_ponto_viz[1], c='gold', s=400, marker='*', \n",
    "            label='Flor Desconhecida', edgecolors='k', linewidth=1.5, zorder=10)\n",
    "\n",
    "for i, ponto in enumerate(X_amostra):\n",
    "    ax1.plot([novo_ponto_viz[0], ponto[0]], [novo_ponto_viz[1], ponto[1]], \n",
    "             'gray', alpha=0.3, linewidth=1.5, linestyle='-', zorder=1)\n",
    "\n",
    "ax1.set_xlabel(\"Comprimento da Pétala (cm)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Largura da Pétala (cm)\", fontsize=12)\n",
    "ax1.set_title(\"Passo 1: Calcular Distâncias\")\n",
    "ax1.legend(loc='upper left', fontsize=10, framealpha=0.95)\n",
    "ax1.set_xlim(0, 7)\n",
    "ax1.set_ylim(0, 3)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRÁFICO 2: Encontrar K Vizinhos\n",
    "# -----------------------------------------------------\n",
    "ax2 = axes[1]\n",
    "\n",
    "for i in range(3):\n",
    "    indices = y_amostra == i\n",
    "    ax2.scatter(X_amostra[indices, 0], X_amostra[indices, 1], \n",
    "                c=cores[i], s=80, alpha=0.4,\n",
    "                edgecolors='gray', linewidth=0.5, zorder=1)\n",
    "\n",
    "# Destacando os k vizinhos mais próximos\n",
    "vizinhos_pts = X_amostra[vizinhos_viz]\n",
    "vizinhos_cls = y_amostra[vizinhos_viz]\n",
    "cores_viz = [cores[int(cls)] for cls in vizinhos_cls]\n",
    "\n",
    "ax2.scatter(vizinhos_pts[:, 0], vizinhos_pts[:, 1],\n",
    "            c=cores_viz, s=300, alpha=0.95,\n",
    "            edgecolors='black', linewidth=2.5, zorder=5)\n",
    "    \n",
    "ax2.scatter(novo_ponto_viz[0], novo_ponto_viz[1], c='gold', s=400, marker='*',\n",
    "            edgecolors='k', linewidth=2.5, zorder=10)\n",
    "\n",
    "ax2.set_xlabel(\"Comprimento da Pétala (cm)\", fontsize=12)\n",
    "ax2.set_title(\"Passo 2: Encontrar os K Vizinhos Mais Próximos\")\n",
    "ax2.set_xlim(0, 7)\n",
    "ax2.set_ylim(0, 3)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRÁFICO 3: Prever a Classe\n",
    "# -----------------------------------------------------\n",
    "ax3 = axes[2]\n",
    "\n",
    "for i in range(3):\n",
    "    indices = y_amostra == i\n",
    "    ax3.scatter(X_amostra[indices, 0], X_amostra[indices, 1], \n",
    "                c=cores[i], s=60, alpha=0.3,\n",
    "                edgecolors='gray', linewidth=0.3, zorder=1)\n",
    "    \n",
    "ax3.scatter(vizinhos_pts[:, 0], vizinhos_pts[:, 1],\n",
    "            c=cores_viz, s=180, alpha=0.5,\n",
    "            edgecolors='gray', linewidth=1.5, zorder=3)\n",
    "\n",
    "cor_prev = cores[int(classe_viz)]\n",
    "ax3.scatter(novo_ponto_viz[0], novo_ponto_viz[1], c=cor_prev, s=500, marker='*',\n",
    "            edgecolors='k', linewidth=3, zorder=10, alpha=0.95, \n",
    "            label=f'Classe: {labels[int(classe_viz)].capitalize()}')\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "circulo = Circle(novo_ponto_viz, dist_viz[vizinhos_viz[-1]],\n",
    "                 fill=False, edgecolor=cor_prev, linewidth=3, linestyle='--', alpha=0.7, zorder=2)\n",
    "ax3.add_patch(circulo)\n",
    "\n",
    "ax3.set_xlabel(\"Comprimento da Pétala (cm)\", fontsize=12)\n",
    "ax3.set_title(\"Passo 3: Prever a Classe\")\n",
    "ax3.set_xlim(0, 7)\n",
    "ax3.set_ylim(0, 3)\n",
    "ax3.legend(loc='upper left', fontsize=10, framealpha=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**\n",
    "\n",
    "Agora que você entendeu intuitivamente como o KNN funciona, vamos formalizar o algoritmo matematicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrada do Algoritmo**\n",
    "\n",
    "O KNN recebe como entrada:\n",
    "\n",
    "- **Conjunto de treinamento:** $D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$\n",
    "  \n",
    "  Onde:\n",
    "  - $x_i$: vetor de características (no nosso exemplo: comprimento e largura da pétala)\n",
    "  - $y_i$: rótulo/classe (no nosso exemplo: Setosa, Versicolor, Virginica)\n",
    "  - Corresponde à variável `X_train` no código\n",
    "\n",
    "- **Hiperparâmetro K:** Número de vizinhos a serem considerados (variável `k` no código)\n",
    "\n",
    "- **Ponto para previsão:** $x_0$ (a nova observação que queremos classificar — variável `novo_ponto` no código)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processo do Algoritmo**\n",
    "\n",
    "**1. Calcular Distâncias**\n",
    "\n",
    "Para cada ponto $x_i$ no conjunto de treinamento, calculamos a distância até $x_0$. A métrica mais comum é a **distância euclidiana**:\n",
    "\n",
    "$$d(x_0, x_i) = \\sqrt{\\sum^n_{j=1}(x_{0j} - x_{ij})^2}$$\n",
    "\n",
    "Essa é simplesmente a distância em linha reta entre dois pontos em um espaço de $n$ dimensões (onde $n$ é o número de features). No código, implementamos isso através da função `euclidean_distance(X, point)` que retorna um array com todas as distâncias.\n",
    "\n",
    "**Nota:** Outras métricas de distância podem ser usadas dependendo do problema:\n",
    "- **Distância de Manhattan:** $d(x_0, x_i) = \\sum^n_{j=1}|x_{0j} - x_{ij}|$\n",
    "- **Distância de Minkowski:** Generalização das anteriores\n",
    "- **Distância de Hamming:** Para dados categóricos\n",
    "\n",
    "**2. Ordenar e Identificar os Vizinhos**\n",
    "\n",
    "Após calcular todas as distâncias:\n",
    "- Ordenamos as distâncias em ordem crescente\n",
    "- Selecionamos o conjunto $N$ que contém os $K$ pontos com as $K$ menores distâncias\n",
    "- Esse conjunto $N$ forma a \"vizinhança\" do ponto $x_0$\n",
    "\n",
    "No código, usamos `np.argsort(distancias)[:k]` para obter os índices dos K vizinhos mais próximos (função `find_knn`).\n",
    "\n",
    "**Importante sobre a escolha de K:**\n",
    "- K pequeno (ex: K=1): Modelo sensível a ruído, fronteiras complexas\n",
    "- K grande (ex: K=50): Modelo mais suave, mas pode ignorar padrões locais\n",
    "- K ótimo: Geralmente encontrado por validação cruzada (veremos na seção de técnicas avançadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saída: Previsão**\n",
    "\n",
    "A previsão $\\hat{y}$ para classificação é simplesmente a **classe mais frequente** (moda) entre os rótulos dos $K$ vizinhos no conjunto $N$.\n",
    "\n",
    "Formalmente:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_v \\sum_{(x_i, y_i) \\in N} I(y_i = v)$$\n",
    "\n",
    "Onde:\n",
    "- $\\arg\\max_v$: encontra o valor $v$ que maximiza a expressão\n",
    "- $I(y_i = v)$: função indicadora que vale 1 se $y_i = v$, e 0 caso contrário\n",
    "- Traduzindo: \"conte os votos para cada classe $v$ e escolha a que tiver mais\"\n",
    "\n",
    "### **Pseudocódigo**\n",
    "\n",
    "```\n",
    "Algoritmo KNN(D, K, x₀):\n",
    "    1. Para cada (xᵢ, yᵢ) em D:\n",
    "        a. Calcular d(x₀, xᵢ)\n",
    "    \n",
    "    2. Ordenar as distâncias em ordem crescente\n",
    "    \n",
    "    3. Selecionar os K vizinhos mais próximos → conjunto N\n",
    "    \n",
    "    4. Contar votos: para cada classe v, contar quantos vizinhos têm yᵢ = v\n",
    "    \n",
    "    5. Retornar a classe com mais votos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='from-scratch'></a>\n",
    "## **Implementação do Zero**\n",
    "\n",
    "Agora que você entende a matemática, vamos implementar o KNN completamente do zero, usando apenas NumPy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Desafio: Implemente você mesmo!**\n",
    "\n",
    "Tente completar a classe KNN abaixo. Use o que aprendemos nos passos anteriores como referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Inicializa o classificador KNN\n",
    "\n",
    "        Parâmetros:\n",
    "        k (int): Número de vizinhos a considerar (padrão é 3)\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit (self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Armazena os dados de treinamento\n",
    "\n",
    "        Parâmetros:\n",
    "        X_train (array-like): Dados de treinamento (features)\n",
    "        y_train (array-like): Labels de treinamento (classes)\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Calcula a distância Euclidiana entre dois pontos\n",
    "\n",
    "        Parâmetros:\n",
    "        x1, x2 (array-like): Pontos para calcular a distância\n",
    "\n",
    "        Retorna:\n",
    "        float: Distância Euclidiana\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Faz previsões para os dados de teste\n",
    "\n",
    "        Parâmetros:\n",
    "        X_test (array-like): Dados de teste (features)\n",
    "\n",
    "        Retorna:\n",
    "        array: Previsões das classes\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Faz a previsão para um único ponto de dados\n",
    "\n",
    "        Parâmetros:\n",
    "        x (array-like): Ponto de dados\n",
    "\n",
    "        Retorna:\n",
    "        int: Classe prevista\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Calcula a acurácia do classificador\n",
    "\n",
    "        Parâmetros:\n",
    "        X_test (array-like): Dados de teste (features)\n",
    "        y_test (array-like): Labels verdadeiros (classes)\n",
    "\n",
    "        Retorna:\n",
    "        float: Acurácia do classificador\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para Teste (dataset Iris)\n",
    "# Carregar o dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Implementação com Scikit-Learn**\n",
    "\n",
    "Na prática, usamos bibliotecas otimizadas como o Scikit-Learn. Vamos ver como é simples implementar o mesmo algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementação Básica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)  # Modelo com K=7\n",
    "\n",
    "knn.fit(X_train, y_train)  # Treinando o modelo\n",
    "\n",
    "predictions = knn.predict(X_test)  # Fazendo previsões\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Acurácia: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Técnicas Avançadas**\n",
    "\n",
    "Agora vamos explorar algumas técnicas importantes para melhorar o desempenho do KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 1: Cross-Validation (Validação Cruzada)\n",
    "# ===================================================================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"Cross-Validation: encontrando o melhor K\\n\")\n",
    "print(\"Objetivo:\")\n",
    "print(\"  Dividir dados em 'folds' (partes)\")\n",
    "print(\"  Treinar em alguns folds e testar em outros\")\n",
    "print(\"  Repetir várias vezes e calcular média da acurácia\\n\")\n",
    "\n",
    "k_range = range(1, 31)\n",
    "k_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "best_k = k_range[np.argmax(k_scores)]\n",
    "best_score = max(k_scores)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MELHOR K: {best_k}\")\n",
    "print(f\"Acurácia média (5-fold CV): {best_score*100:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 2: Normalização (Crucial para KNN!)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Normalização: por que é importante para KNN?\\n\")\n",
    "print(\"Problema:\")\n",
    "print(\"  • KNN usa distância Euclidiana\")\n",
    "print(\"  • Features com escalas diferentes dominam o cálculo\")\n",
    "print(\"  • Ex: feature de 0-1000 vs feature de 0-1\\n\")\n",
    "\n",
    "print(\"Solução:\")\n",
    "print(\"  • Normalizar: média 0, desvio padrão 1\")\n",
    "print(\"  • Coloca todas features na mesma escala\\n\")\n",
    "\n",
    "# Criando dados com escalas diferentes\n",
    "from sklearn.datasets import make_classification\n",
    "X_test_scale, y_test_scale = make_classification(n_samples=200, n_features=2, n_informative=2, \n",
    "                                                   n_redundant=0, random_state=42)\n",
    "X_test_scale[:, 1] = X_test_scale[:, 1] * 100  # Segunda feature com escala 100x maior\n",
    "\n",
    "X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(\n",
    "    X_test_scale, y_test_scale, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Sem Normalização:\")\n",
    "knn_without = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_without.fit(X_train_us, y_train_us)\n",
    "acc_without = knn_without.score(X_test_us, y_test_us)\n",
    "print(f\"  Acurácia: {acc_without*100:.2f}%\\n\")\n",
    "\n",
    "print(\"Com Normalização:\")\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train_us)\n",
    "X_test_norm = scaler.transform(X_test_us)\n",
    "\n",
    "knn_with = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_with.fit(X_train_norm, y_train_us)\n",
    "acc_with = knn_with.score(X_test_norm, y_test_us)\n",
    "print(f\"  Acurácia: {acc_with*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"Melhoria de {((acc_with - acc_without) / acc_without * 100):.1f}%\")\n",
    "print(\"\\nSEMPRE normalize seus dados ao usar KNN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 3: Pipeline (Fluxo Completo)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Pipeline: encadeando normalização + modelo\\n\")\n",
    "print(\"Vantagens:\")\n",
    "print(\"  Facilita deployment\")\n",
    "print(\"  Previne data leakage\")\n",
    "print(\"  Garante mesmo pré-processamento em treino e teste\\n\")\n",
    "\n",
    "# Criando pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),           # Etapa 1: Normalização\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))  # Etapa 2: Modelo\n",
    "])\n",
    "\n",
    "# Usando o pipeline (fit + transform + predict em uma linha!)\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions_pipeline = pipeline.predict(X_test)\n",
    "\n",
    "accuracy_pipeline = accuracy_score(y_test, predictions_pipeline)\n",
    "print(\"=\" * 70)\n",
    "print(f\"Acurácia com Pipeline: {accuracy_pipeline*100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nPipeline aplica automaticamente as transformações na ordem correta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vantagens-desvantagens'></a>\n",
    "## **Vantagens e Desvantagens do KNN**\n",
    "\n",
    "Agora que você conhece o KNN profundamente, vamos resumir seus pontos fortes e fracos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **✅ Vantagens**\n",
    "\n",
    "1. **Algoritmo intuitivo e simples:** Fácil de entender e explicar\n",
    "2. **Não há fase de treinamento:** Apenas armazena os dados\n",
    "3. **Flexível:** Funciona bem com dados não-lineares\n",
    "4. **Naturalmente multiclasse:** Sem necessidade de adaptações\n",
    "5. **Atualização incremental:** Novos dados podem ser adicionados facilmente\n",
    "\n",
    "### **❌ Desvantagens**\n",
    "\n",
    "1. **Lento na previsão:** Precisa calcular distância para todos os pontos de treino\n",
    "2. **Sensível à escala:** Features com escalas diferentes dominam o cálculo\n",
    "3. **Maldição da dimensionalidade:** Performance cai drasticamente com muitas features\n",
    "4. **Sensível a dados esparsos:** Ruim quando a maioria dos valores é zero\n",
    "5. **Escolha do K:** Requer tuning, não há valor universalmente ótimo\n",
    "6. **Alto uso de memória:** Precisa armazenar todo o conjunto de treino\n",
    "\n",
    "### **Quando usar KNN?**\n",
    "\n",
    "✅ **Bom para:**\n",
    "- Datasets pequenos a médios\n",
    "- Poucas features (<20)\n",
    "- Fronteiras de decisão irregulares\n",
    "- Baseline rápido\n",
    "\n",
    "❌ **Evitar quando:**\n",
    "- Datasets muito grandes (milhões de exemplos)\n",
    "- Muitas features (centenas/milhares)\n",
    "- Dados esparsos (texto, por exemplo)\n",
    "- Necessidade de previsões muito rápidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<-- [**Anterior: Métricas de Classificação**](../02_Fundamentos_ML/02_metricas_classificacao.ipynb) | [**Próximo: Naive Bayes**](02_naive_bayes.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
