{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Árvores de Decisão — Módulo 3, Notebook 5/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução às Árvores de Decisão](#introducao)\n",
    "2. [Como Funcionam as Árvores de Decisão?](#como-funciona)\n",
    "3. [Intuição Prática](#intuicao)\n",
    "4. [Critérios de Divisão](#criterios)\n",
    "5. [Formalização Matemática](#formalizacao)\n",
    "6. [Implementação com Scikit-Learn](#sklearn)\n",
    "7. [Vantagens e Desvantagens](#vantagens-desvantagens)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução às Árvores de Decisão**\n",
    "\n",
    "Árvores de Decisão são algoritmos de aprendizado supervisionado utilizados para classificação e regressão. Diferentemente dos outros algoritmos estudados, as árvores de decisão funcionam como **um fluxograma de perguntas e respostas** - uma abordagem fundamentalmente intuitiva e humana para tomada de decisões.\n",
    "\n",
    "Imagine um gerente de banco experiente, Sr. Roberto, que precisa decidir se aprova ou rejeita pedidos de crédito. Sua experiência é baseada em anos de intuição, mas como tornar esse processo replicável? A resposta: mapear seu raciocínio em uma série de perguntas estruturadas.\n",
    "\n",
    "**Comparação com outros algoritmos de classificação:**\n",
    "\n",
    "- **KNN**: Baseado em proximidade dos vizinhos mais próximos\n",
    "- **Naive Bayes**: Abordagem probabilística com independência entre features\n",
    "- **Regressão Logística**: Modelo linear que estima probabilidades\n",
    "- **SVM**: Encontra hiperplano ótimo que maximiza margem\n",
    "- **Árvores de Decisão**: Sequência de regras if/else que levam a uma decisão\n",
    "\n",
    "**Características principais:**\n",
    "\n",
    "- **Alta interpretabilidade**: Modelo \"caixa branca\" onde cada decisão é explícita\n",
    "- **Não requer normalização**: Funciona com dados categóricos e numéricos\n",
    "- **Captura não-linearidade**: Pode modelar relacionamentos complexos\n",
    "- **Seleção automática de features**: Ignora features irrelevantes\n",
    "\n",
    "**Conteúdo do notebook:**\n",
    "\n",
    "1. Como as árvores de decisão funcionam (nós, ramos, folhas)\n",
    "2. Intuição prática (aprovação de crédito)\n",
    "3. Critérios de divisão (Gini, Entropia)\n",
    "4. Formalização matemática (algoritmo de construção)\n",
    "5. Implementação com Scikit-Learn (aplicação prática)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Como Funcionam as Árvores de Decisão?**\n",
    "\n",
    "Imagine o Sr. Roberto estruturando seu processo de aprovação de crédito:\n",
    "\n",
    "**Passo 1: A Pergunta Mais Importante (Nó Raiz)**\n",
    "\n",
    "Sr. Roberto pensa: \"Qual é a primeira pergunta que melhor separa bons de maus pagadores?\". Ele percebe que a informação mais impactante é se o cliente possui fonte de renda.\n",
    "\n",
    "- Pergunta: *O cliente tem fonte de renda?*\n",
    "  - **Sim**: Cliente passa para próxima análise\n",
    "  - **Não**: Risco alto → **NEGAR**\n",
    "\n",
    "**Passo 2: Criando os Ramos (Nós Internos)**\n",
    "\n",
    "Para quem respondeu \"Sim\", Sr. Roberto precisa de outra pergunta. Ele foca no grupo promissor:\n",
    "\n",
    "- Pergunta (para quem tem renda): *O nível de dívida atual é alto?*\n",
    "  - **Sim**: Grupo arriscado, requer mais análise\n",
    "  - **Não**: Grupo de baixo risco, chance maior de aprovação\n",
    "\n",
    "**Passo 3: Chegando às Folhas (Decisão Final)**\n",
    "\n",
    "O processo continua ramificando até chegar em decisões finais:\n",
    "\n",
    "- Renda estável + dívida baixa → **APROVAR**\n",
    "- Renda estável + dívida alta + garantia → **APROVAR**  \n",
    "- Renda estável + dívida alta + sem garantia → **NEGAR**\n",
    "\n",
    "---\n",
    "\n",
    "### **Anatomia de uma Árvore de Decisão**\n",
    "\n",
    "- **Root Node (Nó Raiz):**\n",
    "  - Primeiro nó no topo\n",
    "  - Representa a feature mais importante\n",
    "  - De onde todas as decisões começam\n",
    "\n",
    "- **Internal Nodes (Nós Internos):**\n",
    "  - Fazem testes/perguntas sobre features\n",
    "  - Têm pelo menos 2 ramos saindo\n",
    "  - Cada ramo representa uma resposta possível\n",
    "\n",
    "- **Branches (Ramos):**\n",
    "  - Conectam os nós\n",
    "  - Representam as respostas possíveis (Sim/Não, valores)\n",
    "\n",
    "- **Leaf Nodes (Folhas):**\n",
    "  - Nós finais, sem ramos saindo\n",
    "  - Contêm a decisão final\n",
    "  - Exemplo: \"Aprovar\", \"Rejeitar\"\n",
    "\n",
    "- **Depth (Profundidade):**\n",
    "  - Número de níveis da árvore\n",
    "  - Quanto mais profunda, mais complexa é a árvore\n",
    "\n",
    "---\n",
    "\n",
    "### **Algoritmo de Construção**\n",
    "\n",
    "O algoritmo usa uma estratégia **greedy (gulosa)**:\n",
    "\n",
    "1. **Escolhe a 'melhor' feature** para dividir os dados\n",
    "   - \"Melhor\" = aquela que mais reduz impureza\n",
    "\n",
    "2. **Divide os dados** baseado nessa feature\n",
    "\n",
    "3. **Repete recursivamente** para cada subconjunto\n",
    "\n",
    "4. **Para quando:**\n",
    "   - Todos os exemplos são da mesma classe (nó puro)\n",
    "   - Não há mais features para dividir\n",
    "   - Atinge profundidade máxima\n",
    "   - Conjunto se torna muito pequeno (min_samples_split)\n",
    "\n",
    "**IMPUREZA**: Mede o quão misturadas estão as classes\n",
    "- Impureza = 0: Todos na mesma classe (puro)\n",
    "- Impureza máxima: Classes uniformemente distribuídas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição Prática**\n",
    "\n",
    "Para ilustrar o funcionamento das árvores de decisão, vamos utilizar o contexto de aprovação de crédito bancário.\n",
    "\n",
    "### **Contexto do Problema**\n",
    "\n",
    "Dadas as informações de um cliente:\n",
    "- Idade\n",
    "- Renda anual\n",
    "- Nível de dívida\n",
    "- Situação de emprego\n",
    "- Score de crédito\n",
    "\n",
    "**Objetivo:** Classificar como **APROVADO** ou **REJEITADO** para concessão de crédito.\n",
    "\n",
    "### **Análise Exploratória**\n",
    "\n",
    "Primeiro, vamos gerar e visualizar dados sintéticos para entender o problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='criterios'></a>\n",
    "## **Critérios de Divisão**\n",
    "\n",
    "Para decidir qual pergunta fazer em cada nó, a árvore precisa medir quão \"pura\" ou \"impura\" está cada divisão.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gini Impurity**\n",
    "\n",
    "Mede a probabilidade de classificação incorreta ao escolher um elemento aleatoriamente:\n",
    "\n",
    "$$Gini = 1 - \\sum_{i=1}^{k} p_i^2$$\n",
    "\n",
    "**Exemplo:**\n",
    "- [50 aprovados, 50 rejeitados]:\n",
    "  - $Gini = 1 - (0.5^2 + 0.5^2) = 0.5$ (Máxima impureza para 2 classes)\n",
    "\n",
    "- [90 aprovados, 10 rejeitados]:\n",
    "  - $Gini = 1 - (0.9^2 + 0.1^2) = 0.18$ (Baixa impureza, mais puro)\n",
    "\n",
    "---\n",
    "\n",
    "### **Shannon Entropy**\n",
    "\n",
    "Mede o grau de desordem ou incerteza nos dados:\n",
    "\n",
    "$$Entropy = -\\sum_{i=1}^{k} p_i \\log_2(p_i)$$\n",
    "\n",
    "**Exemplo:**\n",
    "- [50 aprovados, 50 rejeitados]:\n",
    "  - $Entropy = -(0.5 \\times \\log_2(0.5) + 0.5 \\times \\log_2(0.5)) = 1$ (Máxima entropia)\n",
    "\n",
    "- [90 aprovados, 10 rejeitados]:\n",
    "  - $Entropy \\approx 0.47$ (Baixa entropia, mais organizado)\n",
    "\n",
    "---\n",
    "\n",
    "### **Information Gain**\n",
    "\n",
    "Mede quanto a divisão reduziu a impureza:\n",
    "\n",
    "$$Gain = Impureza(antes) - Impureza(depois)$$\n",
    "\n",
    "**Objetivo:** Escolher a divisão que maximiza o ganho de informação.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualização Prática**\n",
    "\n",
    "Vamos calcular e visualizar esses conceitos com dados reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dessa vez, vamos gerar os dados para interpretarmos\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "age = np.random.randint(18, 70, n_samples)\n",
    "income = np.random.randint(20000, 120000, n_samples)\n",
    "debt = np.random.randint(0, 50000, n_samples)\n",
    "employed = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # 0: No, 1: Yes\n",
    "credit_score = np.random.choice(['Good', 'Fair', 'Poor'], n_samples, p=[0.4, 0.4, 0.2])\n",
    "\n",
    "approved = []\n",
    "for i in range(n_samples):\n",
    "    score = 0\n",
    "\n",
    "    if income[i] > 60000:\n",
    "        score += 3\n",
    "    if income[i] > 40000:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "\n",
    "    if debt[i] < 15000:\n",
    "        score += 3\n",
    "    elif debt[i] < 30000:\n",
    "        score += 1\n",
    "\n",
    "    if employed[i] == 1:\n",
    "        score += 2\n",
    "        \n",
    "    if credit_score[i] == 'Good':\n",
    "        score += 3\n",
    "    elif credit_score[i] == 'Fair':\n",
    "        score += 1\n",
    "  \n",
    "    approved.append(1 if score >= 6 else 0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Debt': debt,\n",
    "    'Employed': employed,\n",
    "    'Credit_Score': credit_score,\n",
    "    'Approved': approved\n",
    "})\n",
    "\n",
    "print(\"Total de registros:\", len(df))\n",
    "print(f\"\\nDistribuição das decisões:\")\n",
    "print(df['Approved'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular Gini Impurity\n",
    "def gini_impurity(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "def entropy(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    proportions = proportions[proportions > 0]\n",
    "    return -np.sum(proportions * np.log2(proportions))\n",
    "\n",
    "gini_initial = gini_impurity(df['Approved'])\n",
    "entropy_initial = entropy(df['Approved'])\n",
    "print(f\"Gini Impurity (Initial): {gini_initial}\")\n",
    "print(f\"Entropy (Initial): {entropy_initial}\")\n",
    "print(f\"\\nTemos {np.sum(df['Approved'])} aprovações e {len(df) - np.sum(df['Approved'])} rejeições.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Approved'].values\n",
    "threshold_income = 60000\n",
    "high_income_mask = df['Income'] > threshold_income\n",
    "low_income_mask = ~high_income_mask\n",
    "\n",
    "# Grupo: Income > 60000\n",
    "high_income_approved = y[high_income_mask]\n",
    "gini_high = gini_impurity(high_income_approved)\n",
    "print(f\"\\nGrupo 1 (Income > ${threshold_income:,}):\")\n",
    "print(f\"   Total: {len(high_income_approved)} pessoas\")\n",
    "print(f\"   Aprovados: {np.sum(high_income_approved)} ({np.mean(high_income_approved)*100:.1f}%)\")\n",
    "print(f\"   Gini: {gini_high:.4f}\")\n",
    "\n",
    "# Grupo: Income <= 60000\n",
    "low_income_approved = y[low_income_mask]\n",
    "gini_low = gini_impurity(low_income_approved)\n",
    "print(f\"\\nGrupo 2 (Income <= ${threshold_income:,}):\")\n",
    "print(f\"   Total: {len(low_income_approved)} pessoas\")\n",
    "print(f\"   Aprovados: {np.sum(low_income_approved)} ({np.mean(low_income_approved)*100:.1f}%)\")\n",
    "print(f\"   Gini: {gini_low:.4f}\")\n",
    "\n",
    "# Calcular Gini após a divisão\n",
    "gini_after_split = (len(high_income_approved) / len(y)) * gini_high + (len(low_income_approved) / len(y)) * gini_low\n",
    "print(f\"\\nGini após a divisão: {gini_after_split:.4f}\")\n",
    "print(f\"\\nGanho: {gini_initial - gini_after_split:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenho da árvore de decisão\n",
    "\n",
    "def draw_node(ax, x, y, text, width=2, height=0.6, color='lightblue'):\n",
    "    box = FancyBboxPatch((x-width/2, y-height/2), width, height,\n",
    "                         boxstyle=\"round,pad=0.1\", \n",
    "                         edgecolor='black', facecolor=color, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10, \n",
    "           fontweight='bold', wrap=True)\n",
    "\n",
    "def draw_arrow(ax, x1, y1, x2, y2, label=''):\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    if label:\n",
    "        mid_x, mid_y = (x1+x2)/2, (y1+y2)/2\n",
    "        ax.text(mid_x, mid_y, label, ha='center', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Root Node\n",
    "draw_node(ax, 7, 7, 'Income > $60k?', color='#fff3cd')\n",
    "\n",
    "# Ramo esquerdo (Sim)\n",
    "draw_arrow(ax, 6.5, 6.7, 4, 5.3, 'Sim')\n",
    "draw_node(ax, 4, 5, 'Employed = Yes?', color='#d4edda')\n",
    "\n",
    "# Sub-ramos esquerda\n",
    "draw_arrow(ax, 3.5, 4.7, 2.5, 3.3, 'Sim')\n",
    "draw_node(ax, 2.5, 3, 'APROVADO', width=1.8, color='#28a745')\n",
    "\n",
    "draw_arrow(ax, 4.5, 4.7, 5.5, 3.3, 'Não')\n",
    "draw_node(ax, 5.5, 3, 'Debt > $20k', color='#fff3cd')\n",
    "\n",
    "draw_arrow(ax, 5.2, 2.7, 4.5, 1.3, 'Sim')\n",
    "draw_node(ax, 4.5, 1, 'REJEITADO', width=1.8, color='#dc3545')\n",
    "\n",
    "draw_arrow(ax, 5.8, 2.7, 6.5, 1.3, 'Não')\n",
    "draw_node(ax, 6.5, 1, 'APROVADO', width=1.8, color='#28a745')\n",
    "\n",
    "# Ramo direito (Não)\n",
    "draw_arrow(ax, 7.5, 6.7, 10, 5.3, 'Não')\n",
    "draw_node(ax, 10, 5, 'CreditScore = Good?\\n(Gini=0.44)', color='#f8d7da')\n",
    "\n",
    "draw_arrow(ax, 9.5, 4.7, 8.5, 3.3, 'Sim')\n",
    "draw_node(ax, 8.5, 3, 'APROVADO', width=1.8, color='#28a745')\n",
    "\n",
    "draw_arrow(ax, 10.5, 4.7, 11.5, 3.3, 'Não')\n",
    "draw_node(ax, 11.5, 3, 'REJEITADO', width=1.8, color='#dc3545')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Árvore de Decisão - Credit Approval', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrada**\n",
    "\n",
    "1. Conjunto de treinamento de n amostras em pares $D=\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "    onde:\n",
    "\n",
    "    - $x^{(i)}$: vetor de d features ($x^{(i)} = (x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d)$), portanto $x^{(i)} \\in \\mathbb{R}^d$\n",
    "\n",
    "    - $y^{(i)}$: rótulo de classe, $y^{(i)} \\in C = \\{c_1, c_2, ..., c_k\\}$ para classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processamento**\n",
    "\n",
    "O algoritmo de árvore de decisão funciona recursivamente dividindo o espaço de características.\n",
    "\n",
    "**1. Critérios de Impureza**\n",
    "\n",
    "**Entropia**: Para um conjunto $S$ com $k$ classes:\n",
    "\n",
    "$H(S) = -\\sum_{i=1}^{k} p_i \\log_2(p_i)$\n",
    "\n",
    "onde $p_i$ é a proporção de amostras da classe $i$ em $S$.\n",
    "\n",
    "**Gini Impurity**: \n",
    "\n",
    "$Gini(S) = 1 - \\sum_{i=1}^{k} p_i^2$\n",
    "\n",
    "**2. Information Gain**\n",
    "\n",
    "Para uma feature $A$ que divide $S$ em subconjuntos $S_1, S_2, ..., S_m$:\n",
    "\n",
    "$IG(S, A) = H(S) - \\sum_{j=1}^{m} \\frac{|S_j|}{|S|} H(S_j)$\n",
    "\n",
    "**3. Algoritmo de Construção**\n",
    "\n",
    "``` code\n",
    "função BuildTree(S, Features):\n",
    "    se S é puro (todas as amostras têm a mesma classe):\n",
    "        retorna folha com essa classe\n",
    "    \n",
    "    se Features é vazio:\n",
    "        retorna folha com classe majoritária em S\n",
    "    \n",
    "    seleciona feature A que maximiza Information Gain\n",
    "    cria nó com teste em A\n",
    "    \n",
    "    para cada valor v de A:\n",
    "        S_v = subconjunto de S onde A = v\n",
    "        se S_v é vazio:\n",
    "            adiciona folha com classe majoritária em S\n",
    "        senão:\n",
    "            adiciona subárvore BuildTree(S_v, Features - {A})\n",
    "```\n",
    "\n",
    "**4. Critérios de Parada**\n",
    "\n",
    "- Profundidade máxima atingida\n",
    "- Número mínimo de amostras por folha\n",
    "- Information Gain mínimo\n",
    "- Nó puro (todas amostras da mesma classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída - Classificação**\n",
    "\n",
    "- **Estrutura da Árvore**: Conjunto de nós internos (testes) e folhas (predições)\n",
    "\n",
    "- **Predição**: Para uma nova amostra $x^{new}$:\n",
    "  1. Começar na raiz\n",
    "  2. Aplicar o teste do nó atual em $x^{new}$\n",
    "  3. Seguir o ramo correspondente ao resultado\n",
    "  4. Repetir até chegar a uma folha\n",
    "  5. Retornar a classe da folha\n",
    "\n",
    "- **Probabilidades**: Para probabilidades de classe, usar a distribuição das classes na folha\n",
    "\n",
    "**Poda (Pruning)**\n",
    "\n",
    "Para evitar overfitting, aplicamos poda:\n",
    "\n",
    "**Pré-poda**: Parar o crescimento da árvore antecipadamente\n",
    "- Profundidade máxima\n",
    "- Número mínimo de amostras por nó\n",
    "- Information gain mínimo\n",
    "\n",
    "**Pós-poda**: Remover partes da árvore após construção\n",
    "- Cost complexity pruning (usado no sklearn)\n",
    "- Validação cruzada para escolher melhor complexidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para Teste - Credit Approval Dataset\n",
    "# ==================================================\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch dataset\n",
    "credit_approval = fetch_ucirepo(id=27) \n",
    "\n",
    "# Data (as pandas dataframes) \n",
    "X = credit_approval.data.features \n",
    "y = credit_approval.data.targets\n",
    "\n",
    "# Simplificar dataset para fins didáticos\n",
    "# Selecionar algumas features importantes e tratar valores ausentes\n",
    "X_simple = X[['A2', 'A3', 'A8', 'A11', 'A14', 'A15']].copy()\n",
    "X_simple.columns = ['Age', 'Years_Employed', 'Debt_to_Income', 'Credit_Score', 'Income', 'Debt']\n",
    "\n",
    "# Tratar valores ausentes - substituir por mediana para numéricos\n",
    "for col in X_simple.columns:\n",
    "    if X_simple[col].dtype in ['float64', 'int64']:\n",
    "        X_simple[col] = pd.to_numeric(X_simple[col], errors='coerce')\n",
    "        X_simple[col] = X_simple[col].fillna(X_simple[col].median())\n",
    "\n",
    "# Remover amostras com muitos valores ausentes\n",
    "X_simple = X_simple.dropna()\n",
    "y_simple = y.loc[X_simple.index]\n",
    "\n",
    "# Converter target para binário (aprovado/negado)\n",
    "le = LabelEncoder()\n",
    "y_binary = le.fit_transform(y_simple.values.ravel())\n",
    "\n",
    "print(f\"Dataset shape: {X_simple.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Distribuição das classes: {np.bincount(y_binary)}\")\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simple.values, y_binary, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Implementação com Scikit-Learn**\n",
    "\n",
    "Nesta seção, aplicaremos árvores de decisão utilizando Scikit-Learn no dataset Credit Approval para prever aprovação de crédito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import export_text\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 1. Implementação Básica\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "predictions = tree.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Acurácia: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 2. Visualização do Modelo\n",
    "\n",
    "tree_small = DecisionTreeClassifier(max_depth=3, random_state=42) #melhorar visualização\n",
    "tree_small.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRegras da Árvore de Decisão:\")\n",
    "rules = export_text(tree_small, feature_names=list(X_simple.columns))\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 3. Parâmetros Importantes da Árvore de Decisão\n",
    "\n",
    "# criterion (default='gini')\n",
    "# - 'gini': Gini impurity\n",
    "# - 'entropy': Information gain\n",
    "# - 'log_loss': Log loss\n",
    "\n",
    "# max_depth (default=None)\n",
    "# - Profundidade máxima da árvore\n",
    "# - None = sem limite (pode causar overfitting)\n",
    "\n",
    "# min_samples_split (default=2)\n",
    "# - Número mínimo de amostras necessárias para dividir um nó interno\n",
    "\n",
    "# min_samples_leaf (default=1)\n",
    "# - Número mínimo de amostras necessárias em uma folha\n",
    "\n",
    "# max_features (default=None)\n",
    "# - Número de features a considerar ao procurar a melhor divisão\n",
    "# - None: usa todas as features\n",
    "\n",
    "# ccp_alpha (default=0.0)\n",
    "# - Parâmetro de complexidade para poda de custo mínimo\n",
    "# =================================================\n",
    "\n",
    "# Testar diferentes critérios\n",
    "criteria = ['gini', 'entropy']\n",
    "results = []\n",
    "\n",
    "for criterion in criteria:\n",
    "    dt = DecisionTreeClassifier(criterion=criterion, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    accuracy = dt.score(X_test, y_test)\n",
    "    depth = dt.get_depth()\n",
    "    leaves = dt.get_n_leaves()\n",
    "    \n",
    "    results.append((criterion, accuracy, depth, leaves))\n",
    "    print(f\"Critério {criterion:8}: Acurácia = {accuracy*100:5.2f}%, Profundidade = {depth:2d}, Folhas = {leaves:3d}\")\n",
    "\n",
    "# Testar diferentes profundidades máximas\n",
    "max_depths = [3, 5, 7, 10, None]\n",
    "print(f\"\\nTestando diferentes profundidades máximas:\")\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_acc = dt.score(X_train, y_train)\n",
    "    test_acc = dt.score(X_test, y_test)\n",
    "    depth = dt.get_depth()\n",
    "    leaves = dt.get_n_leaves()\n",
    "    \n",
    "    max_depth_str = str(max_depth) if max_depth else \"None\"\n",
    "    print(f\"Max_depth = {max_depth_str:4}: Train = {train_acc*100:5.2f}%, Test = {test_acc*100:5.2f}%, \"\n",
    "          f\"Profundidade Real = {depth:2d}, Folhas = {leaves:3d}\")\n",
    "\n",
    "print(f\"\"\"\\nNotas: \n",
    "      \\n - como profundidades menores podem ter melhor generalização (menor overfitting)\n",
    "      \\n - Note como a árvore sem limites de profundade tem 100% de acurácia no treino, mas pior no teste. (decorou os dados)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =================================================\n",
    "# 4. FEATURE IMPORTANCE\n",
    "# =================================================\n",
    "\n",
    "tree_analysis = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_analysis.fit(X_train, y_train)\n",
    "\n",
    "# Importar bibliotecas necessárias\n",
    "importances = tree_analysis.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Exibir as características mais importantes (limitado ao número de features disponíveis)\n",
    "print(\"Características mais importantes:\")\n",
    "for f in range(min(10, len(importances))):\n",
    "    print(f\"{f + 1}. {X_simple.columns[indices[f]]} ({importances[indices[f]]:.4f})\")\n",
    "\n",
    "# Plotar a importância das características usando gráfico ASCII\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRÁFICO DE IMPORTÂNCIA DAS CARACTERÍSTICAS (ASCII)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_bar_length = 50\n",
    "for f in range(len(importances)):\n",
    "    idx = indices[f]\n",
    "    importance = importances[idx]\n",
    "    bar_length = int(importance * max_bar_length / importances[indices[0]])\n",
    "    bar = '█' * bar_length\n",
    "    print(f\"{X_simple.columns[idx]:20} | {bar} {importance:.4f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 5. Otimização de Hiperparâmetros com Grid Search\n",
    "# =================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir espaço de busca\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid search com validação cruzada\n",
    "dt_grid = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt_grid, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Melhor score CV: {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Acurácia no teste: {accuracy_best*100:.2f}%\")\n",
    "\n",
    "# Informações sobre a melhor árvore\n",
    "print(f\"\\nInformações da melhor árvore:\")\n",
    "print(f\"Profundidade: {best_dt.get_depth()}\")\n",
    "print(f\"Número de folhas: {best_dt.get_n_leaves()}\")\n",
    "print(f\"Número de nós: {best_dt.tree_.node_count}\")\n",
    "\n",
    "# Relatório detalhado\n",
    "print(f\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Negado', 'Aprovado']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 5. Análise de Casos Específicos e Interpretabilidade\n",
    "# =================================================\n",
    "\n",
    "# Vamos analisar alguns casos específicos para entender as decisões da árvore\n",
    "test_cases = [\n",
    "    [35, 5, 0.3, 700, 50000, 15000],  # Caso favorável: idade média, empregado há tempo, baixo debt-to-income\n",
    "    [25, 1, 0.8, 500, 25000, 20000],  # Caso desfavorável: jovem, pouco tempo empregado, alto debt-to-income\n",
    "    [45, 10, 0.2, 800, 80000, 16000], # Caso muito favorável: idade madura, muito tempo empregado, baixo debt-to-income\n",
    "    [22, 0.5, 0.9, 450, 20000, 18000] # Caso muito desfavorável: muito jovem, pouco tempo empregado, muito alto debt-to-income\n",
    "]\n",
    "\n",
    "feature_names = ['Age', 'Years_Employed', 'Debt_to_Income', 'Credit_Score', 'Income', 'Debt']\n",
    "\n",
    "print(\"Análise de Casos Específicos:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    # Fazer predição\n",
    "    prediction = best_dt.predict([case])[0]\n",
    "    probabilities = best_dt.predict_proba([case])[0]\n",
    "    \n",
    "    # Mostrar o caminho da decisão\n",
    "    decision_path = best_dt.decision_path([case])\n",
    "    leaf_id = best_dt.apply([case])[0]\n",
    "    \n",
    "    result_names = {0: \"Negado\", 1: \"Aprovado\"}\n",
    "    \n",
    "    print(f\"Caso {i+1}:\")\n",
    "    print(f\"  Características:\")\n",
    "    for j, (feature, value) in enumerate(zip(feature_names, case)):\n",
    "        print(f\"    {feature}: {value}\")\n",
    "    \n",
    "    print(f\"  Predição: {result_names[prediction]}\")\n",
    "    print(f\"  Probabilidades: Negado={probabilities[0]:.3f}, Aprovado={probabilities[1]:.3f}\")\n",
    "    print(f\"  Confiança: {max(probabilities):.3f}\")\n",
    "    print(f\"  {'*' * 60}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negado', 'Aprovado'],\n",
    "            yticklabels=['Negado', 'Aprovado'])\n",
    "plt.title('Matriz de Confusão - Árvore de Decisão Otimizada')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predição')\n",
    "plt.show()\n",
    "\n",
    "# Curva de learning\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_dt, X_train, y_train, cv=5, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Score de Treino')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Score de Validação')\n",
    "plt.xlabel('Tamanho do Conjunto de Treino')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.title('Curva de Aprendizado - Árvore de Decisão')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vantagens-desvantagens'></a>\n",
    "## **Vantagens e Desvantagens**\n",
    "\n",
    "### **Vantagens**\n",
    "\n",
    "1. **Alta Interpretabilidade**: Fácil de entender e explicar para stakeholders não-técnicos\n",
    "2. **Não Requer Preparação Extensiva dos Dados**: Funciona com dados categóricos e numéricos sem normalização\n",
    "3. **Captura Relacionamentos Não-Lineares**: Pode modelar interações complexas entre features\n",
    "4. **Seleção Automática de Features**: Ignora features irrelevantes automaticamente\n",
    "5. **Robusta a Outliers**: Divisões baseadas em rankings, não em valores absolutos\n",
    "6. **Suporta Dados Ausentes**: Pode lidar com valores faltantes naturalmente\n",
    "\n",
    "### **Desvantagens**\n",
    "\n",
    "1. **Tendência ao Overfitting**: Especialmente com árvores profundas\n",
    "2. **Instabilidade**: Pequenas mudanças nos dados podem resultar em árvores muito diferentes\n",
    "3. **Bias para Features com Mais Categorias**: Features com mais valores únicos têm vantagem\n",
    "4. **Dificuldade com Relacionamentos Lineares**: Pode ser ineficiente para padrões lineares simples\n",
    "5. **Predições \"Escalonadas\"**: Só pode prever valores que existem no conjunto de treino\n",
    "6. **Não captura interações aditivas**: Melhor para interações multiplicativas\n",
    "\n",
    "---\n",
    "\n",
    "### **Quando usar Árvores de Decisão?**\n",
    "\n",
    "**Casos recomendados:**\n",
    "- Quando interpretabilidade é prioritária\n",
    "- Dados com mix de features categóricas e numéricas\n",
    "- Presença de interações não-lineares entre features\n",
    "- Necessidade de explicar decisões para stakeholders\n",
    "- Datasets pequenos a médios\n",
    "\n",
    "**Casos não recomendados:**\n",
    "- Quando máxima acurácia é crítica (preferir ensembles)\n",
    "- Dados com muitas features numéricas correlacionadas\n",
    "- Necessidade de probabilidades bem calibradas\n",
    "- Features em escalas muito diferentes (embora não seja crítico)\n",
    "\n",
    "---\n",
    "\n",
    "### **Controle de Overfitting**\n",
    "\n",
    "**Pré-poda (Pre-pruning):**\n",
    "- `max_depth`: Limitar profundidade máxima\n",
    "- `min_samples_split`: Mínimo de amostras para dividir nó\n",
    "- `min_samples_leaf`: Mínimo de amostras em folha\n",
    "- `max_features`: Limitar features consideradas\n",
    "\n",
    "**Pós-poda (Post-pruning):**\n",
    "- `ccp_alpha`: Cost complexity pruning\n",
    "- Validação cruzada para escolher melhor complexidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Resumo**\n",
    "\n",
    "Neste notebook foram abordados:\n",
    "\n",
    "- Conceitos fundamentais das Árvores de Decisão (nós, ramos, folhas)\n",
    "- Funcionamento do algoritmo (estratégia greedy, divisões recursivas)\n",
    "- Critérios de divisão (Gini Impurity, Entropia, Information Gain)\n",
    "- Formalização matemática (algoritmo de construção, poda)\n",
    "- Implementação prática com Scikit-Learn\n",
    "- Vantagens e limitações do algoritmo\n",
    "\n",
    "**Conceitos principais:**\n",
    "- **Nó Raiz**: Primeira divisão com feature mais importante\n",
    "- **Impureza**: Medida de mistura de classes em um nó\n",
    "- **Information Gain**: Redução de impureza após divisão\n",
    "- **Overfitting**: Árvores profundas memorizam ruído\n",
    "- **Poda**: Técnica para controlar complexidade\n",
    "- **Interpretabilidade**: Principal vantagem das árvores\n",
    "\n",
    "---\n",
    "\n",
    "## **Próximos Passos**\n",
    "\n",
    "No próximo notebook, serão apresentados **Métodos de Ensemble** - combinando múltiplas árvores para melhorar desempenho.\n",
    "\n",
    "---\n",
    "\n",
    "<-- [**Anterior: SVM**](04_svm.ipynb) | [**Próximo: Ensembles de Classificação**](06_ensemble_classificacao.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
