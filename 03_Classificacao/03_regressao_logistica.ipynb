{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regressão Logística — Módulo 3, Notebook 3/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução à Regressão Logística](#introducao)\n",
    "2. [Como a Regressão Logística Funciona?](#como-funciona)\n",
    "3. [Intuição Prática: Sobreviventes do Titanic](#intuicao)\n",
    "4. [Formalização Matemática](#formalizacao)\n",
    "5. [Implementação com Scikit-Learn](#sklearn)\n",
    "6. [Técnicas Avançadas](#tecnicas)\n",
    "7. [Vantagens e Desvantagens](#vantagens-desvantagens)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução à Regressão Logística**\n",
    "\n",
    "Imagine que você trabalha em um hospital e precisa prever se um paciente terá complicações cardíacas com base em idade, pressão arterial e colesterol. Ou talvez você queira prever se um email é spam baseado nas palavras que contém. Esses são problemas de **classificação binária** — a resposta é \"sim\" ou \"não\", 0 ou 1.\n",
    "\n",
    "A **Regressão Logística** é um dos algoritmos mais populares para resolver esse tipo de problema. Apesar do nome \"regressão\", ela é usada para **classificação** e nos fornece a **probabilidade** de uma amostra pertencer a uma determinada classe.\n",
    "\n",
    "Diferentemente da regressão linear que prevê valores contínuos (como o preço de uma casa), a Regressão Logística prevê a probabilidade de uma entrada pertencer a uma classe específica. Essa probabilidade é sempre um valor entre 0 e 1, obtido através da **função sigmoide**:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "A saída do modelo é uma probabilidade, e a classificação final é feita com um limiar (threshold), geralmente 0.5:\n",
    "\n",
    "- Se $P(Y=1) \\geq 0.5$, a classe prevista será 1\n",
    "- Se $P(Y=1) < 0.5$, a classe prevista será 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Como a Regressão Logística Funciona?**\n",
    "\n",
    "A Regressão Logística funciona em três etapas fundamentais:\n",
    "\n",
    "### **Etapa 1: Combinação Linear (Score)**\n",
    "\n",
    "Primeiro, o algoritmo calcula um \"score\" combinando as features da amostra com pesos aprendidos:\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = \\mathbf{w}^T\\mathbf{x} + b$$\n",
    "\n",
    "Este valor $z$ é chamado de **log-odds** (logaritmo da razão de chances) e pode variar de $-\\infty$ a $+\\infty$.\n",
    "\n",
    "### **Etapa 2: Transformação pela Função Sigmoide**\n",
    "\n",
    "O score $z$ não é intuitivo. Para obter uma probabilidade (valor entre 0 e 1), aplicamos a **função sigmoide**:\n",
    "\n",
    "$$P(Y=1|X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Propriedades importantes da sigmoide:\n",
    "- Quando $z$ é muito positivo → $\\sigma(z) \\approx 1$ (alta probabilidade da classe 1)\n",
    "- Quando $z$ é muito negativo → $\\sigma(z) \\approx 0$ (baixa probabilidade da classe 1)\n",
    "- Quando $z = 0$ → $\\sigma(z) = 0.5$ (probabilidade neutra)\n",
    "\n",
    "### **Etapa 3: Classificação via Threshold**\n",
    "\n",
    "Com a probabilidade em mãos, aplicamos um limiar de decisão (geralmente 0.5):\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} \n",
    "1, & \\text{se } P(Y=1|X) \\geq 0.5 \\\\\n",
    "0, & \\text{se } P(Y=1|X) < 0.5\n",
    "\\end{cases}$$\n",
    "\n",
    "### **Log-Odds: A Intuição Matemática**\n",
    "\n",
    "Podemos reescrever a equação de regressão logística em termos da razão de chances (odds ratio):\n",
    "\n",
    "$$\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$$\n",
    "\n",
    "Onde:\n",
    "- $P(Y=1)$ é a probabilidade do evento ocorrer\n",
    "- $\\frac{P(Y=1)}{1 - P(Y=1)}$ representa a razão de chances (odds ratio)\n",
    "- $\\log(\\frac{P(Y=1)}{1 - P(Y=1)})$ é o log-odds, que se relaciona linearmente com as features\n",
    "\n",
    "**Interpretação dos coeficientes:**\n",
    "\n",
    "Se $\\beta_1 = 0.3$, para cada aumento de 1 unidade em $X_1$, o log-odds aumenta em 0.3. Para interpretar diretamente o efeito na razão de chances, aplicamos a exponenciação:\n",
    "\n",
    "$$e^{\\beta_1} = e^{0.3} \\approx 1.35$$\n",
    "\n",
    "Isso significa que as chances do evento positivo aumentam em aproximadamente 35% para cada unidade adicional de $X_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição Prática: Sobreviventes do Titanic**\n",
    "\n",
    "Vamos consolidar esse conhecimento através de um exemplo prático e histórico: o naufrágio do Titanic em 1912.\n",
    "\n",
    "### **O Problema**\n",
    "\n",
    "Imagine que você tem informações sobre um passageiro:\n",
    "\n",
    "- **Nome:** Jack\n",
    "- **Idade:** 22 anos\n",
    "- **Classe:** 3ª classe\n",
    "- **Sexo:** Masculino\n",
    "\n",
    "**Pergunta:** Será que conseguimos prever se Jack sobreviveu ao naufrágio baseado apenas nessas características?\n",
    "\n",
    "Para responder isso, vamos:\n",
    "\n",
    "1. Analisar os dados históricos dos passageiros do Titanic\n",
    "2. Identificar padrões de sobrevivência\n",
    "3. Treinar um modelo de Regressão Logística\n",
    "4. Fazer a predição para Jack\n",
    "\n",
    "Vamos construir o raciocínio do algoritmo passo a passo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset do Titanic\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"yasserh/titanic-dataset\",\n",
    "    \"Titanic-Dataset.csv\",\n",
    "    pandas_kwargs={\"encoding\": \"latin\", \"usecols\": [1, 2, 4, 5]}\n",
    ")\n",
    "\n",
    "print(f\"Total de passageiros: {len(df)}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise exploratória inicial\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColunas disponíveis:\", df.columns.tolist())\n",
    "print(\"\\nInformações do dataset:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ponderando as Evidências**\n",
    "\n",
    "Antes de treinar o modelo, vamos entender quais características influenciam a sobrevivência. A Regressão Logística aprenderá um \"peso\" para cada feature, indicando sua importância e direção:\n",
    "\n",
    "- **Sexo = Feminino:** Será que ser mulher aumentava as chances de sobrevivência? (\"Mulheres e crianças primeiro\")\n",
    "- **Classe = 1ª:** Passageiros da primeira classe tinham acesso prioritário aos botes salva-vidas?\n",
    "- **Classe = 3ª:** Estar na terceira classe diminuiria as chances de sobreviver?\n",
    "- **Idade:** Crianças e jovens teriam prioridade?\n",
    "\n",
    "Vamos analisar os dados para descobrir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de sobrevivência por categoria\n",
    "\n",
    "print(\"TAXA DE SOBREVIVÊNCIA POR SEXO:\")\n",
    "survival_by_sex = df.groupby(\"Sex\")[\"Survived\"].agg([\"mean\", \"count\"])\n",
    "survival_by_sex[\"mean\"] *= 100\n",
    "print(survival_by_sex)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTAXA DE SOBREVIVÊNCIA POR CLASSE:\")\n",
    "survival_by_class = df.groupby(\"Pclass\")[\"Survived\"].agg([\"mean\", \"count\"])\n",
    "survival_by_class[\"mean\"] *= 100\n",
    "print(survival_by_class)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTAXA DE SOBREVIVÊNCIA POR FAIXA ETÁRIA:\")\n",
    "df['Age_group'] = pd.cut(df['Age'], bins=[0, 16, 35, 60, 100], \n",
    "                         labels=['Child(0-16)', 'Young Adult(17-35)', 'Adult(36-60)', 'Senior(61+)'])\n",
    "\n",
    "survival_by_age = df.groupby(\"Age_group\", observed=True)[\"Survived\"].agg([\"mean\", \"count\"])\n",
    "survival_by_age[\"mean\"] *= 100\n",
    "print(survival_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações das taxas de sobrevivência\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Análise de Sobrevivência — Titanic Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Gráfico 1: Sobrevivência por Gênero\n",
    "ax1 = axes[0, 0]\n",
    "survival_sex = df.groupby('Sex')['Survived'].mean() * 100\n",
    "bars = ax1.bar(survival_sex.index, survival_sex.values, color=['lightcoral', 'skyblue'], \n",
    "               alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Taxa de Sobrevivência (%)')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_title('Taxa de Sobrevivência por Gênero', fontsize=14)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, height + 1, \n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 2: Sobrevivência por Classe\n",
    "ax2 = axes[0, 1]\n",
    "survival_class = df.groupby('Pclass')['Survived'].mean() * 100\n",
    "bars = ax2.bar(survival_class.index.astype(str), survival_class.values, \n",
    "               color=['lightgreen', 'orange', 'lightgrey'], alpha=0.7, \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Taxa de Sobrevivência (%)')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_title('Taxa de Sobrevivência por Classe do Passageiro', fontsize=14)\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, height + 1, \n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 3: Distribuição de Idade por Status de Sobrevivência\n",
    "ax3 = axes[1, 0]\n",
    "survived = df[df['Survived'] == 1]['Age']\n",
    "died = df[df['Survived'] == 0]['Age']\n",
    "ax3.hist([died, survived], bins=20, stacked=True, \n",
    "         color=['lightcoral', 'lightblue'], edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Idade')\n",
    "ax3.set_ylabel('Número de Passageiros')\n",
    "ax3.set_title('Distribuição de Idade por Status de Sobrevivência', fontsize=14)\n",
    "ax3.legend(['Não Sobreviveu', 'Sobreviveu'])\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Gráfico 4: Heatmap Classe vs Gênero\n",
    "ax4 = axes[1, 1]\n",
    "pivot = df.pivot_table(values='Survived', index='Pclass', columns='Sex', aggfunc='mean')\n",
    "sns.heatmap(pivot * 100, annot=True, fmt=\".1f\", cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Taxa de Sobrevivência (%)'}, ax=ax4, \n",
    "            linewidths=.5, linecolor='black')\n",
    "ax4.set_title('Taxa de Sobrevivência por Classe e Gênero', fontsize=14)\n",
    "ax4.set_ylabel('Classe do Passageiro')\n",
    "ax4.set_xlabel('Gênero')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Insights dos Dados**\n",
    "\n",
    "Os gráficos revelam padrões claros que a Regressão Logística irá aprender:\n",
    "\n",
    "**1. Gênero foi crucial:**\n",
    "- Mulheres: **74.2%** de sobrevivência\n",
    "- Homens: **18.9%** de sobrevivência\n",
    "- O protocolo \"mulheres e crianças primeiro\" foi real!\n",
    "\n",
    "**2. Classe social importava:**\n",
    "- 1ª classe: **63.0%** de sobrevivência\n",
    "- 2ª classe: **47.3%** de sobrevivência\n",
    "- 3ª classe: **24.2%** de sobrevivência\n",
    "- Acesso aos botes salva-vidas era desigual\n",
    "\n",
    "**3. Idade tinha influência:**\n",
    "- Crianças (0-16 anos): **55%** de sobrevivência\n",
    "- Adultos jovens (17-35 anos): **37%** de sobrevivência\n",
    "- Crianças receberam prioridade\n",
    "\n",
    "Esses padrões serão transformados em **pesos** pelo algoritmo de Regressão Logística!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entendendo a Função Sigmoide Visualmente**\n",
    "\n",
    "Antes de treinar o modelo, vamos visualizar como a função sigmoide transforma os scores (log-odds) em probabilidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da função sigmoide\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Função sigmoide: transforma qualquer valor real em probabilidade (0-1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z_values = np.linspace(-10, 10, 400)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(z_values, sigmoid_values, 'b-', linewidth=2, label='Função Sigmóide')\n",
    "ax.axhline(0.5, color='black', linestyle='--', linewidth=2, alpha=0.7, \n",
    "           label='Limiar de decisão (0.5)')\n",
    "ax.axvline(0, color='black', linewidth=1, linestyle='--', alpha=0.5)\n",
    "ax.fill_between(z_values, 0, sigmoid_values, where=(sigmoid_values >= 0.5), \n",
    "                color='lightblue', alpha=0.5, label='Classe 1 (Sobreviveu)')\n",
    "ax.fill_between(z_values, 0, sigmoid_values, where=(sigmoid_values < 0.5), \n",
    "                color='lightcoral', alpha=0.5, label='Classe 0 (Não Sobreviveu)')\n",
    "\n",
    "# Exemplos de pontos específicos\n",
    "examples = [\n",
    "    (-6, sigmoid(-6), \"z=-6\\nScore muito negativo\"), \n",
    "    (-3, sigmoid(-3), \"z=-3\"), \n",
    "    (0, sigmoid(0), \"z=0\\nPonto neutro\"), \n",
    "    (3, sigmoid(3), \"z=3\"), \n",
    "    (6, sigmoid(6), \"z=6\\nScore muito positivo\")\n",
    "]\n",
    "\n",
    "for z, s, label in examples:  \n",
    "    ax.plot(z, s, 'ro', markersize=10)\n",
    "    ax.annotate(f'{label}\\nP = {s:.3f}', xy=(z, s), xytext=(z, s + 0.05),\n",
    "                arrowprops=dict(facecolor='wheat', arrowstyle='->'),\n",
    "                fontsize=9, ha='center')\n",
    "\n",
    "ax.set_title('Função Sigmóide: De Score para Probabilidade', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Score (z) = w₁x₁ + w₂x₂ + ... + b', fontsize=14)\n",
    "ax.set_ylabel('Probabilidade σ(z)', fontsize=14)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretação:**\n",
    "\n",
    "- **z muito negativo (ex: -6):** Score baixo → Probabilidade ~0% → Classe 0\n",
    "- **z = 0:** Score neutro → Probabilidade 50% → Ponto de decisão\n",
    "- **z muito positivo (ex: +6):** Score alto → Probabilidade ~100% → Classe 1\n",
    "\n",
    "Agora que entendemos a sigmoide, vamos ver a formalização matemática completa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**\n",
    "\n",
    "Agora que você entendeu intuitivamente como a Regressão Logística funciona, vamos formalizar o algoritmo matematicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log-Odds (Logit)**\n",
    "\n",
    "A odd de um evento é a razão entre a probabilidade de ele ocorrer $(p)$ e a probabilidade de não ocorrer $(1-p)$\n",
    "\n",
    "$ Odds = \\frac{p}{1-p}$\n",
    "\n",
    "A Regressão Logística assume que o logaritmo da odd (**log-odds ou logit**) é uma função linear das features:\n",
    "\n",
    "$\\ln \\left(\\frac{p}{1-p}\\right) = \\mathbf{w}^T\\mathbf{x}+b$\n",
    "\n",
    "Onde $\\mathbf{w}$ e $\\mathbf{x}$ são os vetores de pesos e features respectivamente.\n",
    "\n",
    "Note que, ao isolar $p$ na equação, obtemos exatamente a função sigmoide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrada**\n",
    "\n",
    "**Conjunto de treinamento:** $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), ..., (\\mathbf{x}^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "Onde:\n",
    "- $\\mathbf{x}^{(i)} = (x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d)$: vetor de $d$ features da amostra $i$\n",
    "- Adicionamos o termo de intercepto $x_0^{(i)} = 1$, portanto $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{d+1}$\n",
    "- $y^{(i)} \\in \\{0, 1\\}$: rótulo de classe binário (0 ou 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processamento**\n",
    "\n",
    "O processamento consiste em aprender um vetor de parâmetros (pesos) $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ que melhor mapeia as entradas $\\mathbf{x}$ para saídas $y$.\n",
    "\n",
    "#### **1. Hipótese do Modelo**\n",
    "\n",
    "A função hipótese $h_{\\mathbf{w}, b}(\\mathbf{x})$ retorna a probabilidade de $y=1$ dada a amostra $\\mathbf{x}$:\n",
    "\n",
    "$$h_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T\\mathbf{x} + b)}}$$\n",
    "\n",
    "Onde:\n",
    "- $\\mathbf{w}$: vetor de pesos\n",
    "- $b$: bias (intercepto)\n",
    "- $\\sigma$: função sigmoide\n",
    "\n",
    "#### **2. Função de Custo (Log-Loss)**\n",
    "\n",
    "Para uma única amostra de treinamento $(\\mathbf{x}, y)$, a função custo é definida como:\n",
    "\n",
    "$$L(\\hat{p}, y) = -[y\\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})]$$\n",
    "\n",
    "Essa função penaliza predições erradas:\n",
    "- Se a classe real é $y=1$, a perda é $-\\log(\\hat{p})$. Se o modelo prevê $\\hat{p} \\to 0$, a perda tende a $\\infty$\n",
    "- Se a classe real é $y=0$, a perda é $-\\log(1-\\hat{p})$. Se o modelo prevê $\\hat{p} \\to 1$, a perda tende a $\\infty$\n",
    "\n",
    "A função custo total $J(\\mathbf{w}, b)$ para o conjunto de dados inteiro é a média das perdas individuais:\n",
    "\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)}\\log(h_{\\mathbf{w}, b}(\\mathbf{x}^{(i)})) + (1-y^{(i)})\\log(1-h_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}))]$$\n",
    "\n",
    "#### **3. Otimização via Gradiente Descendente**\n",
    "\n",
    "Para minimizar $J(\\mathbf{w}, b)$, utilizamos o algoritmo **Gradiente Descendente**. Ele atualiza iterativamente os parâmetros na direção oposta ao gradiente da função de custo.\n",
    "\n",
    "As derivadas parciais da função de custo em relação a cada peso $w_j$ e ao bias $b$ são:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^{n}(h_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}(h_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "As regras de atualização a cada iteração são:\n",
    "\n",
    "$$w_j := w_j - \\alpha\\frac{\\partial J}{\\partial w_j}$$\n",
    "\n",
    "$$b := b - \\alpha\\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "Onde $\\alpha$ é a **taxa de aprendizado** (learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saída — Classificação**\n",
    "\n",
    "**Parâmetros do Modelo:** Um vetor de pesos $\\mathbf{w}$ e um bias $b$ otimizados.\n",
    "\n",
    "**Predição:** Para uma nova amostra $\\mathbf{x}^o$, o modelo calcula a probabilidade:\n",
    "\n",
    "$$\\hat{p} = \\sigma(\\mathbf{w}^T\\mathbf{x}^o + b)$$\n",
    "\n",
    "**Classificação:** Um limiar de decisão (threshold), geralmente 0.5, é aplicado à probabilidade para obter a classe final:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} \n",
    "1, & \\text{se } \\hat{p} \\geq 0.5 \\\\\n",
    "0, & \\text{se } \\hat{p} < 0.5\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Considerações Importantes**\n",
    "\n",
    "#### **1. Interpretação dos Coeficientes**\n",
    "\n",
    "Os pesos ($w_j$) podem ser interpretados de forma intuitiva. Quando exponenciados, obtemos a razão de chances (odds ratio):\n",
    "\n",
    "- Para cada aumento de uma unidade em $x_j$, as odds do resultado ser $y=1$ são multiplicadas por $e^{w_j}$\n",
    "\n",
    "**Exemplo:** Se $w_{\\text{idade}} = -0.05$, então $e^{-0.05} \\approx 0.95$. Isso significa que para cada ano adicional de idade, as odds de sobreviver diminuem em 5%.\n",
    "\n",
    "#### **2. Fronteira de Decisão (Decision Boundary)**\n",
    "\n",
    "A fronteira de decisão é a superfície que separa as classes. No caso da Regressão Logística, ela ocorre onde a probabilidade é exatamente 0.5, o que corresponde a:\n",
    "\n",
    "$$\\mathbf{w}^T\\mathbf{x} + b = 0$$\n",
    "\n",
    "Note que a fronteira de decisão da Regressão Logística é **linear**.\n",
    "\n",
    "#### **3. Regularização (L1 e L2)**\n",
    "\n",
    "Para evitar overfitting, especialmente com um grande número de features, podemos adicionar termos de regularização:\n",
    "\n",
    "- **Ridge (L2):** Adiciona $\\frac{\\lambda}{2n} \\sum_{j=1}^{d}w_j^2$ para penalizar pesos grandes\n",
    "- **Lasso (L1):** Adiciona $\\frac{\\lambda}{n} \\sum_{j=1}^{d}|w_j|$ para forçar pesos a se tornarem exatamente zero\n",
    "\n",
    "#### **4. Extensão para Classificação Multiclasse**\n",
    "\n",
    "A Regressão Logística pode ser generalizada para problemas com mais de duas classes através da **Regressão Softmax**. Ela usa a função Softmax, em vez da sigmoide, para calcular a probabilidade de cada uma das $K$ classes:\n",
    "\n",
    "$$P(y=k|\\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K}e^{z_j}}$$\n",
    "\n",
    "Onde $z_k = \\mathbf{w}_k^T\\mathbf{x} + b_k$ é o score para a classe $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Implementação com Scikit-Learn**\n",
    "\n",
    "Na prática, usamos bibliotecas otimizadas como o Scikit-Learn. Vamos implementar uma solução completa para o problema do Titanic e **finalmente responder se Jack sobreviveu!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparação dos Dados**\n",
    "\n",
    "Vamos preparar o dataset do Titanic para treinar nosso modelo de Regressão Logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos dados\n",
    "# Remover linhas com valores faltantes em Age\n",
    "df_clean = df.dropna(subset=['Age'])\n",
    "\n",
    "# Codificar a coluna Sex (male=1, female=0)\n",
    "df_clean['Sex'] = (df_clean['Sex'] == 'male').astype(int)\n",
    "\n",
    "# Selecionar features (X) e target (y)\n",
    "X = df_clean[['Pclass', 'Sex', 'Age']].values\n",
    "y = df_clean['Survived'].values\n",
    "\n",
    "print(\"Features selecionadas:\")\n",
    "print(f\"  - Pclass (1ª, 2ª ou 3ª classe)\")\n",
    "print(f\"  - Sex (0=female, 1=male)\")\n",
    "print(f\"  - Age (idade em anos)\")\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nConjunto de treino: {len(X_train)} amostras\")\n",
    "print(f\"Conjunto de teste: {len(X_test)} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criação e Treinamento do Modelo**\n",
    "\n",
    "Agora vamos treinar o modelo de Regressão Logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar e treinar o modelo\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer predições\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTADOS DO MODELO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAcurácia: {accuracy*100:.2f}%\")\n",
    "print(f\"\\nCoeficientes aprendidos (pesos):\")\n",
    "print(f\"  w_Pclass (Classe): {model.coef_[0][0]:.4f}\")\n",
    "print(f\"  w_Sex (Sexo): {model.coef_[0][1]:.4f}\")\n",
    "print(f\"  w_Age (Idade): {model.coef_[0][2]:.4f}\")\n",
    "print(f\"  b (bias/intercepto): {model.intercept_[0]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RELATÓRIO DE CLASSIFICAÇÃO\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Não Sobreviveu', 'Sobreviveu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretação dos Pesos**\n",
    "\n",
    "Vamos interpretar o que o modelo aprendeu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretação dos coeficientes via odds ratio\n",
    "\n",
    "print(\"INTERPRETAÇÃO DOS PESOS (ODDS RATIO):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, feature in enumerate(['Pclass', 'Sex', 'Age']):\n",
    "    coef = model.coef_[0][i]\n",
    "    odds_ratio = np.exp(coef)\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Coeficiente (w): {coef:.4f}\")\n",
    "    print(f\"  Odds Ratio (e^w): {odds_ratio:.4f}\")\n",
    "    \n",
    "    if odds_ratio > 1:\n",
    "        print(f\"  → Para cada aumento de 1 unidade, as odds de sobreviver aumentam {(odds_ratio-1)*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  → Para cada aumento de 1 unidade, as odds de sobreviver diminuem {(1-odds_ratio)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tecnicas'></a>\n",
    "## **Técnicas Avançadas**\n",
    "\n",
    "Vamos explorar técnicas mais sofisticadas para melhorar e analisar o modelo de Regressão Logística.\n",
    "\n",
    "### **Técnica 1: Ajuste do Threshold**\n",
    "\n",
    "O limiar de decisão padrão é 0.5, mas podemos ajustá-lo dependendo do contexto do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando diferentes thresholds\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "print(\"IMPACTO DO AJUSTE DE THRESHOLD:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Aplicar threshold customizado\n",
    "    y_pred_custom = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(y_test, y_pred_custom)\n",
    "    cm = confusion_matrix(y_test, y_pred_custom)\n",
    "    \n",
    "    # Calcular precision e recall manualmente\n",
    "    TP = cm[1, 1]  # True Positives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    FN = cm[1, 0]  # False Negatives\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {threshold}\")\n",
    "    print(f\"  Acurácia: {acc*100:.2f}%\")\n",
    "    print(f\"  Precision: {precision*100:.2f}%\")\n",
    "    print(f\"  Recall: {recall*100:.2f}%\")\n",
    "    print(f\"  Matriz de Confusão: {cm.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Técnica 2: Regularização (L1 e L2)**\n",
    "\n",
    "Regularização previne overfitting adicionando uma penalidade aos pesos grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando diferentes tipos de regularização\n",
    "\n",
    "print(\"COMPARAÇÃO DE REGULARIZAÇÃO:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sem regularização (C muito alto)\n",
    "model_no_reg = LogisticRegression(C=1e10, max_iter=1000, random_state=42)\n",
    "model_no_reg.fit(X_train, y_train)\n",
    "acc_no_reg = model_no_reg.score(X_test, y_test)\n",
    "\n",
    "# Ridge (L2) - padrão\n",
    "model_l2 = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42)\n",
    "model_l2.fit(X_train, y_train)\n",
    "acc_l2 = model_l2.score(X_test, y_test)\n",
    "\n",
    "# Lasso (L1)\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='saga', C=1.0, max_iter=1000, random_state=42)\n",
    "model_l1.fit(X_train, y_train)\n",
    "acc_l1 = model_l1.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nSem Regularização:\")\n",
    "print(f\"  Acurácia: {acc_no_reg*100:.2f}%\")\n",
    "print(f\"  Coeficientes: {model_no_reg.coef_[0]}\")\n",
    "\n",
    "print(f\"\\nRidge (L2):\")\n",
    "print(f\"  Acurácia: {acc_l2*100:.2f}%\")\n",
    "print(f\"  Coeficientes: {model_l2.coef_[0]}\")\n",
    "\n",
    "print(f\"\\nLasso (L1):\")\n",
    "print(f\"  Acurácia: {acc_l1*100:.2f}%\")\n",
    "print(f\"  Coeficientes: {model_l1.coef_[0]}\")\n",
    "\n",
    "print(\"\\nObservação: L1 (Lasso) pode forçar coeficientes a zero,\")\n",
    "print(\"realizando seleção automática de features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vantagens-desvantagens'></a>\n",
    "## **Vantagens e Desvantagens da Regressão Logística**\n",
    "\n",
    "Agora que você domina a Regressão Logística, vamos resumir seus pontos fortes e fracos:\n",
    "\n",
    "### **Vantagens**\n",
    "\n",
    "1. **Rápida e eficiente:** Treina rapidamente mesmo em datasets grandes\n",
    "2. **Fácil interpretação:** Coeficientes têm significado claro (odds ratios)\n",
    "3. **Saída probabilística:** Fornece probabilidades, não apenas classes\n",
    "4. **Não requer muitos dados:** Funciona bem com amostras relativamente pequenas\n",
    "5. **Poucos hiperparâmetros:** Fácil de configurar e ajustar\n",
    "6. **Naturalmente multiclasse:** Extensível via Softmax Regression\n",
    "7. **Baseline excelente:** Ótimo ponto de partida para comparação\n",
    "\n",
    "### **Desvantagens**\n",
    "\n",
    "1. **Assume relação linear:** Não captura relações não-lineares entre features e target\n",
    "2. **Sensível a multicolinearidade:** Não funciona bem quando features são muito correlacionadas\n",
    "3. **Limitado em alta dimensionalidade:** Performance cai com muitas features irrelevantes\n",
    "4. **Fronteira de decisão linear:** Não pode aprender fronteiras complexas\n",
    "5. **Requer features bem escaladas:** Sensível a escala das variáveis\n",
    "6. **Viés com classes desbalanceadas:** Tende a favorecer a classe majoritária\n",
    "7. **Outliers influenciam:** Valores extremos podem afetar os coeficientes\n",
    "\n",
    "### **Quando usar Regressão Logística?**\n",
    "\n",
    "**Bom para:**\n",
    "- Classificação binária simples\n",
    "- Quando interpretabilidade é crucial (medicina, finanças)\n",
    "- Baseline rápido para benchmark\n",
    "- Datasets pequenos a médios\n",
    "- Quando você precisa de probabilidades calibradas\n",
    "\n",
    "**Evitar quando:**\n",
    "- Relações complexas e não-lineares\n",
    "- Features altamente correlacionadas\n",
    "- Fronteiras de decisão complexas são necessárias\n",
    "- Quando você pode sacrificar interpretabilidade por acurácia\n",
    "\n",
    "### **Tipos de Regressão Logística**\n",
    "\n",
    "#### **Regressão Logística Binária**\n",
    "\n",
    "Funciona para problemas com apenas dois resultados possíveis. A variável dependente pode ter apenas dois valores, como sim/não ou 0/1. É o que usamos no exemplo do Titanic.\n",
    "\n",
    "#### **Regressão Logística Multinomial**\n",
    "\n",
    "Pode analisar problemas com vários resultados possíveis, desde que o número seja finito. Por exemplo, classificar emails em \"spam\", \"promoção\" ou \"importante\".\n",
    "\n",
    "#### **Regressão Logística Ordinal**\n",
    "\n",
    "Tipo especial de multinomial para problemas onde os números representam classificações ordenadas. Por exemplo, prever avaliações de 1 a 5 estrelas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recursos Adicionais**\n",
    "\n",
    "Para aprofundar seu conhecimento em Regressão Logística:\n",
    "\n",
    "- **Regularização:** [Logistic Regression and Regularization](https://medium.com/@rithpansanga/logistic-regression-and-regularization-avoiding-overfitting-and-improving-generalization-e9afdcddd09d)\n",
    "- **Softmax Regression:** [Multinomial Logistic Regression](https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/)\n",
    "- **Fundamentos:** [Understanding Logistic Regression](https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/)\n",
    "\n",
    "---\n",
    "\n",
    "<-- [**Anterior: Naive Bayes**](02_naive_bayes.ipynb) | [**Próximo: SVM**](04_svm.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
