{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes — Módulo 3, Notebook 2/6**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introdução ao Naive Bayes](#introducao)\n",
    "2. [Como o Naive Bayes Funciona?](#como-funciona)\n",
    "3. [Intuição Prática](#intuicao)\n",
    "4. [Formalização Matemática](#formalizacao)\n",
    "5. [Tipos de Naive Bayes](#tipos)\n",
    "6. [Implementação com Scikit-Learn](#sklearn)\n",
    "7. [Vantagens e Desvantagens](#vantagens-desvantagens)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introducao'></a>\n",
    "## **Introdução ao Naive Bayes**\n",
    "\n",
    "Imagine que acabamos de receber um email. O sistema precisa decidir se essa mensagem, com o assunto \"Esse está pagando! Aposte agora\", é um email importante ou spam. Como podemos resolver esse problema automaticamente?\n",
    "\n",
    "Esse é um cenário clássico para o algoritmo **Naive Bayes**, um dos métodos mais elegantes e eficientes em machine learning. Diferentemente do KNN que memoriza todos os dados, o Naive Bayes aprende padrões estatísticos para tomar decisões probabilísticas.\n",
    "\n",
    "O Naive Bayes é um classificador probabilístico baseado no **Teorema de Bayes**. Na classificação Bayesiana, estamos interessados em calcular a probabilidade de certa observação pertencer a uma categoria (ou classe) dado as características observadas. Matematicamente, queremos encontrar $P(C_k | X)$ — a probabilidade da categoria $k$ dado o vetor de características $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='como-funciona'></a>\n",
    "## **Como o Naive Bayes Funciona?**\n",
    "\n",
    "O algoritmo Naive Bayes classifica uma amostra $X = (x_1, x_2, ..., x_n)$ atribuindo a classe $C_k$ que maximiza a probabilidade condicional:\n",
    "\n",
    "$$P(C_k|X) = \\frac{P(X|C_k)P(C_k)}{P(X)}$$\n",
    "\n",
    "Onde:\n",
    "- $P(C_k|X)$: **Probabilidade a posteriori** — probabilidade da classe $C_k$ dados os atributos $X$\n",
    "- $P(X|C_k)$: **Verossimilhança** — probabilidade de observar os atributos $X$ na classe $C_k$\n",
    "- $P(C_k)$: **Probabilidade a priori** — probabilidade prévia da classe $C_k$\n",
    "- $P(X)$: **Evidência** — probabilidade dos atributos $X$ (constante para todas as classes)\n",
    "\n",
    "A predição é feita escolhendo a classe que maximiza $P(C_k|X)$.\n",
    "\n",
    "### **De onde vem o \"Naive\" (ingênuo)?**\n",
    "\n",
    "O termo \"naive\" refere-se à principal **hipótese simplificadora**: as variáveis são independentes entre si, dada a classe. Com essa suposição, a probabilidade condicional se torna:\n",
    "\n",
    "$$P(X|C_k) = P(x_1|C_k) \\times P(x_2|C_k) \\times ... \\times P(x_n|C_k)$$\n",
    "\n",
    "Isso facilita enormemente os cálculos, pois agora basta calcular a probabilidade de cada feature separadamente e multiplicá-las."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuicao'></a>\n",
    "## **Intuição Prática: Classificando Emails como Spam**\n",
    "\n",
    "Vamos consolidar esse conhecimento através de um exemplo prático. Utilizaremos o **SMS Spam Collection Dataset** para entender como o Naive Bayes toma decisões.\n",
    "\n",
    "### **O Problema**\n",
    "\n",
    "Recebemos um SMS com o texto:\n",
    "\n",
    "**\"WINNER!! You have won a £1000 prize! Call now to claim!\"**\n",
    "\n",
    "**Desafio:** Classificar essa mensagem como `spam` ou `ham` (não spam).\n",
    "\n",
    "Para resolver isso, o Naive Bayes segue três etapas fundamentais, que implementaremos passo a passo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o SMS Spam Collection Dataset\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"uciml/sms-spam-collection-dataset\",\n",
    "    \"spam.csv\",\n",
    "    pandas_kwargs={\"encoding\": \"latin\", \"usecols\": [0, 1], \"names\": [\"class\", \"sms\"], 'header': 0}\n",
    ")\n",
    "\n",
    "print(f\"Total de mensagens: {len(df)}\")\n",
    "print(f\"\\nPrimeiras 5 mensagens:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pré-processamento de Texto**\n",
    "\n",
    "Antes de aplicar o Naive Bayes, precisamos processar o texto. Vamos criar uma função que:\n",
    "\n",
    "1. Transforma todo o texto em minúsculo\n",
    "2. Remove pontuações\n",
    "3. Tokeniza (separa as palavras)\n",
    "4. Remove stopwords (palavras comuns como \"e\", \"o\") — opcional, mas boa prática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Pré-processa o texto removendo pontuações, convertendo para minúsculo e tokenizando.\n",
    "    \n",
    "    Parâmetros:\n",
    "    text (str): Texto a ser processado\n",
    "    \n",
    "    Retorna:\n",
    "    list: Lista de palavras processadas\n",
    "    \"\"\"\n",
    "    # Converter para minúsculo\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações e caracteres especiais\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar (separar em palavras)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Stopwords básicas em inglês (opcional - simplificado para o exemplo)\n",
    "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were'}\n",
    "    \n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Aplicando o pré-processamento\n",
    "df['words'] = df['sms'].apply(preprocess_text)\n",
    "\n",
    "print(\"Exemplo de pré-processamento:\")\n",
    "print(f\"Original: {df['sms'].iloc[0]}\")\n",
    "print(f\"Processado: {df['words'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 1: CALCULAR PROBABILIDADES A PRIORI\n",
    "# Fórmula: P(class) = número de mensagens da classe / total de mensagens\n",
    "# ===================================================================\n",
    "\n",
    "total_messages = len(df)\n",
    "spam_count = len(df[df['class'] == 'spam'])\n",
    "ham_count = len(df[df['class'] == 'ham'])\n",
    "\n",
    "prior_spam = spam_count / total_messages\n",
    "prior_ham = ham_count / total_messages\n",
    "\n",
    "print(f\"Total de Mensagens: {total_messages}\")\n",
    "print(f\"Total de Mensagens Spam: {spam_count}\")\n",
    "print(f\"Total de Mensagens Ham: {ham_count}\")\n",
    "print(f\"\\nP(SPAM): {prior_spam*100:.2f}%\")\n",
    "print(f\"P(HAM): {prior_ham*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nInterpretação:\")\n",
    "print(f\"  Antes de analisar o conteúdo de uma mensagem:\")\n",
    "print(f\"  - Probabilidade de ser SPAM: {prior_spam*100:.2f}%\")\n",
    "print(f\"  - Probabilidade de ser HAM: {prior_ham*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 2: CALCULAR VEROSSIMILHANÇA (Likelihood)\n",
    "# Fórmula: P(word|class) = (count(word in class) + 1) / (total words in class + V)\n",
    "# Onde V é o tamanho do vocabulário (número de palavras únicas)\n",
    "# O \"+1\" é a suavização de Laplace — evita probabilidade zero\n",
    "# ===================================================================\n",
    "\n",
    "spam_words = df[df['class'] == 'spam']['words'].sum()\n",
    "ham_words = df[df['class'] == 'ham']['words'].sum()\n",
    "\n",
    "# Contar a frequência das palavras em cada classe\n",
    "spam_word_count = Counter(spam_words)\n",
    "ham_word_count = Counter(ham_words)\n",
    "\n",
    "vocabulary = set(spam_word_count.keys()).union(set(ham_word_count.keys()))\n",
    "\n",
    "total_spam_words = len(spam_words)\n",
    "total_ham_words = len(ham_words)\n",
    "\n",
    "print(f\"Estatísticas de Palavras:\\n\")\n",
    "print(f\"  - Vocabulário Total (V): {len(vocabulary)} palavras únicas\")\n",
    "print(f\"  - Total de Palavras em Mensagens SPAM: {total_spam_words}\")\n",
    "print(f\"  - Total de Palavras em Mensagens HAM: {total_ham_words}\")\n",
    "\n",
    "print(f\"\\nPalavras mais comuns em SPAM:\")\n",
    "for word, count in spam_word_count.most_common(5):\n",
    "    print(f\"  {word}: {count}\")\n",
    "    \n",
    "print(f\"\\nPalavras mais comuns em HAM:\")\n",
    "for word, count in ham_word_count.most_common(5):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "def likelihood(word, cls):\n",
    "    \"\"\"Calcula P(word|class) com suavização de Laplace\"\"\"\n",
    "    if cls == 'spam':\n",
    "        word_count = spam_word_count.get(word, 0)\n",
    "        return (word_count + 1) / (total_spam_words + len(vocabulary))\n",
    "    else:\n",
    "        word_count = ham_word_count.get(word, 0)\n",
    "        return (word_count + 1) / (total_ham_words + len(vocabulary))\n",
    "\n",
    "print(f\"\\nExemplo de Cálculo de Verossimilhança:\")\n",
    "print(f\"  P('free'|SPAM) = {likelihood('free', 'spam')*100:.4f}%\")\n",
    "print(f\"  P('free'|HAM) = {likelihood('free', 'ham')*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 3: CLASSIFICAÇÃO DE UMA NOVA MENSAGEM\n",
    "# ===================================================================\n",
    "\n",
    "new_message = \"WINNER!! You have won a £1000 prize! Call now to claim!\"\n",
    "\n",
    "preprocessed_message = preprocess_text(new_message)\n",
    "\n",
    "print(f\"Mensagem original: {new_message}\")\n",
    "print(f\"Mensagem processada: {preprocessed_message}\\n\")\n",
    "\n",
    "# Calcular probabilidade logarítmica para evitar underflow\n",
    "# (multiplicar muitas probabilidades pequenas pode resultar em zero)\n",
    "log_prob_spam = np.log(prior_spam)\n",
    "log_prob_ham = np.log(prior_ham)\n",
    "\n",
    "for word in preprocessed_message:\n",
    "    log_prob_spam += np.log(likelihood(word, 'spam'))\n",
    "    log_prob_ham += np.log(likelihood(word, 'ham'))\n",
    "\n",
    "resultado = \"SPAM\" if log_prob_spam > log_prob_ham else \"HAM\"\n",
    "\n",
    "print(f\"Log P(SPAM|mensagem): {log_prob_spam:.4f}\")\n",
    "print(f\"Log P(HAM|mensagem): {log_prob_ham:.4f}\")\n",
    "print(f\"\\nClassificação: {resultado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explicação dos Três Passos**\n",
    "\n",
    "**1. Probabilidade a Priori**\n",
    "\n",
    "A probabilidade a priori é nossa crença sobre algo antes de analisar qualquer nova evidência:\n",
    "\n",
    "- O algoritmo analisa todo o histórico de mensagens e calcula a proporção de cada categoria\n",
    "- No nosso caso: $P(Spam) \\approx 13\\%$ e $P(Ham) \\approx 87\\%$\n",
    "- Isso significa que, sem ler a nova mensagem, já sabemos que há uma chance baixa de ser spam\n",
    "\n",
    "**2. Verossimilhança**\n",
    "\n",
    "A verossimilhança mede o quão provável é encontrar a evidência (as palavras da mensagem) assumindo que ela pertence a uma determinada categoria:\n",
    "\n",
    "- Se esta mensagem fosse Spam, qual seria a probabilidade de conter \"winner\", \"prize\", \"call\"?\n",
    "- O algoritmo calcula a frequência dessas palavras dentro de cada categoria\n",
    "- Como assumimos independência (naive), multiplicamos as probabilidades individuais\n",
    "\n",
    "**3. Probabilidade a Posteriori**\n",
    "\n",
    "A probabilidade a posteriori é o resultado final — a probabilidade de uma mensagem pertencer a uma categoria depois de analisarmos a evidência:\n",
    "\n",
    "$$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$$\n",
    "\n",
    "Note que apesar da probabilidade a priori favorecer \"Ham\" (87%), a evidência contida nas palavras gera uma verossimilhança tão maior para \"Spam\" que a probabilidade a posteriori de ser Spam será mais alta!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='formalizacao'></a>\n",
    "## **Formalização Matemática**\n",
    "\n",
    "Agora que você entendeu intuitivamente como o Naive Bayes funciona, vamos formalizar o algoritmo matematicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrada do Algoritmo**\n",
    "\n",
    "1. **Conjunto de treinamento:** $D = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\}$\n",
    "\n",
    "   Onde:\n",
    "   - $x^{(i)}$: vetor de $d$ features ($x^{(i)} = (x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d)$)\n",
    "   - $y^{(i)}$: rótulo de classe pertencente ao conjunto finito $C = \\{c_1, c_2, ..., c_k\\}$\n",
    "\n",
    "2. **Amostra para previsão:** $x^o = (x_1, x_2, ..., x_d)$ — vetor de features que precisa ser classificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processo do Algoritmo**\n",
    "\n",
    "**Objetivo:** Encontrar a classe $\\hat{y}$ que maximiza a probabilidade a posteriori $P(y|x^o)$\n",
    "\n",
    "**1. Teorema de Bayes**\n",
    "\n",
    "Para cada classe $c_k \\in C$, calculamos a probabilidade a posteriori:\n",
    "\n",
    "$$P(c_k|x^o) = \\frac{P(x^o|c_k)P(c_k)}{P(x^o)}$$\n",
    "\n",
    "Onde:\n",
    "- $P(c_k|x^o)$: **Probabilidade a posteriori** — probabilidade da classe $c_k$ dado $x^o$\n",
    "- $P(c_k)$: **Probabilidade a priori** — probabilidade da classe $c_k$\n",
    "- $P(x^o|c_k)$: **Verossimilhança** — probabilidade da amostra $x^o$ dada a classe $c_k$\n",
    "- $P(x^o)$: **Evidência** — probabilidade da amostra $x^o$ (constante para todas as classes)\n",
    "\n",
    "**2. Cálculo da Probabilidade a Priori**\n",
    "\n",
    "A priori é estimada a partir da frequência relativa de cada classe no conjunto de treinamento:\n",
    "\n",
    "$$P(c_k) = \\frac{\\text{Número de amostras da classe } c_k}{\\text{Número total de amostras}} = \\frac{\\sum_{i=1}^{n} I(y^{(i)} = c_k)}{n}$$\n",
    "\n",
    "No código, implementamos isso calculando `spam_count / total_messages`.\n",
    "\n",
    "**3. Cálculo da Verossimilhança (Suposição Naive)**\n",
    "\n",
    "Aqui reside a suposição ingênua do algoritmo: todas as features são condicionalmente independentes, dada a classe:\n",
    "\n",
    "$$P(x^o|c_k) = P(x_1, x_2, ..., x_d|c_k) = \\prod^d_{j=1}P(x_j|c_k)$$\n",
    "\n",
    "O cálculo de cada $P(x_j|c_k)$ depende da natureza da feature $x_j$, levando aos diferentes tipos de Naive Bayes.\n",
    "\n",
    "**4. Tomada de Decisão (MAP — Maximum A Posteriori)**\n",
    "\n",
    "Como $P(x^o)$ é constante para todas as classes, pode ser descartado na maximização:\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_{c_k \\in C} P(c_k) \\prod^d_{j=1}P(x_j|c_k)$$\n",
    "\n",
    "Onde $\\hat{y} \\in C$ é o rótulo da classe predita para $x^o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tipos'></a>\n",
    "## **Tipos de Naive Bayes**\n",
    "\n",
    "A principal diferença entre os tipos de Naive Bayes está em como eles modelam a distribuição $P(x_j|c_k)$ para diferentes tipos de dados.\n",
    "\n",
    "### **1. Gaussian Naive Bayes**\n",
    "\n",
    "**Uso:** Features contínuas que seguem uma distribuição normal (Gaussiana)\n",
    "\n",
    "**Hipótese:** Para cada classe $c_k$, o valor da feature $x_j$ é distribuído segundo uma Gaussiana\n",
    "\n",
    "**Processamento:** Durante o treinamento, calcula-se a média $\\mu_{j,k}$ e a variância $\\sigma^2_{j,k}$ da feature $j$ para todas as amostras da classe $c_k$. A verossimilhança é calculada usando a função de densidade de probabilidade (PDF) da distribuição normal:\n",
    "\n",
    "$$P(x_j|c_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{j,k}}}\\exp\\left(-\\frac{(x_j-\\mu_{j,k})^2}{2\\sigma^2_{j,k}}\\right)$$\n",
    "\n",
    "**Aplicações:** Classificação de dados numéricos contínuos (ex: temperatura, pressão, medições físicas)\n",
    "\n",
    "### **2. Multinomial Naive Bayes**\n",
    "\n",
    "**Uso:** Features discretas que representam contagens ou frequências (clássico para classificação de texto)\n",
    "\n",
    "**Hipótese:** As features são geradas a partir de uma distribuição multinomial\n",
    "\n",
    "**Processamento:** A verossimilhança $P(x_j|c_k)$ é a frequência relativa da feature $x_j$ (ex: uma palavra) dentro dos documentos da classe $c_k$. Para evitar probabilidade zero, usa-se a suavização de Laplace:\n",
    "\n",
    "$$P(x_j|c_k) = \\frac{N_{j,k} + \\alpha}{N_k + \\alpha d}$$\n",
    "\n",
    "Onde:\n",
    "- $N_{j,k}$: contagem da feature $x_j$ na classe $c_k$\n",
    "- $N_k$: contagem total de todas as features na classe $c_k$\n",
    "- $d$: número total de features únicas (tamanho do vocabulário)\n",
    "- $\\alpha$: parâmetro de suavização ($\\alpha = 1$ para Laplace smoothing)\n",
    "\n",
    "No código, implementamos isso com `(word_count + 1) / (total_words + len(vocabulary))`.\n",
    "\n",
    "**Aplicações:** Classificação de texto, análise de sentimento, detecção de spam\n",
    "\n",
    "### **3. Bernoulli Naive Bayes**\n",
    "\n",
    "**Uso:** Features binárias (presença ou ausência de uma característica)\n",
    "\n",
    "**Hipótese:** As features são variáveis binárias independentes\n",
    "\n",
    "**Processamento:** A verossimilhança é calculada para a presença ($x_j = 1$) ou ausência ($x_j = 0$) da feature na classe $c_k$:\n",
    "\n",
    "$$P(x_j|c_k) = P(j|c_k)^{x_j}(1-P(j|c_k))^{(1-x_j)}$$\n",
    "\n",
    "Onde $P(j|c_k)$ é a probabilidade da feature $j$ ocorrer na classe $c_k$.\n",
    "\n",
    "**Aplicações:** Classificação de documentos curtos com features binárias (ex: presença/ausência de palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn'></a>\n",
    "## **Implementação com Scikit-Learn**\n",
    "\n",
    "Na prática, usamos bibliotecas otimizadas como o Scikit-Learn. Vamos ver como é simples implementar o mesmo algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para Teste (Spam e Ham)\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"uciml/sms-spam-collection-dataset\",\n",
    "  \"spam.csv\",\n",
    "  pandas_kwargs={\"encoding\": \"latin\", \"usecols\": [0,1], \"names\": [\"class\", \"sms\"], 'header': 0}\n",
    ")\n",
    "\n",
    "# Preparando os dados\n",
    "X = df['sms'].tolist()\n",
    "y = df['class'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {len(X_train)} mensagens\")\n",
    "print(f\"Conjunto de teste: {len(X_test)} mensagens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementação Básica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Vetorização (transformar texto em números)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Vocabulário (amostra): {list(vectorizer.get_feature_names_out())[1000:1005]}\")\n",
    "print(f\"X_train_vec shape: {X_train_vec.shape}\")\n",
    "print(f\"X_test_vec shape: {X_test_vec.shape}\")\n",
    "\n",
    "# Passo 2: Treinar o modelo\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Passo 3: Fazer previsões\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "\n",
    "# Passo 4: Avaliar\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAcurácia: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Técnicas Avançadas**\n",
    "\n",
    "Vamos explorar variações e otimizações do Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 1: Comparando as Variantes do Naive Bayes\n",
    "# ===================================================================\n",
    "\n",
    "# 1. MultinomialNB (dados de contagem)\n",
    "nb_multi = MultinomialNB()\n",
    "nb_multi.fit(X_train_vec, y_train)\n",
    "accuracy_multi = nb_multi.score(X_test_vec, y_test)\n",
    "\n",
    "# 2. BernoulliNB (dados binários)\n",
    "X_train_bin = (X_train_vec > 0).astype(int)\n",
    "X_test_bin = (X_test_vec > 0).astype(int)\n",
    "\n",
    "nb_bernoulli = BernoulliNB()\n",
    "nb_bernoulli.fit(X_train_bin, y_train)\n",
    "accuracy_bernoulli = nb_bernoulli.score(X_test_bin, y_test)\n",
    "\n",
    "# 3. GaussianNB (dados contínuos)\n",
    "X_train_dense = X_train_vec.toarray()\n",
    "X_test_dense = X_test_vec.toarray()\n",
    "\n",
    "nb_gaussian = GaussianNB()\n",
    "nb_gaussian.fit(X_train_dense, y_train)\n",
    "accuracy_gaussian = nb_gaussian.score(X_test_dense, y_test)\n",
    "\n",
    "print(\"Comparação de Variantes do Naive Bayes:\\n\")\n",
    "print(f\"1. MultinomialNB: {accuracy_multi*100:.2f}% - Melhor para contagem de palavras\")\n",
    "print(f\"2. BernoulliNB: {accuracy_bernoulli*100:.2f}% - Melhor para features binárias\")\n",
    "print(f\"3. GaussianNB: {accuracy_gaussian*100:.2f}% - Melhor para dados contínuos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 2: Ajuste do Parâmetro Alpha (Suavização de Laplace)\n",
    "# ===================================================================\n",
    "\n",
    "# Parâmetros:\n",
    "# - alpha: parâmetro de suavização de Laplace (default=1.0)\n",
    "#   - alpha=1.0 é \"add-one smoothing\"\n",
    "#   - alpha=0.0 significa sem suavização\n",
    "# - fit_prior: se True, aprende probabilidade a priori dos dados\n",
    "# - class_prior: probabilidades a priori customizadas\n",
    "\n",
    "alphas = [0.001, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "results = []\n",
    "\n",
    "print(\"Testando diferentes valores de alpha:\\n\")\n",
    "for alpha in alphas:\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb.fit(X_train_vec, y_train)\n",
    "    accuracy = nb.score(X_test_vec, y_test)\n",
    "    results.append((alpha, accuracy))\n",
    "    print(f\"Alpha: {alpha:5.3f} → Acurácia: {accuracy*100:.2f}%\")\n",
    "\n",
    "best_alpha, best_acc = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nMelhor alpha: {best_alpha} com acurácia de {best_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TÉCNICA 3: CountVectorizer vs TF-IDF Vectorizer\n",
    "# ===================================================================\n",
    "\n",
    "# CountVectorizer: conta a frequência das palavras\n",
    "# TF-IDF: considera frequência + importância (palavras raras têm mais peso)\n",
    "\n",
    "print(\"Comparando vetorizações:\\n\")\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer_count = CountVectorizer()\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_count = vectorizer_count.transform(X_test)\n",
    "\n",
    "nb_count = MultinomialNB()\n",
    "nb_count.fit(X_train_count, y_train)\n",
    "acc_count = nb_count.score(X_test_count, y_test)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "acc_tfidf = nb_tfidf.score(X_test_tfidf, y_test)\n",
    "\n",
    "print(f\"CountVectorizer: {acc_count*100:.2f}% - Simples e eficaz\")\n",
    "print(f\"TF-IDF Vectorizer: {acc_tfidf*100:.2f}% - Pondera importância das palavras\")\n",
    "print(f\"\\nNota: CountVectorizer geralmente funciona melhor com Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vantagens-desvantagens'></a>\n",
    "## **Vantagens e Desvantagens do Naive Bayes**\n",
    "\n",
    "Agora que você conhece o Naive Bayes profundamente, vamos resumir seus pontos fortes e fracos:\n",
    "\n",
    "### **Vantagens**\n",
    "\n",
    "1. **Muito rápido:** Extremamente eficiente tanto no treinamento quanto na predição\n",
    "2. **Simples de implementar:** Fácil de entender e interpretar\n",
    "3. **Poucos hiperparâmetros:** Praticamente não requer tuning (apenas alpha em alguns casos)\n",
    "4. **Funciona bem com poucos dados:** Eficaz mesmo com conjuntos de treinamento pequenos\n",
    "5. **Naturalmente multiclasse:** Sem necessidade de adaptações para múltiplas classes\n",
    "6. **Robusto a features irrelevantes:** Lida bem com muitas features\n",
    "\n",
    "### **Desvantagens**\n",
    "\n",
    "1. **Suposição ingênua de independência:** Raramente verdadeira na prática (ex: \"São\" e \"Paulo\" não são independentes)\n",
    "2. **Não funciona bem com features correlacionadas:** Performance cai quando as features são dependentes\n",
    "3. **Problema de frequência zero:** Features não vistas no treinamento podem causar problemas (mitigado com smoothing)\n",
    "4. **Sensível à distribuição:** Gaussian NB assume distribuição normal dos dados\n",
    "5. **Estimativas de probabilidade não calibradas:** As probabilidades podem não refletir a confiança real\n",
    "6. **Desempenho limitado em problemas complexos:** Outros algoritmos podem superar em datasets grandes e complexos\n",
    "\n",
    "### **Quando usar Naive Bayes?**\n",
    "\n",
    "**Bom para:**\n",
    "- Classificação de texto (spam, sentimento, categorização)\n",
    "- Problemas com muitas features\n",
    "- Baseline rápido para comparação\n",
    "- Datasets pequenos a médios\n",
    "- Quando interpretabilidade é importante\n",
    "\n",
    "**Evitar quando:**\n",
    "- Features altamente correlacionadas\n",
    "- Quando probabilidades calibradas são necessárias\n",
    "- Problemas com relações complexas entre features\n",
    "- Quando há tempo para treinar modelos mais sofisticados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Considerações Importantes**\n",
    "\n",
    "### **1. Suposição de Independência Condicional**\n",
    "\n",
    "Embora raramente seja verdadeira em cenários reais, o Naive Bayes frequentemente apresenta boa performance. Isso acontece porque a classificação não exige estimativa de probabilidade perfeitamente calibrada, mas apenas que a classe correta tenha a maior pontuação a posteriori.\n",
    "\n",
    "### **2. Zero-Frequency Problem**\n",
    "\n",
    "Se uma feature de uma nova amostra não foi vista durante o treinamento para uma determinada classe, sua probabilidade $P(x_j|c_k)$ seria 0. Devido ao produtório, isso anularia a posteriori para aquela classe. A **suavização de Laplace** (Laplace smoothing) é a técnica usada para mitigar esse problema, adicionando uma contagem mínima a todas as features.\n",
    "\n",
    "### **3. Estabilidade Numérica (Log-Probabilities)**\n",
    "\n",
    "A multiplicação de muitas probabilidades pode resultar em valores extremamente pequenos, levando a problemas de underflow (arredondar para 0). Por isso, trabalhamos com a soma dos logaritmos das probabilidades, transformando o produtório em um somatório:\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_{c_k \\in C} \\left(\\log(P(c_k)) + \\sum^d_{j=1}\\log(P(x_j|c_k))\\right)$$\n",
    "\n",
    "No código, implementamos isso com `np.log()` para evitar underflow numérico.\n",
    "\n",
    "### **4. Aplicações do Naive Bayes**\n",
    "\n",
    "- **Filtragem de spam:** Classificação de emails e mensagens\n",
    "- **Análise de sentimento:** Determinar se um texto é positivo, negativo ou neutro\n",
    "- **Categorização de documentos:** Organizar documentos por tópico\n",
    "- **Diagnóstico médico:** Probabilidade de doenças baseado em sintomas\n",
    "- **Pontuação de crédito:** Avaliação de credibilidade para empréstimos\n",
    "- **Previsão do tempo:** Classificação de condições meteorológicas\n",
    "\n",
    "---\n",
    "\n",
    "<-- [**Anterior: KNN**](01_knn.ipynb) | [**Próximo: Regressão Logística**](03_regressao_logistica.ipynb) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
